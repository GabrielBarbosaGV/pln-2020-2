{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv('./sentences.csv')\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab = set(data['word'])\n",
    "vocab_list = list(vocab)\n",
    "\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "\n",
    "for idx, word in enumerate(vocab_list):\n",
    "    word2idx[word] = idx\n",
    "    idx2word[idx] = word\n",
    "\n",
    "tags = set(data['tag'])\n",
    "tags_list = list(tags)\n",
    "\n",
    "tag2idx = {}\n",
    "idx2tag = {}\n",
    "\n",
    "for idx, tag in enumerate(tags_list):\n",
    "    tag2idx[tag] = idx\n",
    "    idx2tag[idx] = tag"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "converted_data = data.copy()\n",
    "\n",
    "converted_data['word'] = converted_data['word'].transform(lambda word: word2idx[word])\n",
    "converted_data['tag'] = converted_data['tag'].transform(lambda tag: tag2idx[tag])\n",
    "converted_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list_data = converted_data.groupby(['sentence_number'])['word', 'tag'].agg(lambda i: list(i))\n",
    "list_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_len = list_data['word'].map(len).max()\n",
    "max_len"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pad_tokens = tf.keras.preprocessing.sequence.pad_sequences(list_data['word'])\n",
    "pad_tags = tf.keras.preprocessing.sequence.pad_sequences(list_data['tag'])\n",
    "pad_tags = [tf.keras.utils.to_categorical(tag, num_classes=len(tags)) for tag in pad_tags]\n",
    "\n",
    "train_tokens, test_tokens, train_tags, test_tags = train_test_split(pad_tokens, pad_tags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_dim = len(vocab)\n",
    "input_length = max_len\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=input_dim, output_dim=64, input_length=input_length),\n",
    "    Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat'),\n",
    "    LSTM(units=64, return_sequences=True, dropout=0.5, recurrent_dropout=0.5),\n",
    "    TimeDistributed(Dense(len(tags), activation=\"relu\"))\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def objective(trial):\n",
    "    embedding_output_dim = trial.suggest_int(\"embedding_output_dim\", 16, 256)\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=input_dim, output_dim=embedding_output_dim, input_length=input_length),\n",
    "        Bidirectional(LSTM(units=embedding_output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat'),\n",
    "        LSTM(units=embedding_output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5),\n",
    "        TimeDistributed(Dense(len(tags), activation='relu'))\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 150)\n",
    "\n",
    "    model.fit(train_tokens, np.array(train_tags), verbose=1, epochs=num_epochs)\n",
    "\n",
    "    return model.evaluate(test_tokens, np.array(test_tags), return_dict=True)['accuracy']\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "#study.optimize(objective, n_trials=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.fit(train_tokens, np.array(train_tags), verbose=1, epochs=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "t = 'Como conseguir dinheiro'.split(' ')\n",
    "t2 = [word2idx[word] for word in t]\n",
    "t3 = model.predict(t2)\n",
    "[idx2tag[np.argmax(cat)] for cat in t3]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "interpreter": {
   "hash": "11952d6ef341716fa9615f45cd192d3e9c3fce0851a7ac02304594f37d763982"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}