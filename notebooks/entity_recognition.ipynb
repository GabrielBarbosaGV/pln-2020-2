{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"interpreter":{"hash":"11952d6ef341716fa9615f45cd192d3e9c3fce0851a7ac02304594f37d763982"},"colab":{"name":"entity_identification.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python","version":"3.9.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"cells":[{"cell_type":"markdown","source":["# Reconhecimento de entidades"],"metadata":{}},{"cell_type":"markdown","source":["Optuna será utilizado para _parameter tuning_"],"metadata":{}},{"cell_type":"code","execution_count":33,"source":["!pip install --quiet optuna"],"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"metadata":{"id":"XMOpeCawnHrb","executionInfo":{"status":"ok","timestamp":1629040868119,"user_tz":180,"elapsed":2988,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}}}},{"cell_type":"code","execution_count":34,"source":["import pandas as pd\n","import tensorflow as tf\n","from collections import defaultdict\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import optuna"],"outputs":[],"metadata":{"id":"dZSLr-yjhxMq","executionInfo":{"status":"ok","timestamp":1629040868120,"user_tz":180,"elapsed":94,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}}}},{"cell_type":"markdown","source":["O _dataset_ utilizado consiste de linhas contendo o número da frase (\"_sentence\\_number_\"), seguido da palavra (\"_word_\"), que, por sua vez, é seguida de uma _tag_."],"metadata":{}},{"cell_type":"code","execution_count":35,"source":["data = pd.read_csv('./notebooks/sentences.csv')\n","data.head()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["   sentence_number        word     tag\n","0                1        como   OTHER\n","1                1        usar   OTHER\n","2                1      lucros   OTHER\n","3                1           e   OTHER\n","4                1  reinvestir  INVEST"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_number</th>\n","      <th>word</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>como</td>\n","      <td>OTHER</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>usar</td>\n","      <td>OTHER</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>lucros</td>\n","      <td>OTHER</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>e</td>\n","      <td>OTHER</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>reinvestir</td>\n","      <td>INVEST</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":35}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"RocDKd8NhxMt","executionInfo":{"status":"ok","timestamp":1629040868122,"user_tz":180,"elapsed":92,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}},"outputId":"740ccc69-3fc0-4f73-ca5a-6266a57d987c"}},{"cell_type":"markdown","source":["São criados mapeamentos entre palavras e números distintos para cada. Isso também é feito para as tags."],"metadata":{}},{"cell_type":"code","execution_count":36,"source":["vocab = set(data['word'])\n","vocab_list = list(vocab)\n","\n","word2idx = {}\n","idx2word = {}\n","\n","for idx, word in enumerate(vocab_list):\n","    word2idx[word] = idx\n","    idx2word[idx] = word\n","\n","tags = set(data['tag'])\n","tags_list = list(tags)\n","\n","tag2idx = {}\n","idx2tag = {}\n","\n","for idx, tag in enumerate(tags_list):\n","    tag2idx[tag] = idx\n","    idx2tag[idx] = tag"],"outputs":[],"metadata":{"id":"870P4bwAhxMu","executionInfo":{"status":"ok","timestamp":1629040868132,"user_tz":180,"elapsed":92,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}}}},{"cell_type":"markdown","source":["Para os fins de treinamento, as palavras originais não têm serventia. Cria-se um novo dataset contendo somente os números correspondentes aos valores originais, de acordo com a conversão que acaba de ser feita."],"metadata":{}},{"cell_type":"code","execution_count":37,"source":["converted_data = data.copy()\n","\n","converted_data['word'] = converted_data['word'].transform(lambda word: word2idx[word])\n","converted_data['tag'] = converted_data['tag'].transform(lambda tag: tag2idx[tag])\n","converted_data.head()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["   sentence_number  word  tag\n","0                1    63    0\n","1                1    85    0\n","2                1    39    0\n","3                1    19    0\n","4                1   104    1"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_number</th>\n","      <th>word</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>63</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>39</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>19</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>104</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":37}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"37wos4BEhxMv","executionInfo":{"status":"ok","timestamp":1629040868134,"user_tz":180,"elapsed":92,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}},"outputId":"a5d1c94e-7151-4ef2-e4fd-b555a18f42d7"}},{"cell_type":"markdown","source":["Agregam-se em listas os valores de cada coluna de acordo com o número de cada frase."],"metadata":{}},{"cell_type":"code","execution_count":38,"source":["list_data = converted_data.groupby(['sentence_number'])[['word', 'tag']].agg(lambda i: list(i))\n","list_data.head()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                              word  \\\n","sentence_number                                                      \n","1                        [63, 85, 39, 19, 104, 91, 23, 48, 69, 42]   \n","2                [63, 10, 88, 25, 66, 1, 2, 54, 4, 121, 29, 79,...   \n","3                                       [57, 31, 84, 109, 68, 117]   \n","4                                  [31, 81, 87, 94, 35, 17, 95, 1]   \n","5                  [107, 29, 35, 105, 110, 60, 52, 33, 45, 24, 93]   \n","\n","                                                               tag  \n","sentence_number                                                     \n","1                                 [0, 0, 0, 0, 1, 14, 0, 14, 0, 8]  \n","2                [0, 0, 0, 17, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n","3                                               [0, 0, 0, 0, 0, 0]  \n","4                                        [0, 0, 0, 0, 0, 11, 0, 2]  \n","5                               [0, 0, 0, 13, 0, 0, 0, 0, 1, 0, 0]  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>tag</th>\n","    </tr>\n","    <tr>\n","      <th>sentence_number</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>[63, 85, 39, 19, 104, 91, 23, 48, 69, 42]</td>\n","      <td>[0, 0, 0, 0, 1, 14, 0, 14, 0, 8]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[63, 10, 88, 25, 66, 1, 2, 54, 4, 121, 29, 79,...</td>\n","      <td>[0, 0, 0, 17, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[57, 31, 84, 109, 68, 117]</td>\n","      <td>[0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[31, 81, 87, 94, 35, 17, 95, 1]</td>\n","      <td>[0, 0, 0, 0, 0, 11, 0, 2]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>[107, 29, 35, 105, 110, 60, 52, 33, 45, 24, 93]</td>\n","      <td>[0, 0, 0, 13, 0, 0, 0, 0, 1, 0, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":38}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":303},"id":"_yJy2vhshxMw","executionInfo":{"status":"ok","timestamp":1629040868135,"user_tz":180,"elapsed":87,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}},"outputId":"fe6046a4-957b-4bd2-cfcf-277458bfa7a8"}},{"cell_type":"markdown","source":["Tomando-se uma lista de índices do _dataset_, e convertendo-os de volta em palavras, vê-se que a ordem é mantida"],"metadata":{}},{"cell_type":"code","execution_count":39,"source":["[idx2word[idx] for idx in list_data['word'][5]]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['quais',\n"," 'são',\n"," 'os',\n"," 'melhores',\n"," 'recursos',\n"," 'para',\n"," 'aprender',\n"," 'sobre',\n"," 'investimento',\n"," 'em',\n"," 'títulos']"]},"metadata":{},"execution_count":39}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mb0FX_K4yZlZ","executionInfo":{"status":"ok","timestamp":1629040868138,"user_tz":180,"elapsed":77,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}},"outputId":"1e7aedaf-c70f-4b4b-f31a-711014220bee"}},{"cell_type":"markdown","source":["O tamanho da maior frase será necessário"],"metadata":{}},{"cell_type":"code","execution_count":40,"source":["max_len = list_data['word'].map(len).max()\n","max_len"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["28"]},"metadata":{},"execution_count":40}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_6UKeQKhxMw","executionInfo":{"status":"ok","timestamp":1629040868140,"user_tz":180,"elapsed":71,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}},"outputId":"786ebf31-78af-4bdf-8015-5cb506c8f545"}},{"cell_type":"markdown","source":["A função pad_sequences de `tf.keras.preprocessing.sequence` é útil para adicionar elementos extras a listas, fazendo com que tenham todas o mesmo tamanho.\n","\n","`tf.keras.utils.to_categorical` é usada para converter a representação das tags de números para _arrays_ de _One-Hot-Encoding_."],"metadata":{}},{"cell_type":"code","execution_count":41,"source":["pad_tokens = tf.keras.preprocessing.sequence.pad_sequences(list_data['word'])\n","pad_tags = tf.keras.preprocessing.sequence.pad_sequences(list_data['tag'])\n","pad_tags = [tf.keras.utils.to_categorical(tag, num_classes=len(tags)) for tag in pad_tags]\n","\n","train_tokens, test_tokens, train_tags, test_tags = train_test_split(pad_tokens, pad_tags)"],"outputs":[],"metadata":{"id":"F8cRMqgbhxMx","executionInfo":{"status":"ok","timestamp":1629040868143,"user_tz":180,"elapsed":64,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}}}},{"cell_type":"code","execution_count":42,"source":["from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional\n","from tensorflow.keras import optimizers"],"outputs":[],"metadata":{"id":"BjJfrEJ4hxMx","executionInfo":{"status":"ok","timestamp":1629040868145,"user_tz":180,"elapsed":64,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}}}},{"cell_type":"markdown","source":["Optuna será utilizado para executar um número de experimentos com modelos que receberão parâmetros \"_embedding\\_output\\_dim_\" e \"_num\\_epochs_\" diferentes. \"_embedding\\_output\\_dim_\" dita o tamanho da saída da camada de _embedding_, e \"_num\\_epochs_\" o número de épocas de treinamento."],"metadata":{}},{"cell_type":"code","execution_count":43,"source":["input_dim = len(vocab)\n","input_length = max_len\n","\n","def objective(trial):\n","    embedding_output_dim = trial.suggest_int(\"embedding_output_dim\", 16, 256)\n","\n","    model = Sequential([\n","        Embedding(input_dim=input_dim, output_dim=embedding_output_dim, input_length=input_length),\n","        Bidirectional(LSTM(units=embedding_output_dim, return_sequences=True), merge_mode='concat'),\n","        LSTM(units=embedding_output_dim, return_sequences=True),\n","        TimeDistributed(Dense(len(tags), activation='relu'))\n","    ])\n","\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    num_epochs = trial.suggest_int(\"num_epochs\", 5, 150)\n","\n","    model.fit(train_tokens, np.array(train_tags), verbose=1, epochs=num_epochs)\n","\n","    return model.evaluate(test_tokens, np.array(test_tags), return_dict=True)['accuracy']\n","\n","study = optuna.create_study(direction='maximize')\n","study.optimize(objective, n_trials=50)"],"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:50:08,086]\u001b[0m A new study created in memory with name: no-name-c2581ee8-1805-4ba6-aaea-ff65cbba2f33\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/121\n","1/1 [==============================] - 5s 5s/step - loss: 11.4977 - accuracy: 0.0020\n","Epoch 2/121\n","1/1 [==============================] - 0s 177ms/step - loss: 1.4087 - accuracy: 0.9067\n","Epoch 3/121\n","1/1 [==============================] - 0s 387ms/step - loss: 1.1520 - accuracy: 0.9067\n","Epoch 4/121\n","1/1 [==============================] - 0s 197ms/step - loss: 0.8501 - accuracy: 0.9067\n","Epoch 5/121\n","1/1 [==============================] - 0s 393ms/step - loss: 0.7830 - accuracy: 0.9067\n","Epoch 6/121\n","1/1 [==============================] - 0s 274ms/step - loss: 0.7536 - accuracy: 0.9067\n","Epoch 7/121\n","1/1 [==============================] - 0s 137ms/step - loss: 0.7340 - accuracy: 0.9067\n","Epoch 8/121\n","1/1 [==============================] - 0s 123ms/step - loss: 0.7221 - accuracy: 0.9067\n","Epoch 9/121\n","1/1 [==============================] - 0s 296ms/step - loss: 0.7101 - accuracy: 0.9067\n","Epoch 10/121\n","1/1 [==============================] - 0s 233ms/step - loss: 0.6903 - accuracy: 0.9067\n","Epoch 11/121\n","1/1 [==============================] - 0s 262ms/step - loss: 0.6677 - accuracy: 0.9067\n","Epoch 12/121\n","1/1 [==============================] - 0s 331ms/step - loss: 0.6270 - accuracy: 0.9067\n","Epoch 13/121\n","1/1 [==============================] - 0s 296ms/step - loss: 0.6100 - accuracy: 0.9067\n","Epoch 14/121\n","1/1 [==============================] - 0s 233ms/step - loss: 0.6138 - accuracy: 0.9067\n","Epoch 15/121\n","1/1 [==============================] - 0s 183ms/step - loss: 0.6192 - accuracy: 0.9067\n","Epoch 16/121\n","1/1 [==============================] - 0s 173ms/step - loss: 0.6222 - accuracy: 0.9067\n","Epoch 17/121\n","1/1 [==============================] - 0s 206ms/step - loss: 0.6222 - accuracy: 0.9067\n","Epoch 18/121\n","1/1 [==============================] - 0s 233ms/step - loss: 0.6210 - accuracy: 0.9067\n","Epoch 19/121\n","1/1 [==============================] - 0s 182ms/step - loss: 0.6218 - accuracy: 0.9067\n","Epoch 20/121\n","1/1 [==============================] - 0s 270ms/step - loss: 0.6423 - accuracy: 0.9067\n","Epoch 21/121\n","1/1 [==============================] - 0s 199ms/step - loss: 0.6340 - accuracy: 0.9067\n","Epoch 22/121\n","1/1 [==============================] - 0s 249ms/step - loss: 0.5977 - accuracy: 0.9067\n","Epoch 23/121\n","1/1 [==============================] - 0s 232ms/step - loss: 0.5772 - accuracy: 0.9067\n","Epoch 24/121\n","1/1 [==============================] - 0s 203ms/step - loss: 0.5647 - accuracy: 0.9067\n","Epoch 25/121\n","1/1 [==============================] - 0s 280ms/step - loss: 0.5559 - accuracy: 0.9067\n","Epoch 26/121\n","1/1 [==============================] - 0s 208ms/step - loss: 0.5485 - accuracy: 0.9067\n","Epoch 27/121\n","1/1 [==============================] - 0s 233ms/step - loss: 0.5428 - accuracy: 0.9067\n","Epoch 28/121\n","1/1 [==============================] - 0s 245ms/step - loss: 0.5375 - accuracy: 0.9067\n","Epoch 29/121\n","1/1 [==============================] - 0s 264ms/step - loss: 0.5330 - accuracy: 0.9067\n","Epoch 30/121\n","1/1 [==============================] - 0s 182ms/step - loss: 0.5290 - accuracy: 0.9067\n","Epoch 31/121\n","1/1 [==============================] - 0s 231ms/step - loss: 0.5258 - accuracy: 0.9067\n","Epoch 32/121\n","1/1 [==============================] - 0s 188ms/step - loss: 0.5236 - accuracy: 0.9067\n","Epoch 33/121\n","1/1 [==============================] - 0s 206ms/step - loss: 0.5224 - accuracy: 0.9067\n","Epoch 34/121\n","1/1 [==============================] - 0s 210ms/step - loss: 0.5224 - accuracy: 0.9067\n","Epoch 35/121\n","1/1 [==============================] - 0s 282ms/step - loss: 0.5247 - accuracy: 0.9067\n","Epoch 36/121\n","1/1 [==============================] - 0s 271ms/step - loss: 0.5265 - accuracy: 0.9067\n","Epoch 37/121\n","1/1 [==============================] - 0s 299ms/step - loss: 0.5174 - accuracy: 0.9067\n","Epoch 38/121\n","1/1 [==============================] - 0s 191ms/step - loss: 0.5142 - accuracy: 0.9067\n","Epoch 39/121\n","1/1 [==============================] - 0s 263ms/step - loss: 0.5121 - accuracy: 0.9067\n","Epoch 40/121\n","1/1 [==============================] - 0s 257ms/step - loss: 0.5104 - accuracy: 0.9067\n","Epoch 41/121\n","1/1 [==============================] - 0s 209ms/step - loss: 0.5088 - accuracy: 0.9067\n","Epoch 42/121\n","1/1 [==============================] - 0s 208ms/step - loss: 0.5073 - accuracy: 0.9067\n","Epoch 43/121\n","1/1 [==============================] - 0s 193ms/step - loss: 0.5054 - accuracy: 0.9067\n","Epoch 44/121\n","1/1 [==============================] - 0s 197ms/step - loss: 0.5035 - accuracy: 0.9067\n","Epoch 45/121\n","1/1 [==============================] - 0s 168ms/step - loss: 0.5017 - accuracy: 0.9067\n","Epoch 46/121\n","1/1 [==============================] - 0s 208ms/step - loss: 0.4999 - accuracy: 0.9067\n","Epoch 47/121\n","1/1 [==============================] - 0s 159ms/step - loss: 0.4984 - accuracy: 0.9067\n","Epoch 48/121\n","1/1 [==============================] - 0s 180ms/step - loss: 0.4971 - accuracy: 0.9067\n","Epoch 49/121\n","1/1 [==============================] - 0s 160ms/step - loss: 0.4956 - accuracy: 0.9067\n","Epoch 50/121\n","1/1 [==============================] - 0s 166ms/step - loss: 0.4939 - accuracy: 0.9067\n","Epoch 51/121\n","1/1 [==============================] - 0s 168ms/step - loss: 0.4921 - accuracy: 0.9067\n","Epoch 52/121\n","1/1 [==============================] - 0s 166ms/step - loss: 0.4901 - accuracy: 0.9067\n","Epoch 53/121\n","1/1 [==============================] - 0s 170ms/step - loss: 0.4881 - accuracy: 0.9067\n","Epoch 54/121\n","1/1 [==============================] - 0s 173ms/step - loss: 0.4862 - accuracy: 0.9067\n","Epoch 55/121\n","1/1 [==============================] - 0s 177ms/step - loss: 0.4841 - accuracy: 0.9067\n","Epoch 56/121\n","1/1 [==============================] - 0s 166ms/step - loss: 0.4819 - accuracy: 0.9067\n","Epoch 57/121\n","1/1 [==============================] - 0s 179ms/step - loss: 0.4796 - accuracy: 0.9067\n","Epoch 58/121\n","1/1 [==============================] - 0s 165ms/step - loss: 0.4772 - accuracy: 0.9067\n","Epoch 59/121\n","1/1 [==============================] - 0s 164ms/step - loss: 0.4747 - accuracy: 0.9087\n","Epoch 60/121\n","1/1 [==============================] - 0s 168ms/step - loss: 0.4721 - accuracy: 0.9087\n","Epoch 61/121\n","1/1 [==============================] - 0s 171ms/step - loss: 0.4695 - accuracy: 0.9147\n","Epoch 62/121\n","1/1 [==============================] - 0s 172ms/step - loss: 0.4669 - accuracy: 0.9127\n","Epoch 63/121\n","1/1 [==============================] - 0s 172ms/step - loss: 0.4642 - accuracy: 0.9127\n","Epoch 64/121\n","1/1 [==============================] - 0s 172ms/step - loss: 0.4613 - accuracy: 0.9127\n","Epoch 65/121\n","1/1 [==============================] - 0s 187ms/step - loss: 0.4583 - accuracy: 0.9206\n","Epoch 66/121\n","1/1 [==============================] - 0s 158ms/step - loss: 0.4554 - accuracy: 0.9187\n","Epoch 67/121\n","1/1 [==============================] - 0s 165ms/step - loss: 0.4532 - accuracy: 0.9187\n","Epoch 68/121\n","1/1 [==============================] - 0s 162ms/step - loss: 0.4518 - accuracy: 0.9167\n","Epoch 69/121\n","1/1 [==============================] - 0s 169ms/step - loss: 0.4483 - accuracy: 0.9226\n","Epoch 70/121\n","1/1 [==============================] - 0s 164ms/step - loss: 0.4457 - accuracy: 0.9226\n","Epoch 71/121\n","1/1 [==============================] - 0s 259ms/step - loss: 0.4439 - accuracy: 0.9226\n","Epoch 72/121\n","1/1 [==============================] - 0s 183ms/step - loss: 0.4420 - accuracy: 0.9226\n","Epoch 73/121\n","1/1 [==============================] - 0s 174ms/step - loss: 0.4395 - accuracy: 0.9226\n","Epoch 74/121\n","1/1 [==============================] - 0s 182ms/step - loss: 0.4367 - accuracy: 0.9266\n","Epoch 75/121\n","1/1 [==============================] - 0s 189ms/step - loss: 0.4338 - accuracy: 0.9266\n","Epoch 76/121\n","1/1 [==============================] - 0s 187ms/step - loss: 0.4315 - accuracy: 0.9187\n","Epoch 77/121\n","1/1 [==============================] - 0s 199ms/step - loss: 0.4304 - accuracy: 0.9187\n","Epoch 78/121\n","1/1 [==============================] - 0s 204ms/step - loss: 0.4270 - accuracy: 0.9187\n","Epoch 79/121\n","1/1 [==============================] - 0s 183ms/step - loss: 0.4252 - accuracy: 0.9206\n","Epoch 80/121\n","1/1 [==============================] - 0s 190ms/step - loss: 0.4227 - accuracy: 0.9226\n","Epoch 81/121\n","1/1 [==============================] - 0s 186ms/step - loss: 0.4192 - accuracy: 0.9246\n","Epoch 82/121\n","1/1 [==============================] - 0s 165ms/step - loss: nan - accuracy: 0.9246\n","Epoch 83/121\n","1/1 [==============================] - 0s 168ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/121\n","1/1 [==============================] - 0s 179ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/121\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/121\n","1/1 [==============================] - 0s 165ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/121\n","1/1 [==============================] - 0s 179ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/121\n","1/1 [==============================] - 0s 191ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/121\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/121\n","1/1 [==============================] - 0s 187ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/121\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/121\n","1/1 [==============================] - 0s 190ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/121\n","1/1 [==============================] - 0s 222ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/121\n","1/1 [==============================] - 0s 172ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/121\n","1/1 [==============================] - 0s 267ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/121\n","1/1 [==============================] - 0s 280ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/121\n","1/1 [==============================] - 0s 173ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/121\n","1/1 [==============================] - 0s 184ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/121\n","1/1 [==============================] - 0s 206ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/121\n","1/1 [==============================] - 0s 164ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/121\n","1/1 [==============================] - 0s 190ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/121\n","1/1 [==============================] - 0s 166ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/121\n","1/1 [==============================] - 0s 161ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/121\n","1/1 [==============================] - 0s 162ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/121\n","1/1 [==============================] - 0s 165ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/121\n","1/1 [==============================] - 0s 234ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/121\n","1/1 [==============================] - 0s 205ms/step - loss: nan - accuracy: 0.9067\n","Epoch 108/121\n","1/1 [==============================] - 0s 195ms/step - loss: nan - accuracy: 0.9067\n","Epoch 109/121\n","1/1 [==============================] - 0s 250ms/step - loss: nan - accuracy: 0.9067\n","Epoch 110/121\n","1/1 [==============================] - 0s 317ms/step - loss: nan - accuracy: 0.9067\n","Epoch 111/121\n","1/1 [==============================] - 0s 263ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/121\n","1/1 [==============================] - 0s 201ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/121\n","1/1 [==============================] - 0s 282ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/121\n","1/1 [==============================] - 0s 185ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/121\n","1/1 [==============================] - 0s 203ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/121\n","1/1 [==============================] - 0s 158ms/step - loss: nan - accuracy: 0.9067\n","Epoch 117/121\n","1/1 [==============================] - 0s 253ms/step - loss: nan - accuracy: 0.9067\n","Epoch 118/121\n","1/1 [==============================] - 0s 294ms/step - loss: nan - accuracy: 0.9067\n","Epoch 119/121\n","1/1 [==============================] - 0s 247ms/step - loss: nan - accuracy: 0.9067\n","Epoch 120/121\n","1/1 [==============================] - 0s 298ms/step - loss: nan - accuracy: 0.9067\n","Epoch 121/121\n","1/1 [==============================] - 0s 189ms/step - loss: nan - accuracy: 0.9067\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:50:40,086]\u001b[0m Trial 0 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 191, 'num_epochs': 121}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/23\n","1/1 [==============================] - 5s 5s/step - loss: 3.4190 - accuracy: 0.1984\n","Epoch 2/23\n","1/1 [==============================] - 0s 430ms/step - loss: 1.2884 - accuracy: 0.9067\n","Epoch 3/23\n","1/1 [==============================] - 0s 312ms/step - loss: 1.1813 - accuracy: 0.9067\n","Epoch 4/23\n","1/1 [==============================] - 0s 330ms/step - loss: 1.2140 - accuracy: 0.9048\n","Epoch 5/23\n","1/1 [==============================] - 0s 369ms/step - loss: 1.1731 - accuracy: 0.9028\n","Epoch 6/23\n","1/1 [==============================] - 0s 337ms/step - loss: 1.1056 - accuracy: 0.9048\n","Epoch 7/23\n","1/1 [==============================] - 0s 433ms/step - loss: 1.0596 - accuracy: 0.9067\n","Epoch 8/23\n","1/1 [==============================] - 0s 349ms/step - loss: 1.0134 - accuracy: 0.9067\n","Epoch 9/23\n","1/1 [==============================] - 0s 410ms/step - loss: 0.9844 - accuracy: 0.9067\n","Epoch 10/23\n","1/1 [==============================] - 1s 504ms/step - loss: 1.1330 - accuracy: 0.9067\n","Epoch 11/23\n","1/1 [==============================] - 0s 344ms/step - loss: 1.2097 - accuracy: 0.9067\n","Epoch 12/23\n","1/1 [==============================] - 0s 333ms/step - loss: 1.2301 - accuracy: 0.9067\n","Epoch 13/23\n","1/1 [==============================] - 0s 364ms/step - loss: 1.2362 - accuracy: 0.9067\n","Epoch 14/23\n","1/1 [==============================] - 0s 397ms/step - loss: 1.2191 - accuracy: 0.9067\n","Epoch 15/23\n","1/1 [==============================] - 0s 386ms/step - loss: 1.1926 - accuracy: 0.9067\n","Epoch 16/23\n","1/1 [==============================] - 0s 254ms/step - loss: 1.1650 - accuracy: 0.9067\n","Epoch 17/23\n","1/1 [==============================] - 0s 278ms/step - loss: 1.1359 - accuracy: 0.9067\n","Epoch 18/23\n","1/1 [==============================] - 0s 460ms/step - loss: 1.0837 - accuracy: 0.9067\n","Epoch 19/23\n","1/1 [==============================] - 0s 268ms/step - loss: 1.0282 - accuracy: 0.9067\n","Epoch 20/23\n","1/1 [==============================] - 0s 399ms/step - loss: 1.0251 - accuracy: 0.9067\n","Epoch 21/23\n","1/1 [==============================] - 1s 505ms/step - loss: 0.9800 - accuracy: 0.9067\n","Epoch 22/23\n","1/1 [==============================] - 1s 500ms/step - loss: 0.9787 - accuracy: 0.9067\n","Epoch 23/23\n","1/1 [==============================] - 0s 329ms/step - loss: 0.9834 - accuracy: 0.9067\n","1/1 [==============================] - 2s 2s/step - loss: 1.0699 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:50:55,917]\u001b[0m Trial 1 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 246, 'num_epochs': 23}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","1/1 [==============================] - 5s 5s/step - loss: 11.9267 - accuracy: 0.0000e+00\n","Epoch 2/20\n","1/1 [==============================] - 0s 105ms/step - loss: 1.8673 - accuracy: 0.8552\n","Epoch 3/20\n","1/1 [==============================] - 0s 115ms/step - loss: 1.7334 - accuracy: 0.9067\n","Epoch 4/20\n","1/1 [==============================] - 0s 155ms/step - loss: 1.6509 - accuracy: 0.9067\n","Epoch 5/20\n","1/1 [==============================] - 0s 144ms/step - loss: 1.5963 - accuracy: 0.9067\n","Epoch 6/20\n","1/1 [==============================] - 0s 165ms/step - loss: 1.5384 - accuracy: 0.9067\n","Epoch 7/20\n","1/1 [==============================] - 0s 131ms/step - loss: 1.4801 - accuracy: 0.9067\n","Epoch 8/20\n","1/1 [==============================] - 0s 162ms/step - loss: 1.4287 - accuracy: 0.9067\n","Epoch 9/20\n","1/1 [==============================] - 0s 157ms/step - loss: 1.4061 - accuracy: 0.9067\n","Epoch 10/20\n","1/1 [==============================] - 0s 159ms/step - loss: 1.3914 - accuracy: 0.9067\n","Epoch 11/20\n","1/1 [==============================] - 0s 161ms/step - loss: 1.3623 - accuracy: 0.9067\n","Epoch 12/20\n","1/1 [==============================] - 0s 182ms/step - loss: 1.3319 - accuracy: 0.9067\n","Epoch 13/20\n","1/1 [==============================] - 0s 161ms/step - loss: 1.3094 - accuracy: 0.9067\n","Epoch 14/20\n","1/1 [==============================] - 0s 141ms/step - loss: 1.2906 - accuracy: 0.9067\n","Epoch 15/20\n","1/1 [==============================] - 0s 174ms/step - loss: 1.2769 - accuracy: 0.9067\n","Epoch 16/20\n","1/1 [==============================] - 0s 149ms/step - loss: 1.2721 - accuracy: 0.9067\n","Epoch 17/20\n","1/1 [==============================] - 0s 151ms/step - loss: 1.2712 - accuracy: 0.9067\n","Epoch 18/20\n","1/1 [==============================] - 0s 183ms/step - loss: 1.2487 - accuracy: 0.9067\n","Epoch 19/20\n","1/1 [==============================] - 0s 179ms/step - loss: 1.2224 - accuracy: 0.9067\n","Epoch 20/20\n","1/1 [==============================] - 0s 182ms/step - loss: 1.1883 - accuracy: 0.9067\n","1/1 [==============================] - 1s 1s/step - loss: 1.1500 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:51:05,695]\u001b[0m Trial 2 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 61, 'num_epochs': 20}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/14\n","1/1 [==============================] - 5s 5s/step - loss: 14.4915 - accuracy: 0.0000e+00\n","Epoch 2/14\n","1/1 [==============================] - 0s 247ms/step - loss: 1.3044 - accuracy: 0.9067\n","Epoch 3/14\n","1/1 [==============================] - 0s 378ms/step - loss: 0.7891 - accuracy: 0.9067\n","Epoch 4/14\n","1/1 [==============================] - 0s 270ms/step - loss: 0.6477 - accuracy: 0.9067\n","Epoch 5/14\n","1/1 [==============================] - 0s 476ms/step - loss: 0.7728 - accuracy: 0.9067\n","Epoch 6/14\n","1/1 [==============================] - 0s 374ms/step - loss: 0.6593 - accuracy: 0.9067\n","Epoch 7/14\n","1/1 [==============================] - 0s 304ms/step - loss: 0.6318 - accuracy: 0.9067\n","Epoch 8/14\n","1/1 [==============================] - 0s 261ms/step - loss: 0.6424 - accuracy: 0.9067\n","Epoch 9/14\n","1/1 [==============================] - 0s 251ms/step - loss: 0.6327 - accuracy: 0.9067\n","Epoch 10/14\n","1/1 [==============================] - 0s 208ms/step - loss: 0.6286 - accuracy: 0.9067\n","Epoch 11/14\n","1/1 [==============================] - 0s 225ms/step - loss: 0.6170 - accuracy: 0.9067\n","Epoch 12/14\n","1/1 [==============================] - 0s 259ms/step - loss: 0.6045 - accuracy: 0.9067\n","Epoch 13/14\n","1/1 [==============================] - 0s 236ms/step - loss: 0.6196 - accuracy: 0.9067\n","Epoch 14/14\n","1/1 [==============================] - 0s 255ms/step - loss: 0.6304 - accuracy: 0.9067\n","WARNING:tensorflow:5 out of the last 165 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b1c6e51f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 2s 2s/step - loss: 1.2322 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:51:17,114]\u001b[0m Trial 3 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 254, 'num_epochs': 14}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/137\n","1/1 [==============================] - 5s 5s/step - loss: 15.4420 - accuracy: 0.0040\n","Epoch 2/137\n","1/1 [==============================] - 0s 105ms/step - loss: 2.3814 - accuracy: 0.4544\n","Epoch 3/137\n","1/1 [==============================] - 0s 160ms/step - loss: 1.2281 - accuracy: 0.9067\n","Epoch 4/137\n","1/1 [==============================] - 0s 186ms/step - loss: 1.0006 - accuracy: 0.9067\n","Epoch 5/137\n","1/1 [==============================] - 0s 185ms/step - loss: 0.9646 - accuracy: 0.9067\n","Epoch 6/137\n","1/1 [==============================] - 0s 178ms/step - loss: 0.9732 - accuracy: 0.9067\n","Epoch 7/137\n","1/1 [==============================] - 0s 179ms/step - loss: 1.0037 - accuracy: 0.9067\n","Epoch 8/137\n","1/1 [==============================] - 0s 184ms/step - loss: 1.0098 - accuracy: 0.9067\n","Epoch 9/137\n","1/1 [==============================] - 0s 199ms/step - loss: 1.0045 - accuracy: 0.9067\n","Epoch 10/137\n","1/1 [==============================] - 0s 152ms/step - loss: 0.9938 - accuracy: 0.9067\n","Epoch 11/137\n","1/1 [==============================] - 0s 164ms/step - loss: 0.9868 - accuracy: 0.9067\n","Epoch 12/137\n","1/1 [==============================] - 0s 173ms/step - loss: 0.9873 - accuracy: 0.9067\n","Epoch 13/137\n","1/1 [==============================] - 0s 165ms/step - loss: 1.0111 - accuracy: 0.9067\n","Epoch 14/137\n","1/1 [==============================] - 0s 180ms/step - loss: 1.0095 - accuracy: 0.9067\n","Epoch 15/137\n","1/1 [==============================] - 0s 187ms/step - loss: 1.0107 - accuracy: 0.9067\n","Epoch 16/137\n","1/1 [==============================] - 0s 141ms/step - loss: 1.0120 - accuracy: 0.9067\n","Epoch 17/137\n","1/1 [==============================] - 0s 163ms/step - loss: 1.0103 - accuracy: 0.9067\n","Epoch 18/137\n","1/1 [==============================] - 0s 144ms/step - loss: 1.0075 - accuracy: 0.9067\n","Epoch 19/137\n","1/1 [==============================] - 0s 148ms/step - loss: 1.0063 - accuracy: 0.9067\n","Epoch 20/137\n","1/1 [==============================] - 0s 124ms/step - loss: 1.0059 - accuracy: 0.9067\n","Epoch 21/137\n","1/1 [==============================] - 0s 137ms/step - loss: 1.0040 - accuracy: 0.9067\n","Epoch 22/137\n","1/1 [==============================] - 0s 122ms/step - loss: 1.0027 - accuracy: 0.9067\n","Epoch 23/137\n","1/1 [==============================] - 0s 147ms/step - loss: 1.0023 - accuracy: 0.9067\n","Epoch 24/137\n","1/1 [==============================] - 0s 153ms/step - loss: 0.9802 - accuracy: 0.9067\n","Epoch 25/137\n","1/1 [==============================] - 0s 157ms/step - loss: 0.9758 - accuracy: 0.9067\n","Epoch 26/137\n","1/1 [==============================] - 0s 169ms/step - loss: 0.9766 - accuracy: 0.9067\n","Epoch 27/137\n","1/1 [==============================] - 0s 186ms/step - loss: 0.9790 - accuracy: 0.9067\n","Epoch 28/137\n","1/1 [==============================] - 0s 139ms/step - loss: 0.9800 - accuracy: 0.9067\n","Epoch 29/137\n","1/1 [==============================] - 0s 156ms/step - loss: 0.9785 - accuracy: 0.9067\n","Epoch 30/137\n","1/1 [==============================] - 0s 162ms/step - loss: 0.9756 - accuracy: 0.9067\n","Epoch 31/137\n","1/1 [==============================] - 0s 170ms/step - loss: 0.9725 - accuracy: 0.9067\n","Epoch 32/137\n","1/1 [==============================] - 0s 180ms/step - loss: 0.9697 - accuracy: 0.9067\n","Epoch 33/137\n","1/1 [==============================] - 0s 183ms/step - loss: 0.9676 - accuracy: 0.9067\n","Epoch 34/137\n","1/1 [==============================] - 0s 175ms/step - loss: 0.9668 - accuracy: 0.9067\n","Epoch 35/137\n","1/1 [==============================] - 0s 181ms/step - loss: 0.9668 - accuracy: 0.9067\n","Epoch 36/137\n","1/1 [==============================] - 0s 174ms/step - loss: 0.9670 - accuracy: 0.9067\n","Epoch 37/137\n","1/1 [==============================] - 0s 206ms/step - loss: 0.9656 - accuracy: 0.9067\n","Epoch 38/137\n","1/1 [==============================] - 0s 173ms/step - loss: 0.9640 - accuracy: 0.9067\n","Epoch 39/137\n","1/1 [==============================] - 0s 164ms/step - loss: 0.9630 - accuracy: 0.9067\n","Epoch 40/137\n","1/1 [==============================] - 0s 188ms/step - loss: 0.9620 - accuracy: 0.9067\n","Epoch 41/137\n","1/1 [==============================] - 0s 168ms/step - loss: 0.9612 - accuracy: 0.9067\n","Epoch 42/137\n","1/1 [==============================] - 0s 181ms/step - loss: 0.9606 - accuracy: 0.9067\n","Epoch 43/137\n","1/1 [==============================] - 0s 168ms/step - loss: 0.9600 - accuracy: 0.9067\n","Epoch 44/137\n","1/1 [==============================] - 0s 260ms/step - loss: 0.9592 - accuracy: 0.9067\n","Epoch 45/137\n","1/1 [==============================] - 0s 176ms/step - loss: 0.9584 - accuracy: 0.9067\n","Epoch 46/137\n","1/1 [==============================] - 0s 204ms/step - loss: 0.9577 - accuracy: 0.9067\n","Epoch 47/137\n","1/1 [==============================] - 0s 181ms/step - loss: 0.9567 - accuracy: 0.9067\n","Epoch 48/137\n","1/1 [==============================] - 0s 146ms/step - loss: 0.9554 - accuracy: 0.9067\n","Epoch 49/137\n","1/1 [==============================] - 0s 154ms/step - loss: 0.9543 - accuracy: 0.9067\n","Epoch 50/137\n","1/1 [==============================] - 0s 155ms/step - loss: 0.9304 - accuracy: 0.9067\n","Epoch 51/137\n","1/1 [==============================] - 0s 148ms/step - loss: 0.9256 - accuracy: 0.9087\n","Epoch 52/137\n","1/1 [==============================] - 0s 118ms/step - loss: 0.9499 - accuracy: 0.9107\n","Epoch 53/137\n","1/1 [==============================] - 0s 124ms/step - loss: 0.9355 - accuracy: 0.9107\n","Epoch 54/137\n","1/1 [==============================] - 0s 132ms/step - loss: 0.9222 - accuracy: 0.9107\n","Epoch 55/137\n","1/1 [==============================] - 0s 129ms/step - loss: 0.9199 - accuracy: 0.9107\n","Epoch 56/137\n","1/1 [==============================] - 0s 144ms/step - loss: 0.9224 - accuracy: 0.9107\n","Epoch 57/137\n","1/1 [==============================] - 0s 143ms/step - loss: 0.9466 - accuracy: 0.9107\n","Epoch 58/137\n","1/1 [==============================] - 0s 169ms/step - loss: 0.9472 - accuracy: 0.9107\n","Epoch 59/137\n","1/1 [==============================] - 0s 170ms/step - loss: 0.9453 - accuracy: 0.9107\n","Epoch 60/137\n","1/1 [==============================] - 0s 185ms/step - loss: 0.9439 - accuracy: 0.9087\n","Epoch 61/137\n","1/1 [==============================] - 0s 179ms/step - loss: 0.9435 - accuracy: 0.9107\n","Epoch 62/137\n","1/1 [==============================] - 0s 159ms/step - loss: 0.9414 - accuracy: 0.9107\n","Epoch 63/137\n","1/1 [==============================] - 0s 154ms/step - loss: 0.9411 - accuracy: 0.9067\n","Epoch 64/137\n","1/1 [==============================] - 0s 141ms/step - loss: 0.9387 - accuracy: 0.9087\n","Epoch 65/137\n","1/1 [==============================] - 0s 175ms/step - loss: 0.9372 - accuracy: 0.9107\n","Epoch 66/137\n","1/1 [==============================] - 0s 234ms/step - loss: 0.9350 - accuracy: 0.9107\n","Epoch 67/137\n","1/1 [==============================] - 0s 167ms/step - loss: 0.9341 - accuracy: 0.9087\n","Epoch 68/137\n","1/1 [==============================] - 0s 196ms/step - loss: nan - accuracy: 0.9067\n","Epoch 69/137\n","1/1 [==============================] - 0s 157ms/step - loss: nan - accuracy: 0.9067\n","Epoch 70/137\n","1/1 [==============================] - 0s 139ms/step - loss: nan - accuracy: 0.9067\n","Epoch 71/137\n","1/1 [==============================] - 0s 147ms/step - loss: nan - accuracy: 0.9067\n","Epoch 72/137\n","1/1 [==============================] - 0s 118ms/step - loss: nan - accuracy: 0.9067\n","Epoch 73/137\n","1/1 [==============================] - 0s 161ms/step - loss: nan - accuracy: 0.9067\n","Epoch 74/137\n","1/1 [==============================] - 0s 166ms/step - loss: nan - accuracy: 0.9067\n","Epoch 75/137\n","1/1 [==============================] - 0s 165ms/step - loss: nan - accuracy: 0.9067\n","Epoch 76/137\n","1/1 [==============================] - 0s 166ms/step - loss: nan - accuracy: 0.9067\n","Epoch 77/137\n","1/1 [==============================] - 0s 172ms/step - loss: nan - accuracy: 0.9067\n","Epoch 78/137\n","1/1 [==============================] - 0s 194ms/step - loss: nan - accuracy: 0.9067\n","Epoch 79/137\n","1/1 [==============================] - 0s 150ms/step - loss: nan - accuracy: 0.9067\n","Epoch 80/137\n","1/1 [==============================] - 0s 171ms/step - loss: nan - accuracy: 0.9067\n","Epoch 81/137\n","1/1 [==============================] - 0s 256ms/step - loss: nan - accuracy: 0.9067\n","Epoch 82/137\n","1/1 [==============================] - 0s 156ms/step - loss: nan - accuracy: 0.9067\n","Epoch 83/137\n","1/1 [==============================] - 0s 156ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/137\n","1/1 [==============================] - 0s 167ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/137\n","1/1 [==============================] - 0s 158ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/137\n","1/1 [==============================] - 0s 159ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/137\n","1/1 [==============================] - 0s 163ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/137\n","1/1 [==============================] - 0s 156ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/137\n","1/1 [==============================] - 0s 150ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/137\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/137\n","1/1 [==============================] - 0s 156ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/137\n","1/1 [==============================] - 0s 168ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/137\n","1/1 [==============================] - 0s 153ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/137\n","1/1 [==============================] - 0s 155ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/137\n","1/1 [==============================] - 0s 153ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/137\n","1/1 [==============================] - 0s 138ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/137\n","1/1 [==============================] - 0s 147ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/137\n","1/1 [==============================] - 0s 175ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/137\n","1/1 [==============================] - 0s 159ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/137\n","1/1 [==============================] - 0s 258ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/137\n","1/1 [==============================] - 0s 162ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/137\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/137\n","1/1 [==============================] - 0s 177ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/137\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/137\n","1/1 [==============================] - 0s 159ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/137\n","1/1 [==============================] - 0s 155ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/137\n","1/1 [==============================] - 0s 154ms/step - loss: nan - accuracy: 0.9067\n","Epoch 108/137\n","1/1 [==============================] - 0s 146ms/step - loss: nan - accuracy: 0.9067\n","Epoch 109/137\n","1/1 [==============================] - 0s 170ms/step - loss: nan - accuracy: 0.9067\n","Epoch 110/137\n","1/1 [==============================] - 0s 237ms/step - loss: nan - accuracy: 0.9067\n","Epoch 111/137\n","1/1 [==============================] - 0s 194ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/137\n","1/1 [==============================] - 0s 217ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/137\n","1/1 [==============================] - 0s 202ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/137\n","1/1 [==============================] - 0s 226ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/137\n","1/1 [==============================] - 0s 193ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/137\n","1/1 [==============================] - 0s 218ms/step - loss: nan - accuracy: 0.9067\n","Epoch 117/137\n","1/1 [==============================] - 0s 214ms/step - loss: nan - accuracy: 0.9067\n","Epoch 118/137\n","1/1 [==============================] - 0s 235ms/step - loss: nan - accuracy: 0.9067\n","Epoch 119/137\n","1/1 [==============================] - 0s 227ms/step - loss: nan - accuracy: 0.9067\n","Epoch 120/137\n","1/1 [==============================] - 0s 184ms/step - loss: nan - accuracy: 0.9067\n","Epoch 121/137\n","1/1 [==============================] - 0s 226ms/step - loss: nan - accuracy: 0.9067\n","Epoch 122/137\n","1/1 [==============================] - 0s 237ms/step - loss: nan - accuracy: 0.9067\n","Epoch 123/137\n","1/1 [==============================] - 0s 238ms/step - loss: nan - accuracy: 0.9067\n","Epoch 124/137\n","1/1 [==============================] - 0s 254ms/step - loss: nan - accuracy: 0.9067\n","Epoch 125/137\n","1/1 [==============================] - 0s 215ms/step - loss: nan - accuracy: 0.9067\n","Epoch 126/137\n","1/1 [==============================] - 0s 249ms/step - loss: nan - accuracy: 0.9067\n","Epoch 127/137\n","1/1 [==============================] - 0s 258ms/step - loss: nan - accuracy: 0.9067\n","Epoch 128/137\n","1/1 [==============================] - 0s 249ms/step - loss: nan - accuracy: 0.9067\n","Epoch 129/137\n","1/1 [==============================] - 0s 259ms/step - loss: nan - accuracy: 0.9067\n","Epoch 130/137\n","1/1 [==============================] - 0s 247ms/step - loss: nan - accuracy: 0.9067\n","Epoch 131/137\n","1/1 [==============================] - 0s 246ms/step - loss: nan - accuracy: 0.9067\n","Epoch 132/137\n","1/1 [==============================] - 0s 261ms/step - loss: nan - accuracy: 0.9067\n","Epoch 133/137\n","1/1 [==============================] - 0s 168ms/step - loss: nan - accuracy: 0.9067\n","Epoch 134/137\n","1/1 [==============================] - 0s 142ms/step - loss: nan - accuracy: 0.9067\n","Epoch 135/137\n","1/1 [==============================] - 0s 142ms/step - loss: nan - accuracy: 0.9067\n","Epoch 136/137\n","1/1 [==============================] - 0s 178ms/step - loss: nan - accuracy: 0.9067\n","Epoch 137/137\n","1/1 [==============================] - 0s 222ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:6 out of the last 166 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b24c06700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:51:48,524]\u001b[0m Trial 4 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 130, 'num_epochs': 137}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1/1 [==============================] - 6s 6s/step - loss: 10.3225 - accuracy: 0.0079\n","Epoch 2/10\n","1/1 [==============================] - 1s 791ms/step - loss: 1.6114 - accuracy: 0.9067\n","Epoch 3/10\n","1/1 [==============================] - 0s 190ms/step - loss: 1.3023 - accuracy: 0.9067\n","Epoch 4/10\n","1/1 [==============================] - 0s 264ms/step - loss: 1.1598 - accuracy: 0.9067\n","Epoch 5/10\n","1/1 [==============================] - 1s 543ms/step - loss: 1.1372 - accuracy: 0.9067\n","Epoch 6/10\n","1/1 [==============================] - 0s 287ms/step - loss: 1.1027 - accuracy: 0.9067\n","Epoch 7/10\n","1/1 [==============================] - 0s 361ms/step - loss: 1.1176 - accuracy: 0.9067\n","Epoch 8/10\n","1/1 [==============================] - 0s 391ms/step - loss: 1.0731 - accuracy: 0.9067\n","Epoch 9/10\n","1/1 [==============================] - 0s 310ms/step - loss: 1.0384 - accuracy: 0.9067\n","Epoch 10/10\n","1/1 [==============================] - 1s 602ms/step - loss: 1.0241 - accuracy: 0.9067\n","WARNING:tensorflow:7 out of the last 167 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b44d8cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.2550 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:51:59,842]\u001b[0m Trial 5 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 197, 'num_epochs': 10}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/110\n","1/1 [==============================] - 5s 5s/step - loss: 14.6629 - accuracy: 0.0000e+00\n","Epoch 2/110\n","1/1 [==============================] - 0s 148ms/step - loss: 1.7229 - accuracy: 0.8274\n","Epoch 3/110\n","1/1 [==============================] - 0s 146ms/step - loss: 1.2792 - accuracy: 0.9067\n","Epoch 4/110\n","1/1 [==============================] - 0s 154ms/step - loss: 1.0929 - accuracy: 0.9067\n","Epoch 5/110\n","1/1 [==============================] - 0s 169ms/step - loss: 1.0816 - accuracy: 0.9067\n","Epoch 6/110\n","1/1 [==============================] - 0s 158ms/step - loss: 1.1724 - accuracy: 0.9067\n","Epoch 7/110\n","1/1 [==============================] - 0s 168ms/step - loss: 1.2186 - accuracy: 0.9048\n","Epoch 8/110\n","1/1 [==============================] - 0s 159ms/step - loss: 1.2231 - accuracy: 0.8948\n","Epoch 9/110\n","1/1 [==============================] - 0s 172ms/step - loss: 1.1917 - accuracy: 0.8829\n","Epoch 10/110\n","1/1 [==============================] - 0s 164ms/step - loss: 1.1955 - accuracy: 0.8433\n","Epoch 11/110\n","1/1 [==============================] - 0s 166ms/step - loss: 1.1955 - accuracy: 0.8135\n","Epoch 12/110\n","1/1 [==============================] - 0s 165ms/step - loss: 1.1715 - accuracy: 0.7897\n","Epoch 13/110\n","1/1 [==============================] - 0s 172ms/step - loss: 1.1314 - accuracy: 0.7679\n","Epoch 14/110\n","1/1 [==============================] - 0s 172ms/step - loss: 1.1152 - accuracy: 0.7579\n","Epoch 15/110\n","1/1 [==============================] - 0s 221ms/step - loss: 1.0883 - accuracy: 0.7520\n","Epoch 16/110\n","1/1 [==============================] - 0s 192ms/step - loss: 1.0738 - accuracy: 0.7421\n","Epoch 17/110\n","1/1 [==============================] - 0s 170ms/step - loss: 1.0737 - accuracy: 0.7262\n","Epoch 18/110\n","1/1 [==============================] - 0s 191ms/step - loss: 1.0740 - accuracy: 0.7242\n","Epoch 19/110\n","1/1 [==============================] - 0s 169ms/step - loss: 1.0717 - accuracy: 0.7222\n","Epoch 20/110\n","1/1 [==============================] - 0s 158ms/step - loss: 1.0663 - accuracy: 0.7262\n","Epoch 21/110\n","1/1 [==============================] - 0s 160ms/step - loss: 1.0576 - accuracy: 0.7321\n","Epoch 22/110\n","1/1 [==============================] - 0s 166ms/step - loss: 1.0456 - accuracy: 0.7361\n","Epoch 23/110\n","1/1 [==============================] - 0s 172ms/step - loss: 1.0306 - accuracy: 0.7421\n","Epoch 24/110\n","1/1 [==============================] - 0s 144ms/step - loss: 1.0127 - accuracy: 0.7599\n","Epoch 25/110\n","1/1 [==============================] - 0s 158ms/step - loss: 0.9918 - accuracy: 0.7698\n","Epoch 26/110\n","1/1 [==============================] - 0s 142ms/step - loss: 0.9681 - accuracy: 0.7837\n","Epoch 27/110\n","1/1 [==============================] - 0s 146ms/step - loss: 0.9429 - accuracy: 0.8056\n","Epoch 28/110\n","1/1 [==============================] - 0s 145ms/step - loss: 0.9173 - accuracy: 0.8274\n","Epoch 29/110\n","1/1 [==============================] - 0s 136ms/step - loss: 0.8912 - accuracy: 0.8433\n","Epoch 30/110\n","1/1 [==============================] - 0s 136ms/step - loss: 0.8652 - accuracy: 0.8512\n","Epoch 31/110\n","1/1 [==============================] - 0s 129ms/step - loss: 0.8396 - accuracy: 0.8552\n","Epoch 32/110\n","1/1 [==============================] - 0s 160ms/step - loss: 0.8147 - accuracy: 0.8671\n","Epoch 33/110\n","1/1 [==============================] - 0s 167ms/step - loss: 0.7913 - accuracy: 0.8710\n","Epoch 34/110\n","1/1 [==============================] - 0s 221ms/step - loss: 0.7745 - accuracy: 0.8770\n","Epoch 35/110\n","1/1 [==============================] - 0s 130ms/step - loss: 0.7467 - accuracy: 0.8849\n","Epoch 36/110\n","1/1 [==============================] - 0s 184ms/step - loss: 0.7260 - accuracy: 0.8849\n","Epoch 37/110\n","1/1 [==============================] - 0s 167ms/step - loss: 0.7071 - accuracy: 0.8889\n","Epoch 38/110\n","1/1 [==============================] - 0s 158ms/step - loss: 0.6894 - accuracy: 0.8909\n","Epoch 39/110\n","1/1 [==============================] - 0s 166ms/step - loss: 0.6730 - accuracy: 0.8909\n","Epoch 40/110\n","1/1 [==============================] - 0s 173ms/step - loss: 0.6577 - accuracy: 0.8909\n","Epoch 41/110\n","1/1 [==============================] - 0s 162ms/step - loss: 0.6434 - accuracy: 0.8909\n","Epoch 42/110\n","1/1 [==============================] - 0s 161ms/step - loss: 0.6296 - accuracy: 0.8929\n","Epoch 43/110\n","1/1 [==============================] - 0s 137ms/step - loss: 0.6163 - accuracy: 0.8929\n","Epoch 44/110\n","1/1 [==============================] - 0s 154ms/step - loss: 0.6035 - accuracy: 0.8948\n","Epoch 45/110\n","1/1 [==============================] - 0s 121ms/step - loss: 0.5912 - accuracy: 0.8948\n","Epoch 46/110\n","1/1 [==============================] - 0s 138ms/step - loss: 0.5796 - accuracy: 0.8948\n","Epoch 47/110\n","1/1 [==============================] - 0s 163ms/step - loss: 0.5685 - accuracy: 0.8968\n","Epoch 48/110\n","1/1 [==============================] - 0s 160ms/step - loss: 0.5579 - accuracy: 0.8968\n","Epoch 49/110\n","1/1 [==============================] - 0s 165ms/step - loss: 0.5483 - accuracy: 0.8968\n","Epoch 50/110\n","1/1 [==============================] - 0s 164ms/step - loss: 0.5401 - accuracy: 0.8968\n","Epoch 51/110\n","1/1 [==============================] - 0s 148ms/step - loss: 0.5332 - accuracy: 0.8988\n","Epoch 52/110\n","1/1 [==============================] - 0s 136ms/step - loss: 0.5277 - accuracy: 0.8988\n","Epoch 53/110\n","1/1 [==============================] - 0s 138ms/step - loss: 0.5238 - accuracy: 0.8988\n","Epoch 54/110\n","1/1 [==============================] - 0s 155ms/step - loss: 0.5200 - accuracy: 0.8988\n","Epoch 55/110\n","1/1 [==============================] - 0s 130ms/step - loss: 0.5151 - accuracy: 0.9008\n","Epoch 56/110\n","1/1 [==============================] - 0s 205ms/step - loss: 0.5107 - accuracy: 0.9008\n","Epoch 57/110\n","1/1 [==============================] - 0s 134ms/step - loss: 0.5075 - accuracy: 0.9008\n","Epoch 58/110\n","1/1 [==============================] - 0s 147ms/step - loss: 0.5048 - accuracy: 0.9008\n","Epoch 59/110\n","1/1 [==============================] - 0s 149ms/step - loss: 0.5022 - accuracy: 0.9008\n","Epoch 60/110\n","1/1 [==============================] - 0s 127ms/step - loss: 0.4996 - accuracy: 0.9008\n","Epoch 61/110\n","1/1 [==============================] - 0s 134ms/step - loss: 0.4971 - accuracy: 0.9028\n","Epoch 62/110\n","1/1 [==============================] - 0s 145ms/step - loss: 0.4949 - accuracy: 0.9028\n","Epoch 63/110\n","1/1 [==============================] - 0s 135ms/step - loss: 0.4926 - accuracy: 0.9028\n","Epoch 64/110\n","1/1 [==============================] - 0s 139ms/step - loss: 0.4903 - accuracy: 0.9028\n","Epoch 65/110\n","1/1 [==============================] - 0s 124ms/step - loss: 0.4880 - accuracy: 0.9028\n","Epoch 66/110\n","1/1 [==============================] - 0s 143ms/step - loss: 0.4857 - accuracy: 0.9028\n","Epoch 67/110\n","1/1 [==============================] - 0s 144ms/step - loss: 0.4836 - accuracy: 0.9028\n","Epoch 68/110\n","1/1 [==============================] - 0s 148ms/step - loss: 0.4817 - accuracy: 0.9028\n","Epoch 69/110\n","1/1 [==============================] - 0s 143ms/step - loss: 0.4800 - accuracy: 0.9028\n","Epoch 70/110\n","1/1 [==============================] - 0s 151ms/step - loss: 0.4786 - accuracy: 0.9048\n","Epoch 71/110\n","1/1 [==============================] - 0s 149ms/step - loss: 0.4774 - accuracy: 0.9048\n","Epoch 72/110\n","1/1 [==============================] - 0s 135ms/step - loss: 0.4762 - accuracy: 0.9048\n","Epoch 73/110\n","1/1 [==============================] - 0s 128ms/step - loss: 0.4750 - accuracy: 0.9048\n","Epoch 74/110\n","1/1 [==============================] - 0s 155ms/step - loss: 0.4738 - accuracy: 0.9048\n","Epoch 75/110\n","1/1 [==============================] - 0s 147ms/step - loss: 0.4725 - accuracy: 0.9048\n","Epoch 76/110\n","1/1 [==============================] - 0s 135ms/step - loss: 0.4712 - accuracy: 0.9048\n","Epoch 77/110\n","1/1 [==============================] - 0s 156ms/step - loss: 0.4699 - accuracy: 0.9048\n","Epoch 78/110\n","1/1 [==============================] - 0s 133ms/step - loss: 0.4687 - accuracy: 0.9048\n","Epoch 79/110\n","1/1 [==============================] - 0s 146ms/step - loss: 0.4675 - accuracy: 0.9048\n","Epoch 80/110\n","1/1 [==============================] - 0s 146ms/step - loss: 0.4663 - accuracy: 0.9048\n","Epoch 81/110\n","1/1 [==============================] - 0s 152ms/step - loss: 0.4652 - accuracy: 0.9048\n","Epoch 82/110\n","1/1 [==============================] - 0s 162ms/step - loss: 0.4641 - accuracy: 0.9048\n","Epoch 83/110\n","1/1 [==============================] - 0s 190ms/step - loss: 0.4630 - accuracy: 0.9048\n","Epoch 84/110\n","1/1 [==============================] - 0s 133ms/step - loss: 0.4620 - accuracy: 0.9048\n","Epoch 85/110\n","1/1 [==============================] - 0s 144ms/step - loss: 0.4609 - accuracy: 0.9048\n","Epoch 86/110\n","1/1 [==============================] - 0s 128ms/step - loss: 0.4598 - accuracy: 0.9067\n","Epoch 87/110\n","1/1 [==============================] - 0s 149ms/step - loss: 0.4587 - accuracy: 0.9067\n","Epoch 88/110\n","1/1 [==============================] - 0s 143ms/step - loss: 0.4576 - accuracy: 0.9067\n","Epoch 89/110\n","1/1 [==============================] - 0s 150ms/step - loss: 0.4565 - accuracy: 0.9067\n","Epoch 90/110\n","1/1 [==============================] - 0s 159ms/step - loss: 0.4555 - accuracy: 0.9067\n","Epoch 91/110\n","1/1 [==============================] - 0s 139ms/step - loss: 0.4545 - accuracy: 0.9067\n","Epoch 92/110\n","1/1 [==============================] - 0s 142ms/step - loss: 0.4534 - accuracy: 0.9067\n","Epoch 93/110\n","1/1 [==============================] - 0s 129ms/step - loss: 0.4524 - accuracy: 0.9067\n","Epoch 94/110\n","1/1 [==============================] - 0s 219ms/step - loss: 0.4514 - accuracy: 0.9067\n","Epoch 95/110\n","1/1 [==============================] - 0s 117ms/step - loss: 0.4504 - accuracy: 0.9067\n","Epoch 96/110\n","1/1 [==============================] - 0s 153ms/step - loss: 0.4493 - accuracy: 0.9067\n","Epoch 97/110\n","1/1 [==============================] - 0s 194ms/step - loss: 0.4483 - accuracy: 0.9067\n","Epoch 98/110\n","1/1 [==============================] - 0s 148ms/step - loss: 0.4473 - accuracy: 0.9067\n","Epoch 99/110\n","1/1 [==============================] - 0s 170ms/step - loss: 0.4462 - accuracy: 0.9067\n","Epoch 100/110\n","1/1 [==============================] - 0s 163ms/step - loss: 0.4452 - accuracy: 0.9067\n","Epoch 101/110\n","1/1 [==============================] - 0s 164ms/step - loss: 0.4441 - accuracy: 0.9067\n","Epoch 102/110\n","1/1 [==============================] - 0s 148ms/step - loss: 0.4430 - accuracy: 0.9067\n","Epoch 103/110\n","1/1 [==============================] - 0s 167ms/step - loss: 0.4420 - accuracy: 0.9067\n","Epoch 104/110\n","1/1 [==============================] - 0s 155ms/step - loss: 0.4408 - accuracy: 0.9067\n","Epoch 105/110\n","1/1 [==============================] - 0s 157ms/step - loss: 0.4397 - accuracy: 0.9067\n","Epoch 106/110\n","1/1 [==============================] - 0s 171ms/step - loss: 0.4385 - accuracy: 0.9067\n","Epoch 107/110\n","1/1 [==============================] - 0s 213ms/step - loss: 0.4372 - accuracy: 0.9067\n","Epoch 108/110\n","1/1 [==============================] - 0s 144ms/step - loss: 0.4359 - accuracy: 0.9067\n","Epoch 109/110\n","1/1 [==============================] - 0s 190ms/step - loss: 0.4343 - accuracy: 0.9087\n","Epoch 110/110\n","1/1 [==============================] - 0s 191ms/step - loss: 0.4323 - accuracy: 0.9087\n","WARNING:tensorflow:8 out of the last 168 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b24cbd430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1168 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:52:23,971]\u001b[0m Trial 6 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 87, 'num_epochs': 110}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","1/1 [==============================] - 6s 6s/step - loss: 15.4347 - accuracy: 0.0079\n","Epoch 2/15\n","1/1 [==============================] - 0s 133ms/step - loss: 11.6459 - accuracy: 0.0694\n","Epoch 3/15\n","1/1 [==============================] - 0s 170ms/step - loss: 1.5875 - accuracy: 0.1270\n","Epoch 4/15\n","1/1 [==============================] - 0s 153ms/step - loss: 1.2191 - accuracy: 0.8472\n","Epoch 5/15\n","1/1 [==============================] - 0s 163ms/step - loss: 1.0277 - accuracy: 0.9067\n","Epoch 6/15\n","1/1 [==============================] - 0s 178ms/step - loss: 1.0529 - accuracy: 0.9067\n","Epoch 7/15\n","1/1 [==============================] - 0s 181ms/step - loss: 0.9824 - accuracy: 0.9067\n","Epoch 8/15\n","1/1 [==============================] - 0s 211ms/step - loss: 0.9127 - accuracy: 0.9067\n","Epoch 9/15\n","1/1 [==============================] - 0s 155ms/step - loss: 0.8484 - accuracy: 0.9067\n","Epoch 10/15\n","1/1 [==============================] - 0s 185ms/step - loss: 0.8122 - accuracy: 0.9067\n","Epoch 11/15\n","1/1 [==============================] - 0s 176ms/step - loss: 0.8209 - accuracy: 0.9067\n","Epoch 12/15\n","1/1 [==============================] - 0s 192ms/step - loss: 0.8759 - accuracy: 0.9067\n","Epoch 13/15\n","1/1 [==============================] - 0s 184ms/step - loss: 0.8759 - accuracy: 0.9067\n","Epoch 14/15\n","1/1 [==============================] - 0s 192ms/step - loss: 0.8270 - accuracy: 0.9067\n","Epoch 15/15\n","1/1 [==============================] - 0s 177ms/step - loss: 0.7692 - accuracy: 0.9067\n","WARNING:tensorflow:9 out of the last 169 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b24cbd700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.2016 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:52:34,208]\u001b[0m Trial 7 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 145, 'num_epochs': 15}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","1/1 [==============================] - 5s 5s/step - loss: 14.0342 - accuracy: 0.0119\n","Epoch 2/50\n","1/1 [==============================] - 0s 109ms/step - loss: 1.8068 - accuracy: 0.8631\n","Epoch 3/50\n","1/1 [==============================] - 0s 155ms/step - loss: 1.4010 - accuracy: 0.9067\n","Epoch 4/50\n","1/1 [==============================] - 0s 165ms/step - loss: 1.2943 - accuracy: 0.9067\n","Epoch 5/50\n","1/1 [==============================] - 0s 133ms/step - loss: 1.2720 - accuracy: 0.9067\n","Epoch 6/50\n","1/1 [==============================] - 0s 166ms/step - loss: 1.2318 - accuracy: 0.9067\n","Epoch 7/50\n","1/1 [==============================] - 0s 169ms/step - loss: 1.1571 - accuracy: 0.9067\n","Epoch 8/50\n","1/1 [==============================] - 0s 141ms/step - loss: 1.1538 - accuracy: 0.9067\n","Epoch 9/50\n","1/1 [==============================] - 0s 154ms/step - loss: 1.1222 - accuracy: 0.9067\n","Epoch 10/50\n","1/1 [==============================] - 0s 160ms/step - loss: 1.1078 - accuracy: 0.9067\n","Epoch 11/50\n","1/1 [==============================] - 0s 148ms/step - loss: 1.1023 - accuracy: 0.9067\n","Epoch 12/50\n","1/1 [==============================] - 0s 152ms/step - loss: 1.1019 - accuracy: 0.9067\n","Epoch 13/50\n","1/1 [==============================] - 0s 125ms/step - loss: 1.1023 - accuracy: 0.9067\n","Epoch 14/50\n","1/1 [==============================] - 0s 127ms/step - loss: 1.1015 - accuracy: 0.9067\n","Epoch 15/50\n","1/1 [==============================] - 0s 122ms/step - loss: 1.0993 - accuracy: 0.9067\n","Epoch 16/50\n","1/1 [==============================] - 0s 132ms/step - loss: 1.0969 - accuracy: 0.9067\n","Epoch 17/50\n","1/1 [==============================] - 0s 140ms/step - loss: 1.0952 - accuracy: 0.9067\n","Epoch 18/50\n","1/1 [==============================] - 0s 131ms/step - loss: 1.0948 - accuracy: 0.9067\n","Epoch 19/50\n","1/1 [==============================] - 0s 132ms/step - loss: 1.0943 - accuracy: 0.9067\n","Epoch 20/50\n","1/1 [==============================] - 0s 121ms/step - loss: 1.0931 - accuracy: 0.9067\n","Epoch 21/50\n","1/1 [==============================] - 0s 126ms/step - loss: 1.0915 - accuracy: 0.9067\n","Epoch 22/50\n","1/1 [==============================] - 0s 148ms/step - loss: 1.0903 - accuracy: 0.9067\n","Epoch 23/50\n","1/1 [==============================] - 0s 108ms/step - loss: 1.0896 - accuracy: 0.9067\n","Epoch 24/50\n","1/1 [==============================] - 0s 112ms/step - loss: 1.0892 - accuracy: 0.9067\n","Epoch 25/50\n","1/1 [==============================] - 0s 106ms/step - loss: 1.0885 - accuracy: 0.9067\n","Epoch 26/50\n","1/1 [==============================] - 0s 102ms/step - loss: 1.0875 - accuracy: 0.9067\n","Epoch 27/50\n","1/1 [==============================] - 0s 125ms/step - loss: 1.0862 - accuracy: 0.9067\n","Epoch 28/50\n","1/1 [==============================] - 0s 181ms/step - loss: 1.0852 - accuracy: 0.9067\n","Epoch 29/50\n","1/1 [==============================] - 0s 179ms/step - loss: 1.0845 - accuracy: 0.9067\n","Epoch 30/50\n","1/1 [==============================] - 0s 294ms/step - loss: 1.0633 - accuracy: 0.9067\n","Epoch 31/50\n","1/1 [==============================] - 0s 166ms/step - loss: 1.0617 - accuracy: 0.9067\n","Epoch 32/50\n","1/1 [==============================] - 0s 170ms/step - loss: 1.0423 - accuracy: 0.9067\n","Epoch 33/50\n","1/1 [==============================] - 0s 192ms/step - loss: 1.0214 - accuracy: 0.9067\n","Epoch 34/50\n","1/1 [==============================] - 0s 186ms/step - loss: 1.0204 - accuracy: 0.9067\n","Epoch 35/50\n","1/1 [==============================] - 0s 155ms/step - loss: 1.0168 - accuracy: 0.9067\n","Epoch 36/50\n","1/1 [==============================] - 0s 155ms/step - loss: 1.0128 - accuracy: 0.9067\n","Epoch 37/50\n","1/1 [==============================] - 0s 152ms/step - loss: 1.0117 - accuracy: 0.9067\n","Epoch 38/50\n","1/1 [==============================] - 0s 162ms/step - loss: 1.0080 - accuracy: 0.9067\n","Epoch 39/50\n","1/1 [==============================] - 0s 165ms/step - loss: 1.0058 - accuracy: 0.9067\n","Epoch 40/50\n","1/1 [==============================] - 0s 191ms/step - loss: 1.0040 - accuracy: 0.9067\n","Epoch 41/50\n","1/1 [==============================] - 0s 195ms/step - loss: 1.0019 - accuracy: 0.9067\n","Epoch 42/50\n","1/1 [==============================] - 0s 189ms/step - loss: 1.0000 - accuracy: 0.9067\n","Epoch 43/50\n","1/1 [==============================] - 0s 137ms/step - loss: 0.9987 - accuracy: 0.9067\n","Epoch 44/50\n","1/1 [==============================] - 0s 128ms/step - loss: 0.9977 - accuracy: 0.9067\n","Epoch 45/50\n","1/1 [==============================] - 0s 141ms/step - loss: 0.9967 - accuracy: 0.9067\n","Epoch 46/50\n","1/1 [==============================] - 0s 171ms/step - loss: 0.9957 - accuracy: 0.9067\n","Epoch 47/50\n","1/1 [==============================] - 0s 171ms/step - loss: 0.9947 - accuracy: 0.9067\n","Epoch 48/50\n","1/1 [==============================] - 0s 201ms/step - loss: 0.9938 - accuracy: 0.9067\n","Epoch 49/50\n","1/1 [==============================] - 0s 136ms/step - loss: 0.9929 - accuracy: 0.9067\n","Epoch 50/50\n","1/1 [==============================] - 0s 140ms/step - loss: 0.9919 - accuracy: 0.9067\n","WARNING:tensorflow:10 out of the last 170 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b273b51f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1678 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:52:48,577]\u001b[0m Trial 8 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 88, 'num_epochs': 50}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/82\n","1/1 [==============================] - 6s 6s/step - loss: 14.6115 - accuracy: 0.0040\n","Epoch 2/82\n","1/1 [==============================] - 0s 168ms/step - loss: 1.3400 - accuracy: 0.9067\n","Epoch 3/82\n","1/1 [==============================] - 0s 347ms/step - loss: 0.9690 - accuracy: 0.9067\n","Epoch 4/82\n","1/1 [==============================] - 0s 211ms/step - loss: 0.7179 - accuracy: 0.9067\n","Epoch 5/82\n","1/1 [==============================] - 0s 319ms/step - loss: 0.7460 - accuracy: 0.9067\n","Epoch 6/82\n","1/1 [==============================] - 0s 217ms/step - loss: 0.7335 - accuracy: 0.9067\n","Epoch 7/82\n","1/1 [==============================] - 0s 173ms/step - loss: 0.7122 - accuracy: 0.9067\n","Epoch 8/82\n","1/1 [==============================] - 0s 271ms/step - loss: 0.6977 - accuracy: 0.9067\n","Epoch 9/82\n","1/1 [==============================] - 0s 285ms/step - loss: 0.7573 - accuracy: 0.9067\n","Epoch 10/82\n","1/1 [==============================] - 0s 244ms/step - loss: 0.6972 - accuracy: 0.9067\n","Epoch 11/82\n","1/1 [==============================] - 0s 172ms/step - loss: 0.7061 - accuracy: 0.9067\n","Epoch 12/82\n","1/1 [==============================] - 0s 295ms/step - loss: 0.7146 - accuracy: 0.9067\n","Epoch 13/82\n","1/1 [==============================] - 0s 463ms/step - loss: 0.7158 - accuracy: 0.9067\n","Epoch 14/82\n","1/1 [==============================] - 0s 313ms/step - loss: 0.7100 - accuracy: 0.9067\n","Epoch 15/82\n","1/1 [==============================] - 0s 244ms/step - loss: 0.6993 - accuracy: 0.9067\n","Epoch 16/82\n","1/1 [==============================] - 0s 323ms/step - loss: 0.6880 - accuracy: 0.9067\n","Epoch 17/82\n","1/1 [==============================] - 0s 273ms/step - loss: 0.6797 - accuracy: 0.9067\n","Epoch 18/82\n","1/1 [==============================] - 0s 423ms/step - loss: 0.7248 - accuracy: 0.9067\n","Epoch 19/82\n","1/1 [==============================] - 0s 252ms/step - loss: 0.7505 - accuracy: 0.9067\n","Epoch 20/82\n","1/1 [==============================] - 0s 232ms/step - loss: 0.7883 - accuracy: 0.9067\n","Epoch 21/82\n","1/1 [==============================] - 0s 322ms/step - loss: 0.6930 - accuracy: 0.9067\n","Epoch 22/82\n","1/1 [==============================] - 0s 421ms/step - loss: 0.7573 - accuracy: 0.9067\n","Epoch 23/82\n","1/1 [==============================] - 0s 389ms/step - loss: 0.8257 - accuracy: 0.9008\n","Epoch 24/82\n","1/1 [==============================] - 0s 373ms/step - loss: 0.8797 - accuracy: 0.8869\n","Epoch 25/82\n","1/1 [==============================] - 0s 375ms/step - loss: 0.9154 - accuracy: 0.8730\n","Epoch 26/82\n","1/1 [==============================] - 1s 520ms/step - loss: 0.9345 - accuracy: 0.8651\n","Epoch 27/82\n","1/1 [==============================] - 1s 710ms/step - loss: 0.9399 - accuracy: 0.8690\n","Epoch 28/82\n","1/1 [==============================] - 0s 381ms/step - loss: 0.9355 - accuracy: 0.8869\n","Epoch 29/82\n","1/1 [==============================] - 0s 393ms/step - loss: 0.9237 - accuracy: 0.9008\n","Epoch 30/82\n","1/1 [==============================] - 0s 330ms/step - loss: 0.9056 - accuracy: 0.9067\n","Epoch 31/82\n","1/1 [==============================] - 0s 296ms/step - loss: 0.8830 - accuracy: 0.9067\n","Epoch 32/82\n","1/1 [==============================] - 0s 299ms/step - loss: 0.8583 - accuracy: 0.9067\n","Epoch 33/82\n","1/1 [==============================] - 0s 252ms/step - loss: 0.8329 - accuracy: 0.9067\n","Epoch 34/82\n","1/1 [==============================] - 0s 238ms/step - loss: 0.8082 - accuracy: 0.9067\n","Epoch 35/82\n","1/1 [==============================] - 0s 265ms/step - loss: 0.7841 - accuracy: 0.9067\n","Epoch 36/82\n","1/1 [==============================] - 0s 342ms/step - loss: 0.7611 - accuracy: 0.9067\n","Epoch 37/82\n","1/1 [==============================] - 0s 275ms/step - loss: 0.7399 - accuracy: 0.9067\n","Epoch 38/82\n","1/1 [==============================] - 0s 250ms/step - loss: 0.7213 - accuracy: 0.9067\n","Epoch 39/82\n","1/1 [==============================] - 0s 200ms/step - loss: 0.7068 - accuracy: 0.9067\n","Epoch 40/82\n","1/1 [==============================] - 0s 311ms/step - loss: 0.6973 - accuracy: 0.9067\n","Epoch 41/82\n","1/1 [==============================] - 0s 228ms/step - loss: 0.6950 - accuracy: 0.9067\n","Epoch 42/82\n","1/1 [==============================] - 0s 331ms/step - loss: 0.7258 - accuracy: 0.9067\n","Epoch 43/82\n","1/1 [==============================] - 0s 252ms/step - loss: 0.7723 - accuracy: 0.9067\n","Epoch 44/82\n","1/1 [==============================] - 0s 214ms/step - loss: 0.7720 - accuracy: 0.9067\n","Epoch 45/82\n","1/1 [==============================] - 0s 275ms/step - loss: 0.7684 - accuracy: 0.9067\n","Epoch 46/82\n","1/1 [==============================] - 0s 212ms/step - loss: 0.7637 - accuracy: 0.9067\n","Epoch 47/82\n","1/1 [==============================] - 0s 241ms/step - loss: 0.7593 - accuracy: 0.9067\n","Epoch 48/82\n","1/1 [==============================] - 0s 245ms/step - loss: 0.7562 - accuracy: 0.9067\n","Epoch 49/82\n","1/1 [==============================] - 0s 276ms/step - loss: 0.7541 - accuracy: 0.9067\n","Epoch 50/82\n","1/1 [==============================] - 0s 267ms/step - loss: 0.7521 - accuracy: 0.9067\n","Epoch 51/82\n","1/1 [==============================] - 0s 271ms/step - loss: 0.7501 - accuracy: 0.9067\n","Epoch 52/82\n","1/1 [==============================] - 0s 286ms/step - loss: 0.7297 - accuracy: 0.9067\n","Epoch 53/82\n","1/1 [==============================] - 0s 293ms/step - loss: 0.7241 - accuracy: 0.9067\n","Epoch 54/82\n","1/1 [==============================] - 0s 237ms/step - loss: 0.7249 - accuracy: 0.9067\n","Epoch 55/82\n","1/1 [==============================] - 0s 341ms/step - loss: 0.7027 - accuracy: 0.9067\n","Epoch 56/82\n","1/1 [==============================] - 0s 371ms/step - loss: 0.6794 - accuracy: 0.9067\n","Epoch 57/82\n","1/1 [==============================] - 0s 296ms/step - loss: 0.6775 - accuracy: 0.9067\n","Epoch 58/82\n","1/1 [==============================] - 0s 306ms/step - loss: 0.6769 - accuracy: 0.9067\n","Epoch 59/82\n","1/1 [==============================] - 0s 400ms/step - loss: 0.6767 - accuracy: 0.9067\n","Epoch 60/82\n","1/1 [==============================] - 0s 299ms/step - loss: 0.6764 - accuracy: 0.9067\n","Epoch 61/82\n","1/1 [==============================] - 0s 283ms/step - loss: 0.6755 - accuracy: 0.9067\n","Epoch 62/82\n","1/1 [==============================] - 0s 299ms/step - loss: 0.6743 - accuracy: 0.9067\n","Epoch 63/82\n","1/1 [==============================] - 0s 376ms/step - loss: nan - accuracy: 0.9087\n","Epoch 64/82\n","1/1 [==============================] - 0s 384ms/step - loss: nan - accuracy: 0.9067\n","Epoch 65/82\n","1/1 [==============================] - 0s 322ms/step - loss: nan - accuracy: 0.9067\n","Epoch 66/82\n","1/1 [==============================] - 0s 316ms/step - loss: nan - accuracy: 0.9067\n","Epoch 67/82\n","1/1 [==============================] - 1s 536ms/step - loss: nan - accuracy: 0.9067\n","Epoch 68/82\n","1/1 [==============================] - 0s 301ms/step - loss: nan - accuracy: 0.9067\n","Epoch 69/82\n","1/1 [==============================] - 0s 295ms/step - loss: nan - accuracy: 0.9067\n","Epoch 70/82\n","1/1 [==============================] - 0s 289ms/step - loss: nan - accuracy: 0.9067\n","Epoch 71/82\n","1/1 [==============================] - 0s 250ms/step - loss: nan - accuracy: 0.9067\n","Epoch 72/82\n","1/1 [==============================] - 0s 417ms/step - loss: nan - accuracy: 0.9067\n","Epoch 73/82\n","1/1 [==============================] - 0s 410ms/step - loss: nan - accuracy: 0.9067\n","Epoch 74/82\n","1/1 [==============================] - 0s 301ms/step - loss: nan - accuracy: 0.9067\n","Epoch 75/82\n","1/1 [==============================] - 0s 322ms/step - loss: nan - accuracy: 0.9067\n","Epoch 76/82\n","1/1 [==============================] - 0s 359ms/step - loss: nan - accuracy: 0.9067\n","Epoch 77/82\n","1/1 [==============================] - 0s 480ms/step - loss: nan - accuracy: 0.9067\n","Epoch 78/82\n","1/1 [==============================] - 1s 550ms/step - loss: nan - accuracy: 0.9067\n","Epoch 79/82\n","1/1 [==============================] - 0s 312ms/step - loss: nan - accuracy: 0.9067\n","Epoch 80/82\n","1/1 [==============================] - 0s 475ms/step - loss: nan - accuracy: 0.9067\n","Epoch 81/82\n","1/1 [==============================] - 0s 264ms/step - loss: nan - accuracy: 0.9067\n","Epoch 82/82\n","1/1 [==============================] - 0s 456ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 171 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b457a71f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:53:22,478]\u001b[0m Trial 9 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 218, 'num_epochs': 82}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","1/1 [==============================] - 6s 6s/step - loss: 3.6284 - accuracy: 0.0000e+00\n","Epoch 2/150\n","1/1 [==============================] - 0s 159ms/step - loss: 1.4068 - accuracy: 0.9067\n","Epoch 3/150\n","1/1 [==============================] - 0s 216ms/step - loss: 1.1654 - accuracy: 0.9067\n","Epoch 4/150\n","1/1 [==============================] - 0s 159ms/step - loss: 1.0276 - accuracy: 0.9067\n","Epoch 5/150\n","1/1 [==============================] - 0s 198ms/step - loss: 0.9662 - accuracy: 0.9067\n","Epoch 6/150\n","1/1 [==============================] - 0s 208ms/step - loss: 1.0199 - accuracy: 0.9067\n","Epoch 7/150\n","1/1 [==============================] - 0s 171ms/step - loss: 1.0659 - accuracy: 0.9067\n","Epoch 8/150\n","1/1 [==============================] - 0s 234ms/step - loss: 1.0964 - accuracy: 0.9067\n","Epoch 9/150\n","1/1 [==============================] - 0s 214ms/step - loss: 1.0914 - accuracy: 0.9067\n","Epoch 10/150\n","1/1 [==============================] - 0s 275ms/step - loss: 1.0567 - accuracy: 0.9067\n","Epoch 11/150\n","1/1 [==============================] - 0s 238ms/step - loss: 1.0083 - accuracy: 0.9067\n","Epoch 12/150\n","1/1 [==============================] - 0s 239ms/step - loss: 1.0835 - accuracy: 0.9067\n","Epoch 13/150\n","1/1 [==============================] - 0s 228ms/step - loss: 1.1239 - accuracy: 0.9067\n","Epoch 14/150\n","1/1 [==============================] - 0s 287ms/step - loss: 1.1229 - accuracy: 0.9067\n","Epoch 15/150\n","1/1 [==============================] - 0s 160ms/step - loss: 1.1226 - accuracy: 0.9067\n","Epoch 16/150\n","1/1 [==============================] - 0s 155ms/step - loss: 1.1211 - accuracy: 0.9067\n","Epoch 17/150\n","1/1 [==============================] - 0s 162ms/step - loss: 1.1190 - accuracy: 0.9067\n","Epoch 18/150\n","1/1 [==============================] - 0s 181ms/step - loss: 1.1174 - accuracy: 0.9067\n","Epoch 19/150\n","1/1 [==============================] - 0s 178ms/step - loss: 1.1166 - accuracy: 0.9067\n","Epoch 20/150\n","1/1 [==============================] - 0s 170ms/step - loss: 1.1155 - accuracy: 0.9067\n","Epoch 21/150\n","1/1 [==============================] - 0s 196ms/step - loss: 1.1136 - accuracy: 0.9067\n","Epoch 22/150\n","1/1 [==============================] - 0s 196ms/step - loss: 1.1113 - accuracy: 0.9067\n","Epoch 23/150\n","1/1 [==============================] - 0s 264ms/step - loss: 1.1092 - accuracy: 0.9067\n","Epoch 24/150\n","1/1 [==============================] - 0s 176ms/step - loss: 1.1073 - accuracy: 0.9067\n","Epoch 25/150\n","1/1 [==============================] - 0s 216ms/step - loss: 1.1048 - accuracy: 0.9087\n","Epoch 26/150\n","1/1 [==============================] - 0s 172ms/step - loss: 1.1015 - accuracy: 0.9107\n","Epoch 27/150\n","1/1 [==============================] - 0s 154ms/step - loss: 1.0980 - accuracy: 0.9147\n","Epoch 28/150\n","1/1 [==============================] - 0s 205ms/step - loss: 1.0955 - accuracy: 0.9127\n","Epoch 29/150\n","1/1 [==============================] - 0s 263ms/step - loss: 1.1259 - accuracy: 0.9147\n","Epoch 30/150\n","1/1 [==============================] - 0s 257ms/step - loss: 1.0934 - accuracy: 0.9127\n","Epoch 31/150\n","1/1 [==============================] - 0s 166ms/step - loss: 1.0942 - accuracy: 0.9167\n","Epoch 32/150\n","1/1 [==============================] - 0s 146ms/step - loss: 1.0928 - accuracy: 0.9187\n","Epoch 33/150\n","1/1 [==============================] - 0s 146ms/step - loss: 1.0928 - accuracy: 0.9147\n","Epoch 34/150\n","1/1 [==============================] - 0s 140ms/step - loss: 1.0924 - accuracy: 0.9167\n","Epoch 35/150\n","1/1 [==============================] - 0s 192ms/step - loss: 1.0906 - accuracy: 0.9167\n","Epoch 36/150\n","1/1 [==============================] - 0s 223ms/step - loss: 1.0891 - accuracy: 0.9167\n","Epoch 37/150\n","1/1 [==============================] - 0s 209ms/step - loss: 1.0887 - accuracy: 0.9147\n","Epoch 38/150\n","1/1 [==============================] - 0s 188ms/step - loss: 1.0889 - accuracy: 0.9167\n","Epoch 39/150\n","1/1 [==============================] - 0s 187ms/step - loss: 1.0892 - accuracy: 0.9167\n","Epoch 40/150\n","1/1 [==============================] - 0s 168ms/step - loss: 1.0878 - accuracy: 0.9187\n","Epoch 41/150\n","1/1 [==============================] - 0s 179ms/step - loss: 1.0871 - accuracy: 0.9147\n","Epoch 42/150\n","1/1 [==============================] - 0s 158ms/step - loss: 1.0871 - accuracy: 0.9147\n","Epoch 43/150\n","1/1 [==============================] - 0s 144ms/step - loss: 1.0872 - accuracy: 0.9147\n","Epoch 44/150\n","1/1 [==============================] - 0s 138ms/step - loss: 1.0869 - accuracy: 0.9167\n","Epoch 45/150\n","1/1 [==============================] - 0s 157ms/step - loss: 1.0864 - accuracy: 0.9167\n","Epoch 46/150\n","1/1 [==============================] - 0s 168ms/step - loss: 1.0859 - accuracy: 0.9187\n","Epoch 47/150\n","1/1 [==============================] - 0s 189ms/step - loss: 1.0854 - accuracy: 0.9187\n","Epoch 48/150\n","1/1 [==============================] - 0s 187ms/step - loss: 1.0849 - accuracy: 0.9187\n","Epoch 49/150\n","1/1 [==============================] - 0s 235ms/step - loss: 1.0844 - accuracy: 0.9187\n","Epoch 50/150\n","1/1 [==============================] - 0s 209ms/step - loss: 1.0846 - accuracy: 0.9206\n","Epoch 51/150\n","1/1 [==============================] - 0s 228ms/step - loss: 1.0836 - accuracy: 0.9187\n","Epoch 52/150\n","1/1 [==============================] - 0s 222ms/step - loss: 1.0832 - accuracy: 0.9187\n","Epoch 53/150\n","1/1 [==============================] - 0s 178ms/step - loss: 1.0828 - accuracy: 0.9187\n","Epoch 54/150\n","1/1 [==============================] - 0s 169ms/step - loss: 1.0822 - accuracy: 0.9206\n","Epoch 55/150\n","1/1 [==============================] - 0s 222ms/step - loss: 1.0818 - accuracy: 0.9226\n","Epoch 56/150\n","1/1 [==============================] - 0s 170ms/step - loss: 1.0812 - accuracy: 0.9226\n","Epoch 57/150\n","1/1 [==============================] - 0s 221ms/step - loss: 1.0805 - accuracy: 0.9226\n","Epoch 58/150\n","1/1 [==============================] - 0s 202ms/step - loss: 1.0798 - accuracy: 0.9187\n","Epoch 59/150\n","1/1 [==============================] - 0s 289ms/step - loss: 1.0793 - accuracy: 0.9226\n","Epoch 60/150\n","1/1 [==============================] - 0s 186ms/step - loss: 1.0786 - accuracy: 0.9226\n","Epoch 61/150\n","1/1 [==============================] - 0s 216ms/step - loss: 1.0780 - accuracy: 0.9226\n","Epoch 62/150\n","1/1 [==============================] - 0s 176ms/step - loss: 1.0775 - accuracy: 0.9226\n","Epoch 63/150\n","1/1 [==============================] - 0s 153ms/step - loss: 1.0772 - accuracy: 0.9226\n","Epoch 64/150\n","1/1 [==============================] - 0s 192ms/step - loss: 1.0770 - accuracy: 0.9226\n","Epoch 65/150\n","1/1 [==============================] - 0s 172ms/step - loss: 1.0765 - accuracy: 0.9226\n","Epoch 66/150\n","1/1 [==============================] - 0s 210ms/step - loss: 1.0765 - accuracy: 0.9226\n","Epoch 67/150\n","1/1 [==============================] - 0s 251ms/step - loss: 1.0763 - accuracy: 0.9226\n","Epoch 68/150\n","1/1 [==============================] - 0s 202ms/step - loss: 1.0760 - accuracy: 0.9226\n","Epoch 69/150\n","1/1 [==============================] - 0s 162ms/step - loss: 1.0756 - accuracy: 0.9226\n","Epoch 70/150\n","1/1 [==============================] - 0s 163ms/step - loss: 1.0754 - accuracy: 0.9226\n","Epoch 71/150\n","1/1 [==============================] - 0s 156ms/step - loss: 1.0755 - accuracy: 0.9226\n","Epoch 72/150\n","1/1 [==============================] - 0s 173ms/step - loss: 1.0749 - accuracy: 0.9226\n","Epoch 73/150\n","1/1 [==============================] - 0s 205ms/step - loss: 1.0748 - accuracy: 0.9226\n","Epoch 74/150\n","1/1 [==============================] - 0s 235ms/step - loss: 1.0747 - accuracy: 0.9226\n","Epoch 75/150\n","1/1 [==============================] - 0s 149ms/step - loss: nan - accuracy: 0.9226\n","Epoch 76/150\n","1/1 [==============================] - 0s 146ms/step - loss: nan - accuracy: 0.9067\n","Epoch 77/150\n","1/1 [==============================] - 0s 172ms/step - loss: nan - accuracy: 0.9067\n","Epoch 78/150\n","1/1 [==============================] - 0s 167ms/step - loss: nan - accuracy: 0.9067\n","Epoch 79/150\n","1/1 [==============================] - 0s 158ms/step - loss: nan - accuracy: 0.9067\n","Epoch 80/150\n","1/1 [==============================] - 0s 156ms/step - loss: nan - accuracy: 0.9067\n","Epoch 81/150\n","1/1 [==============================] - 0s 155ms/step - loss: nan - accuracy: 0.9067\n","Epoch 82/150\n","1/1 [==============================] - 0s 162ms/step - loss: nan - accuracy: 0.9067\n","Epoch 83/150\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/150\n","1/1 [==============================] - 0s 199ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/150\n","1/1 [==============================] - 0s 250ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/150\n","1/1 [==============================] - 0s 234ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/150\n","1/1 [==============================] - 0s 175ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/150\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/150\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/150\n","1/1 [==============================] - 0s 157ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/150\n","1/1 [==============================] - 0s 180ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/150\n","1/1 [==============================] - 0s 189ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/150\n","1/1 [==============================] - 0s 230ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/150\n","1/1 [==============================] - 0s 207ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/150\n","1/1 [==============================] - 0s 195ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/150\n","1/1 [==============================] - 0s 200ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/150\n","1/1 [==============================] - 0s 208ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/150\n","1/1 [==============================] - 0s 198ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/150\n","1/1 [==============================] - 0s 257ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/150\n","1/1 [==============================] - 0s 201ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/150\n","1/1 [==============================] - 0s 198ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/150\n","1/1 [==============================] - 0s 314ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/150\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/150\n","1/1 [==============================] - 0s 195ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/150\n","1/1 [==============================] - 0s 193ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/150\n","1/1 [==============================] - 0s 183ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/150\n","1/1 [==============================] - 0s 218ms/step - loss: nan - accuracy: 0.9067\n","Epoch 108/150\n","1/1 [==============================] - 0s 227ms/step - loss: nan - accuracy: 0.9067\n","Epoch 109/150\n","1/1 [==============================] - 0s 246ms/step - loss: nan - accuracy: 0.9067\n","Epoch 110/150\n","1/1 [==============================] - 0s 229ms/step - loss: nan - accuracy: 0.9067\n","Epoch 111/150\n","1/1 [==============================] - 0s 202ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/150\n","1/1 [==============================] - 0s 227ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/150\n","1/1 [==============================] - 0s 231ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/150\n","1/1 [==============================] - 0s 199ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/150\n","1/1 [==============================] - 0s 230ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/150\n","1/1 [==============================] - 0s 202ms/step - loss: nan - accuracy: 0.9067\n","Epoch 117/150\n","1/1 [==============================] - 0s 231ms/step - loss: nan - accuracy: 0.9067\n","Epoch 118/150\n","1/1 [==============================] - 0s 189ms/step - loss: nan - accuracy: 0.9067\n","Epoch 119/150\n","1/1 [==============================] - 0s 225ms/step - loss: nan - accuracy: 0.9067\n","Epoch 120/150\n","1/1 [==============================] - 0s 211ms/step - loss: nan - accuracy: 0.9067\n","Epoch 121/150\n","1/1 [==============================] - 0s 252ms/step - loss: nan - accuracy: 0.9067\n","Epoch 122/150\n","1/1 [==============================] - 0s 240ms/step - loss: nan - accuracy: 0.9067\n","Epoch 123/150\n","1/1 [==============================] - 0s 211ms/step - loss: nan - accuracy: 0.9067\n","Epoch 124/150\n","1/1 [==============================] - 0s 236ms/step - loss: nan - accuracy: 0.9067\n","Epoch 125/150\n","1/1 [==============================] - 0s 239ms/step - loss: nan - accuracy: 0.9067\n","Epoch 126/150\n","1/1 [==============================] - 0s 234ms/step - loss: nan - accuracy: 0.9067\n","Epoch 127/150\n","1/1 [==============================] - 0s 235ms/step - loss: nan - accuracy: 0.9067\n","Epoch 128/150\n","1/1 [==============================] - 0s 241ms/step - loss: nan - accuracy: 0.9067\n","Epoch 129/150\n","1/1 [==============================] - 0s 201ms/step - loss: nan - accuracy: 0.9067\n","Epoch 130/150\n","1/1 [==============================] - 0s 237ms/step - loss: nan - accuracy: 0.9067\n","Epoch 131/150\n","1/1 [==============================] - 0s 251ms/step - loss: nan - accuracy: 0.9067\n","Epoch 132/150\n","1/1 [==============================] - 0s 241ms/step - loss: nan - accuracy: 0.9067\n","Epoch 133/150\n","1/1 [==============================] - 0s 249ms/step - loss: nan - accuracy: 0.9067\n","Epoch 134/150\n","1/1 [==============================] - 0s 221ms/step - loss: nan - accuracy: 0.9067\n","Epoch 135/150\n","1/1 [==============================] - 0s 213ms/step - loss: nan - accuracy: 0.9067\n","Epoch 136/150\n","1/1 [==============================] - 0s 229ms/step - loss: nan - accuracy: 0.9067\n","Epoch 137/150\n","1/1 [==============================] - 0s 216ms/step - loss: nan - accuracy: 0.9067\n","Epoch 138/150\n","1/1 [==============================] - 0s 202ms/step - loss: nan - accuracy: 0.9067\n","Epoch 139/150\n","1/1 [==============================] - 0s 241ms/step - loss: nan - accuracy: 0.9067\n","Epoch 140/150\n","1/1 [==============================] - 0s 227ms/step - loss: nan - accuracy: 0.9067\n","Epoch 141/150\n","1/1 [==============================] - 0s 161ms/step - loss: nan - accuracy: 0.9067\n","Epoch 142/150\n","1/1 [==============================] - 0s 165ms/step - loss: nan - accuracy: 0.9067\n","Epoch 143/150\n","1/1 [==============================] - 0s 212ms/step - loss: nan - accuracy: 0.9067\n","Epoch 144/150\n","1/1 [==============================] - 0s 189ms/step - loss: nan - accuracy: 0.9067\n","Epoch 145/150\n","1/1 [==============================] - 0s 160ms/step - loss: nan - accuracy: 0.9067\n","Epoch 146/150\n","1/1 [==============================] - 0s 191ms/step - loss: nan - accuracy: 0.9067\n","Epoch 147/150\n","1/1 [==============================] - 0s 169ms/step - loss: nan - accuracy: 0.9067\n","Epoch 148/150\n","1/1 [==============================] - 0s 236ms/step - loss: nan - accuracy: 0.9067\n","Epoch 149/150\n","1/1 [==============================] - 0s 390ms/step - loss: nan - accuracy: 0.9067\n","Epoch 150/150\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b27830a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:54:00,596]\u001b[0m Trial 10 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 186, 'num_epochs': 150}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/111\n","1/1 [==============================] - 5s 5s/step - loss: 3.1777 - accuracy: 0.1508\n","Epoch 2/111\n","1/1 [==============================] - 0s 399ms/step - loss: 1.3639 - accuracy: 0.9067\n","Epoch 3/111\n","1/1 [==============================] - 0s 419ms/step - loss: 1.1816 - accuracy: 0.9067\n","Epoch 4/111\n","1/1 [==============================] - 0s 497ms/step - loss: 1.4214 - accuracy: 0.9067\n","Epoch 5/111\n","1/1 [==============================] - 1s 531ms/step - loss: 1.4030 - accuracy: 0.9067\n","Epoch 6/111\n","1/1 [==============================] - 1s 551ms/step - loss: 1.3742 - accuracy: 0.9067\n","Epoch 7/111\n","1/1 [==============================] - 1s 557ms/step - loss: 1.3536 - accuracy: 0.9067\n","Epoch 8/111\n","1/1 [==============================] - 1s 572ms/step - loss: 1.3504 - accuracy: 0.9067\n","Epoch 9/111\n","1/1 [==============================] - 0s 470ms/step - loss: 1.3741 - accuracy: 0.9067\n","Epoch 10/111\n","1/1 [==============================] - 0s 481ms/step - loss: 1.3734 - accuracy: 0.9067\n","Epoch 11/111\n","1/1 [==============================] - 0s 459ms/step - loss: 1.3729 - accuracy: 0.9067\n","Epoch 12/111\n","1/1 [==============================] - 0s 347ms/step - loss: 1.3726 - accuracy: 0.9067\n","Epoch 13/111\n","1/1 [==============================] - 0s 295ms/step - loss: 1.3723 - accuracy: 0.9067\n","Epoch 14/111\n","1/1 [==============================] - 1s 532ms/step - loss: 1.3720 - accuracy: 0.9067\n","Epoch 15/111\n","1/1 [==============================] - 0s 380ms/step - loss: 1.3718 - accuracy: 0.9067\n","Epoch 16/111\n","1/1 [==============================] - 0s 362ms/step - loss: 1.3716 - accuracy: 0.9067\n","Epoch 17/111\n","1/1 [==============================] - 0s 302ms/step - loss: 1.3714 - accuracy: 0.9067\n","Epoch 18/111\n","1/1 [==============================] - 0s 336ms/step - loss: 1.3712 - accuracy: 0.9067\n","Epoch 19/111\n","1/1 [==============================] - 0s 387ms/step - loss: 1.3710 - accuracy: 0.9067\n","Epoch 20/111\n","1/1 [==============================] - 0s 386ms/step - loss: 1.3708 - accuracy: 0.9067\n","Epoch 21/111\n","1/1 [==============================] - 0s 326ms/step - loss: 1.3706 - accuracy: 0.9067\n","Epoch 22/111\n","1/1 [==============================] - 0s 260ms/step - loss: 1.3705 - accuracy: 0.9067\n","Epoch 23/111\n","1/1 [==============================] - 0s 289ms/step - loss: 1.3702 - accuracy: 0.9067\n","Epoch 24/111\n","1/1 [==============================] - 0s 302ms/step - loss: 1.3699 - accuracy: 0.9067\n","Epoch 25/111\n","1/1 [==============================] - 0s 380ms/step - loss: 1.3697 - accuracy: 0.9067\n","Epoch 26/111\n","1/1 [==============================] - 1s 1s/step - loss: 1.3694 - accuracy: 0.9067\n","Epoch 27/111\n","1/1 [==============================] - 0s 490ms/step - loss: 1.3692 - accuracy: 0.9067\n","Epoch 28/111\n","1/1 [==============================] - 1s 728ms/step - loss: 1.3689 - accuracy: 0.9067\n","Epoch 29/111\n","1/1 [==============================] - 1s 563ms/step - loss: 1.3686 - accuracy: 0.9067\n","Epoch 30/111\n","1/1 [==============================] - 1s 885ms/step - loss: 1.3684 - accuracy: 0.9067\n","Epoch 31/111\n","1/1 [==============================] - 0s 373ms/step - loss: 1.3682 - accuracy: 0.9067\n","Epoch 32/111\n","1/1 [==============================] - 0s 324ms/step - loss: 1.3680 - accuracy: 0.9067\n","Epoch 33/111\n","1/1 [==============================] - 0s 296ms/step - loss: 1.3679 - accuracy: 0.9067\n","Epoch 34/111\n","1/1 [==============================] - 0s 306ms/step - loss: 1.3678 - accuracy: 0.9067\n","Epoch 35/111\n","1/1 [==============================] - 0s 310ms/step - loss: 1.3678 - accuracy: 0.9067\n","Epoch 36/111\n","1/1 [==============================] - 0s 339ms/step - loss: 1.3677 - accuracy: 0.9067\n","Epoch 37/111\n","1/1 [==============================] - 0s 292ms/step - loss: 1.3676 - accuracy: 0.9067\n","Epoch 38/111\n","1/1 [==============================] - 0s 404ms/step - loss: 1.3674 - accuracy: 0.9067\n","Epoch 39/111\n","1/1 [==============================] - 0s 290ms/step - loss: 1.3672 - accuracy: 0.9067\n","Epoch 40/111\n","1/1 [==============================] - 0s 326ms/step - loss: 1.3670 - accuracy: 0.9067\n","Epoch 41/111\n","1/1 [==============================] - 0s 406ms/step - loss: 1.3669 - accuracy: 0.9067\n","Epoch 42/111\n","1/1 [==============================] - 0s 296ms/step - loss: 1.3667 - accuracy: 0.9067\n","Epoch 43/111\n","1/1 [==============================] - 0s 273ms/step - loss: 1.3666 - accuracy: 0.9067\n","Epoch 44/111\n","1/1 [==============================] - 0s 300ms/step - loss: 1.3665 - accuracy: 0.9067\n","Epoch 45/111\n","1/1 [==============================] - 0s 392ms/step - loss: 1.3664 - accuracy: 0.9067\n","Epoch 46/111\n","1/1 [==============================] - 0s 363ms/step - loss: 1.3663 - accuracy: 0.9067\n","Epoch 47/111\n","1/1 [==============================] - 0s 323ms/step - loss: 1.3662 - accuracy: 0.9067\n","Epoch 48/111\n","1/1 [==============================] - 1s 605ms/step - loss: 1.3661 - accuracy: 0.9067\n","Epoch 49/111\n","1/1 [==============================] - 1s 590ms/step - loss: 1.3659 - accuracy: 0.9067\n","Epoch 50/111\n","1/1 [==============================] - 0s 297ms/step - loss: 1.3658 - accuracy: 0.9067\n","Epoch 51/111\n","1/1 [==============================] - 0s 316ms/step - loss: 1.3656 - accuracy: 0.9067\n","Epoch 52/111\n","1/1 [==============================] - 0s 335ms/step - loss: 1.3655 - accuracy: 0.9067\n","Epoch 53/111\n","1/1 [==============================] - 1s 828ms/step - loss: 1.3653 - accuracy: 0.9067\n","Epoch 54/111\n","1/1 [==============================] - 0s 392ms/step - loss: 1.3651 - accuracy: 0.9067\n","Epoch 55/111\n","1/1 [==============================] - 0s 401ms/step - loss: 1.3648 - accuracy: 0.9067\n","Epoch 56/111\n","1/1 [==============================] - 0s 352ms/step - loss: 1.3646 - accuracy: 0.9067\n","Epoch 57/111\n","1/1 [==============================] - 1s 571ms/step - loss: 1.3643 - accuracy: 0.9067\n","Epoch 58/111\n","1/1 [==============================] - 1s 611ms/step - loss: 1.3640 - accuracy: 0.9067\n","Epoch 59/111\n","1/1 [==============================] - 0s 374ms/step - loss: 1.3637 - accuracy: 0.9067\n","Epoch 60/111\n","1/1 [==============================] - 0s 368ms/step - loss: 1.3633 - accuracy: 0.9087\n","Epoch 61/111\n","1/1 [==============================] - 0s 338ms/step - loss: 1.3626 - accuracy: 0.9087\n","Epoch 62/111\n","1/1 [==============================] - 1s 576ms/step - loss: 1.3617 - accuracy: 0.9087\n","Epoch 63/111\n","1/1 [==============================] - 1s 1s/step - loss: 1.3624 - accuracy: 0.9087\n","Epoch 64/111\n","1/1 [==============================] - 1s 537ms/step - loss: 2.0991 - accuracy: 0.8254\n","Epoch 65/111\n","1/1 [==============================] - 1s 1s/step - loss: 1.3723 - accuracy: 0.9067\n","Epoch 66/111\n","1/1 [==============================] - 0s 379ms/step - loss: 1.2512 - accuracy: 0.9067\n","Epoch 67/111\n","1/1 [==============================] - 0s 382ms/step - loss: 1.1105 - accuracy: 0.9067\n","Epoch 68/111\n","1/1 [==============================] - 0s 352ms/step - loss: 1.1316 - accuracy: 0.9067\n","Epoch 69/111\n","1/1 [==============================] - 0s 451ms/step - loss: 1.1493 - accuracy: 0.9067\n","Epoch 70/111\n","1/1 [==============================] - 1s 601ms/step - loss: 1.1585 - accuracy: 0.9067\n","Epoch 71/111\n","1/1 [==============================] - 1s 630ms/step - loss: 1.1589 - accuracy: 0.9067\n","Epoch 72/111\n","1/1 [==============================] - 0s 315ms/step - loss: 1.1521 - accuracy: 0.9067\n","Epoch 73/111\n","1/1 [==============================] - 0s 309ms/step - loss: 1.1398 - accuracy: 0.9067\n","Epoch 74/111\n","1/1 [==============================] - 0s 346ms/step - loss: 1.1244 - accuracy: 0.9067\n","Epoch 75/111\n","1/1 [==============================] - 0s 393ms/step - loss: 1.1098 - accuracy: 0.9067\n","Epoch 76/111\n","1/1 [==============================] - 0s 345ms/step - loss: 1.1057 - accuracy: 0.9067\n","Epoch 77/111\n","1/1 [==============================] - 0s 324ms/step - loss: 1.2045 - accuracy: 0.9067\n","Epoch 78/111\n","1/1 [==============================] - 1s 566ms/step - loss: 1.1021 - accuracy: 0.9067\n","Epoch 79/111\n","1/1 [==============================] - 0s 456ms/step - loss: 1.1034 - accuracy: 0.9067\n","Epoch 80/111\n","1/1 [==============================] - 0s 423ms/step - loss: 1.1096 - accuracy: 0.9067\n","Epoch 81/111\n","1/1 [==============================] - 0s 299ms/step - loss: 1.1146 - accuracy: 0.9067\n","Epoch 82/111\n","1/1 [==============================] - 0s 403ms/step - loss: 1.1168 - accuracy: 0.9067\n","Epoch 83/111\n","1/1 [==============================] - 0s 477ms/step - loss: 1.1163 - accuracy: 0.9067\n","Epoch 84/111\n","1/1 [==============================] - 0s 317ms/step - loss: 1.1132 - accuracy: 0.9067\n","Epoch 85/111\n","1/1 [==============================] - 0s 329ms/step - loss: 1.1083 - accuracy: 0.9067\n","Epoch 86/111\n","1/1 [==============================] - 0s 417ms/step - loss: 1.1027 - accuracy: 0.9067\n","Epoch 87/111\n","1/1 [==============================] - 0s 379ms/step - loss: 1.0976 - accuracy: 0.9067\n","Epoch 88/111\n","1/1 [==============================] - 1s 640ms/step - loss: 1.0945 - accuracy: 0.9067\n","Epoch 89/111\n","1/1 [==============================] - 0s 344ms/step - loss: 1.0968 - accuracy: 0.9067\n","Epoch 90/111\n","1/1 [==============================] - 0s 469ms/step - loss: 1.1175 - accuracy: 0.9067\n","Epoch 91/111\n","1/1 [==============================] - 0s 463ms/step - loss: 1.1177 - accuracy: 0.9067\n","Epoch 92/111\n","1/1 [==============================] - 1s 564ms/step - loss: 1.1175 - accuracy: 0.9067\n","Epoch 93/111\n","1/1 [==============================] - 0s 497ms/step - loss: 1.1169 - accuracy: 0.9067\n","Epoch 94/111\n","1/1 [==============================] - 0s 367ms/step - loss: 1.1160 - accuracy: 0.9067\n","Epoch 95/111\n","1/1 [==============================] - 0s 439ms/step - loss: 1.0971 - accuracy: 0.9067\n","Epoch 96/111\n","1/1 [==============================] - 0s 373ms/step - loss: 1.0928 - accuracy: 0.9067\n","Epoch 97/111\n","1/1 [==============================] - 1s 769ms/step - loss: 1.0996 - accuracy: 0.9067\n","Epoch 98/111\n","1/1 [==============================] - 0s 417ms/step - loss: 1.1069 - accuracy: 0.9067\n","Epoch 99/111\n","1/1 [==============================] - 0s 446ms/step - loss: 1.1124 - accuracy: 0.9067\n","Epoch 100/111\n","1/1 [==============================] - 0s 368ms/step - loss: 1.1154 - accuracy: 0.9067\n","Epoch 101/111\n","1/1 [==============================] - 0s 389ms/step - loss: 1.1155 - accuracy: 0.9067\n","Epoch 102/111\n","1/1 [==============================] - 1s 523ms/step - loss: 1.1131 - accuracy: 0.9087\n","Epoch 103/111\n","1/1 [==============================] - 0s 400ms/step - loss: 1.1085 - accuracy: 0.9087\n","Epoch 104/111\n","1/1 [==============================] - 0s 363ms/step - loss: 1.1026 - accuracy: 0.9087\n","Epoch 105/111\n","1/1 [==============================] - 0s 483ms/step - loss: 1.0962 - accuracy: 0.9087\n","Epoch 106/111\n","1/1 [==============================] - 1s 765ms/step - loss: 1.0901 - accuracy: 0.9087\n","Epoch 107/111\n","1/1 [==============================] - 1s 639ms/step - loss: 1.0854 - accuracy: 0.9087\n","Epoch 108/111\n","1/1 [==============================] - 1s 544ms/step - loss: 1.0837 - accuracy: 0.9087\n","Epoch 109/111\n","1/1 [==============================] - 1s 583ms/step - loss: 1.0858 - accuracy: 0.9067\n","Epoch 110/111\n","1/1 [==============================] - 1s 745ms/step - loss: 1.0905 - accuracy: 0.9067\n","Epoch 111/111\n","1/1 [==============================] - 1s 518ms/step - loss: 1.0925 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b1c5b6c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1057 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:54:58,001]\u001b[0m Trial 11 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 255, 'num_epochs': 111}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/53\n","1/1 [==============================] - 6s 6s/step - loss: 10.1177 - accuracy: 0.0020\n","Epoch 2/53\n","1/1 [==============================] - 0s 195ms/step - loss: 1.4830 - accuracy: 0.9067\n","Epoch 3/53\n","1/1 [==============================] - 0s 383ms/step - loss: 1.3986 - accuracy: 0.9067\n","Epoch 4/53\n","1/1 [==============================] - 0s 248ms/step - loss: 1.3347 - accuracy: 0.9067\n","Epoch 5/53\n","1/1 [==============================] - 0s 281ms/step - loss: 1.2842 - accuracy: 0.9067\n","Epoch 6/53\n","1/1 [==============================] - 0s 332ms/step - loss: 1.2542 - accuracy: 0.9067\n","Epoch 7/53\n","1/1 [==============================] - 0s 275ms/step - loss: 1.2366 - accuracy: 0.9067\n","Epoch 8/53\n","1/1 [==============================] - 0s 287ms/step - loss: 1.2269 - accuracy: 0.9067\n","Epoch 9/53\n","1/1 [==============================] - 0s 375ms/step - loss: 1.2028 - accuracy: 0.9067\n","Epoch 10/53\n","1/1 [==============================] - 0s 352ms/step - loss: 0.9346 - accuracy: 0.9067\n","Epoch 11/53\n","1/1 [==============================] - 0s 341ms/step - loss: 0.9026 - accuracy: 0.9067\n","Epoch 12/53\n","1/1 [==============================] - 0s 361ms/step - loss: 0.9271 - accuracy: 0.9067\n","Epoch 13/53\n","1/1 [==============================] - 0s 293ms/step - loss: 0.9487 - accuracy: 0.9067\n","Epoch 14/53\n","1/1 [==============================] - 0s 416ms/step - loss: 0.9630 - accuracy: 0.9067\n","Epoch 15/53\n","1/1 [==============================] - 0s 330ms/step - loss: 0.9707 - accuracy: 0.9067\n","Epoch 16/53\n","1/1 [==============================] - 0s 293ms/step - loss: 0.9771 - accuracy: 0.9067\n","Epoch 17/53\n","1/1 [==============================] - 0s 239ms/step - loss: 0.9722 - accuracy: 0.9067\n","Epoch 18/53\n","1/1 [==============================] - 0s 432ms/step - loss: 0.9672 - accuracy: 0.9067\n","Epoch 19/53\n","1/1 [==============================] - 0s 347ms/step - loss: 0.9580 - accuracy: 0.9067\n","Epoch 20/53\n","1/1 [==============================] - 0s 385ms/step - loss: 0.9447 - accuracy: 0.9067\n","Epoch 21/53\n","1/1 [==============================] - 0s 385ms/step - loss: 0.9287 - accuracy: 0.9067\n","Epoch 22/53\n","1/1 [==============================] - 0s 407ms/step - loss: 0.9122 - accuracy: 0.9067\n","Epoch 23/53\n","1/1 [==============================] - 0s 271ms/step - loss: 0.8980 - accuracy: 0.9067\n","Epoch 24/53\n","1/1 [==============================] - 0s 361ms/step - loss: 0.8895 - accuracy: 0.9067\n","Epoch 25/53\n","1/1 [==============================] - 0s 269ms/step - loss: 0.8903 - accuracy: 0.9067\n","Epoch 26/53\n","1/1 [==============================] - 0s 307ms/step - loss: 0.9146 - accuracy: 0.9067\n","Epoch 27/53\n","1/1 [==============================] - 0s 316ms/step - loss: 0.9444 - accuracy: 0.9067\n","Epoch 28/53\n","1/1 [==============================] - 1s 516ms/step - loss: 0.9178 - accuracy: 0.9067\n","Epoch 29/53\n","1/1 [==============================] - 1s 509ms/step - loss: 0.9113 - accuracy: 0.9067\n","Epoch 30/53\n","1/1 [==============================] - 0s 296ms/step - loss: 0.8930 - accuracy: 0.9067\n","Epoch 31/53\n","1/1 [==============================] - 0s 453ms/step - loss: 0.8917 - accuracy: 0.9067\n","Epoch 32/53\n","1/1 [==============================] - 1s 587ms/step - loss: 0.8982 - accuracy: 0.9067\n","Epoch 33/53\n","1/1 [==============================] - 0s 348ms/step - loss: 0.9037 - accuracy: 0.9067\n","Epoch 34/53\n","1/1 [==============================] - 0s 278ms/step - loss: 0.9070 - accuracy: 0.9067\n","Epoch 35/53\n","1/1 [==============================] - 0s 407ms/step - loss: 0.9079 - accuracy: 0.9067\n","Epoch 36/53\n","1/1 [==============================] - 0s 273ms/step - loss: 0.9076 - accuracy: 0.9067\n","Epoch 37/53\n","1/1 [==============================] - 0s 335ms/step - loss: 0.9071 - accuracy: 0.9067\n","Epoch 38/53\n","1/1 [==============================] - 0s 353ms/step - loss: 0.9071 - accuracy: 0.9067\n","Epoch 39/53\n","1/1 [==============================] - 0s 287ms/step - loss: 0.9069 - accuracy: 0.9067\n","Epoch 40/53\n","1/1 [==============================] - 0s 362ms/step - loss: 0.8981 - accuracy: 0.9067\n","Epoch 41/53\n","1/1 [==============================] - 1s 532ms/step - loss: 0.8933 - accuracy: 0.9067\n","Epoch 42/53\n","1/1 [==============================] - 0s 417ms/step - loss: 0.8894 - accuracy: 0.9067\n","Epoch 43/53\n","1/1 [==============================] - 0s 358ms/step - loss: 0.8860 - accuracy: 0.9067\n","Epoch 44/53\n","1/1 [==============================] - 1s 727ms/step - loss: 0.8827 - accuracy: 0.9067\n","Epoch 45/53\n","1/1 [==============================] - 1s 578ms/step - loss: 0.8798 - accuracy: 0.9067\n","Epoch 46/53\n","1/1 [==============================] - 0s 324ms/step - loss: 0.8776 - accuracy: 0.9067\n","Epoch 47/53\n","1/1 [==============================] - 0s 248ms/step - loss: 0.8770 - accuracy: 0.9067\n","Epoch 48/53\n","1/1 [==============================] - 0s 313ms/step - loss: 0.8786 - accuracy: 0.9067\n","Epoch 49/53\n","1/1 [==============================] - 0s 280ms/step - loss: 0.8817 - accuracy: 0.9067\n","Epoch 50/53\n","1/1 [==============================] - 0s 239ms/step - loss: 0.8792 - accuracy: 0.9067\n","Epoch 51/53\n","1/1 [==============================] - 0s 284ms/step - loss: 0.8753 - accuracy: 0.9067\n","Epoch 52/53\n","1/1 [==============================] - 0s 224ms/step - loss: 0.8726 - accuracy: 0.9067\n","Epoch 53/53\n","1/1 [==============================] - 0s 425ms/step - loss: 0.8710 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b12f02040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.0481 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:55:24,307]\u001b[0m Trial 12 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 228, 'num_epochs': 53}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/116\n","1/1 [==============================] - 5s 5s/step - loss: 10.7008 - accuracy: 0.0139\n","Epoch 2/116\n","1/1 [==============================] - 0s 178ms/step - loss: 1.3379 - accuracy: 0.9067\n","Epoch 3/116\n","1/1 [==============================] - 0s 191ms/step - loss: 1.2155 - accuracy: 0.9067\n","Epoch 4/116\n","1/1 [==============================] - 0s 188ms/step - loss: 1.1374 - accuracy: 0.9067\n","Epoch 5/116\n","1/1 [==============================] - 0s 189ms/step - loss: 1.0784 - accuracy: 0.9067\n","Epoch 6/116\n","1/1 [==============================] - 0s 203ms/step - loss: 1.0079 - accuracy: 0.9067\n","Epoch 7/116\n","1/1 [==============================] - 0s 169ms/step - loss: 0.9773 - accuracy: 0.9067\n","Epoch 8/116\n","1/1 [==============================] - 0s 188ms/step - loss: 0.9653 - accuracy: 0.9067\n","Epoch 9/116\n","1/1 [==============================] - 0s 179ms/step - loss: 0.9472 - accuracy: 0.9067\n","Epoch 10/116\n","1/1 [==============================] - 0s 200ms/step - loss: 0.9595 - accuracy: 0.9067\n","Epoch 11/116\n","1/1 [==============================] - 0s 157ms/step - loss: 1.0156 - accuracy: 0.9067\n","Epoch 12/116\n","1/1 [==============================] - 0s 206ms/step - loss: 1.0102 - accuracy: 0.9067\n","Epoch 13/116\n","1/1 [==============================] - 0s 176ms/step - loss: 0.9706 - accuracy: 0.9067\n","Epoch 14/116\n","1/1 [==============================] - 0s 181ms/step - loss: 0.6127 - accuracy: 0.9067\n","Epoch 15/116\n","1/1 [==============================] - 0s 179ms/step - loss: 0.6246 - accuracy: 0.9067\n","Epoch 16/116\n","1/1 [==============================] - 0s 152ms/step - loss: 0.6542 - accuracy: 0.9067\n","Epoch 17/116\n","1/1 [==============================] - 0s 144ms/step - loss: 0.6736 - accuracy: 0.9067\n","Epoch 18/116\n","1/1 [==============================] - 0s 166ms/step - loss: 0.6808 - accuracy: 0.9067\n","Epoch 19/116\n","1/1 [==============================] - 0s 153ms/step - loss: 0.6788 - accuracy: 0.9067\n","Epoch 20/116\n","1/1 [==============================] - 0s 173ms/step - loss: 0.6710 - accuracy: 0.9067\n","Epoch 21/116\n","1/1 [==============================] - 0s 163ms/step - loss: 0.6601 - accuracy: 0.9067\n","Epoch 22/116\n","1/1 [==============================] - 0s 163ms/step - loss: 0.6473 - accuracy: 0.9067\n","Epoch 23/116\n","1/1 [==============================] - 0s 186ms/step - loss: 0.6339 - accuracy: 0.9067\n","Epoch 24/116\n","1/1 [==============================] - 0s 177ms/step - loss: 0.6203 - accuracy: 0.9067\n","Epoch 25/116\n","1/1 [==============================] - 0s 179ms/step - loss: 0.6068 - accuracy: 0.9067\n","Epoch 26/116\n","1/1 [==============================] - 0s 178ms/step - loss: 0.5937 - accuracy: 0.9067\n","Epoch 27/116\n","1/1 [==============================] - 0s 173ms/step - loss: 0.5812 - accuracy: 0.9067\n","Epoch 28/116\n","1/1 [==============================] - 0s 166ms/step - loss: 0.5783 - accuracy: 0.9067\n","Epoch 29/116\n","1/1 [==============================] - 0s 178ms/step - loss: 0.6247 - accuracy: 0.9067\n","Epoch 30/116\n","1/1 [==============================] - 0s 263ms/step - loss: 0.5745 - accuracy: 0.9067\n","Epoch 31/116\n","1/1 [==============================] - 0s 208ms/step - loss: 0.5633 - accuracy: 0.9067\n","Epoch 32/116\n","1/1 [==============================] - 0s 142ms/step - loss: 0.5684 - accuracy: 0.9067\n","Epoch 33/116\n","1/1 [==============================] - 0s 181ms/step - loss: 0.5748 - accuracy: 0.9067\n","Epoch 34/116\n","1/1 [==============================] - 0s 181ms/step - loss: 0.5785 - accuracy: 0.9067\n","Epoch 35/116\n","1/1 [==============================] - 0s 172ms/step - loss: 0.5790 - accuracy: 0.9067\n","Epoch 36/116\n","1/1 [==============================] - 0s 160ms/step - loss: 0.5773 - accuracy: 0.9067\n","Epoch 37/116\n","1/1 [==============================] - 0s 154ms/step - loss: 0.5742 - accuracy: 0.9067\n","Epoch 38/116\n","1/1 [==============================] - 0s 161ms/step - loss: 0.5704 - accuracy: 0.9067\n","Epoch 39/116\n","1/1 [==============================] - 0s 169ms/step - loss: 0.5662 - accuracy: 0.9067\n","Epoch 40/116\n","1/1 [==============================] - 0s 173ms/step - loss: 0.5622 - accuracy: 0.9067\n","Epoch 41/116\n","1/1 [==============================] - 0s 153ms/step - loss: 0.5583 - accuracy: 0.9067\n","Epoch 42/116\n","1/1 [==============================] - 0s 156ms/step - loss: 0.5546 - accuracy: 0.9067\n","Epoch 43/116\n","1/1 [==============================] - 0s 149ms/step - loss: 0.5514 - accuracy: 0.9067\n","Epoch 44/116\n","1/1 [==============================] - 0s 160ms/step - loss: 0.5494 - accuracy: 0.9067\n","Epoch 45/116\n","1/1 [==============================] - 0s 156ms/step - loss: 0.5489 - accuracy: 0.9067\n","Epoch 46/116\n","1/1 [==============================] - 0s 179ms/step - loss: 0.5499 - accuracy: 0.9067\n","Epoch 47/116\n","1/1 [==============================] - 0s 162ms/step - loss: 0.5504 - accuracy: 0.9067\n","Epoch 48/116\n","1/1 [==============================] - 0s 214ms/step - loss: 0.5458 - accuracy: 0.9067\n","Epoch 49/116\n","1/1 [==============================] - 0s 144ms/step - loss: 0.5420 - accuracy: 0.9067\n","Epoch 50/116\n","1/1 [==============================] - 0s 161ms/step - loss: 0.5393 - accuracy: 0.9067\n","Epoch 51/116\n","1/1 [==============================] - 0s 162ms/step - loss: 0.5377 - accuracy: 0.9067\n","Epoch 52/116\n","1/1 [==============================] - 0s 168ms/step - loss: 0.5366 - accuracy: 0.9067\n","Epoch 53/116\n","1/1 [==============================] - 0s 167ms/step - loss: 0.5356 - accuracy: 0.9067\n","Epoch 54/116\n","1/1 [==============================] - 0s 190ms/step - loss: 0.5345 - accuracy: 0.9067\n","Epoch 55/116\n","1/1 [==============================] - 0s 174ms/step - loss: 0.5330 - accuracy: 0.9067\n","Epoch 56/116\n","1/1 [==============================] - 0s 187ms/step - loss: 0.5311 - accuracy: 0.9067\n","Epoch 57/116\n","1/1 [==============================] - 0s 187ms/step - loss: 0.5288 - accuracy: 0.9067\n","Epoch 58/116\n","1/1 [==============================] - 0s 180ms/step - loss: 0.5265 - accuracy: 0.9067\n","Epoch 59/116\n","1/1 [==============================] - 0s 178ms/step - loss: 0.5241 - accuracy: 0.9067\n","Epoch 60/116\n","1/1 [==============================] - 0s 172ms/step - loss: 0.5220 - accuracy: 0.9087\n","Epoch 61/116\n","1/1 [==============================] - 0s 170ms/step - loss: 0.5200 - accuracy: 0.9087\n","Epoch 62/116\n","1/1 [==============================] - 0s 179ms/step - loss: 0.5181 - accuracy: 0.9087\n","Epoch 63/116\n","1/1 [==============================] - 0s 166ms/step - loss: 0.5162 - accuracy: 0.9087\n","Epoch 64/116\n","1/1 [==============================] - 0s 236ms/step - loss: 0.5142 - accuracy: 0.9127\n","Epoch 65/116\n","1/1 [==============================] - 0s 233ms/step - loss: 0.5120 - accuracy: 0.9127\n","Epoch 66/116\n","1/1 [==============================] - 0s 208ms/step - loss: 0.5096 - accuracy: 0.9127\n","Epoch 67/116\n","1/1 [==============================] - 0s 254ms/step - loss: 0.5070 - accuracy: 0.9147\n","Epoch 68/116\n","1/1 [==============================] - 0s 237ms/step - loss: 0.5042 - accuracy: 0.9147\n","Epoch 69/116\n","1/1 [==============================] - 0s 173ms/step - loss: 0.5013 - accuracy: 0.9127\n","Epoch 70/116\n","1/1 [==============================] - 0s 203ms/step - loss: 0.4985 - accuracy: 0.9167\n","Epoch 71/116\n","1/1 [==============================] - 0s 180ms/step - loss: 0.4956 - accuracy: 0.9187\n","Epoch 72/116\n","1/1 [==============================] - 0s 183ms/step - loss: 0.4930 - accuracy: 0.9187\n","Epoch 73/116\n","1/1 [==============================] - 0s 207ms/step - loss: 0.4910 - accuracy: 0.9206\n","Epoch 74/116\n","1/1 [==============================] - 0s 196ms/step - loss: 0.4896 - accuracy: 0.9226\n","Epoch 75/116\n","1/1 [==============================] - 0s 215ms/step - loss: 0.4867 - accuracy: 0.9246\n","Epoch 76/116\n","1/1 [==============================] - 0s 167ms/step - loss: 0.4836 - accuracy: 0.9226\n","Epoch 77/116\n","1/1 [==============================] - 0s 201ms/step - loss: 0.4813 - accuracy: 0.9226\n","Epoch 78/116\n","1/1 [==============================] - 0s 265ms/step - loss: 0.4796 - accuracy: 0.9266\n","Epoch 79/116\n","1/1 [==============================] - 0s 275ms/step - loss: 0.4778 - accuracy: 0.9266\n","Epoch 80/116\n","1/1 [==============================] - 0s 167ms/step - loss: 0.4752 - accuracy: 0.9286\n","Epoch 81/116\n","1/1 [==============================] - 0s 168ms/step - loss: 0.4726 - accuracy: 0.9286\n","Epoch 82/116\n","1/1 [==============================] - 0s 168ms/step - loss: nan - accuracy: 0.9266\n","Epoch 83/116\n","1/1 [==============================] - 0s 258ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/116\n","1/1 [==============================] - 0s 183ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/116\n","1/1 [==============================] - 0s 203ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/116\n","1/1 [==============================] - 0s 227ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/116\n","1/1 [==============================] - 0s 175ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/116\n","1/1 [==============================] - 0s 298ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/116\n","1/1 [==============================] - 0s 228ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/116\n","1/1 [==============================] - 0s 190ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/116\n","1/1 [==============================] - 0s 220ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/116\n","1/1 [==============================] - 0s 212ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/116\n","1/1 [==============================] - 0s 228ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/116\n","1/1 [==============================] - 0s 197ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/116\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/116\n","1/1 [==============================] - 0s 165ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/116\n","1/1 [==============================] - 0s 203ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/116\n","1/1 [==============================] - 0s 197ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/116\n","1/1 [==============================] - 0s 209ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/116\n","1/1 [==============================] - 0s 170ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/116\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/116\n","1/1 [==============================] - 0s 178ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/116\n","1/1 [==============================] - 0s 244ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/116\n","1/1 [==============================] - 0s 234ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/116\n","1/1 [==============================] - 0s 240ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/116\n","1/1 [==============================] - 0s 169ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/116\n","1/1 [==============================] - 0s 152ms/step - loss: nan - accuracy: 0.9067\n","Epoch 108/116\n","1/1 [==============================] - 0s 158ms/step - loss: nan - accuracy: 0.9067\n","Epoch 109/116\n","1/1 [==============================] - 0s 151ms/step - loss: nan - accuracy: 0.9067\n","Epoch 110/116\n","1/1 [==============================] - 0s 198ms/step - loss: nan - accuracy: 0.9067\n","Epoch 111/116\n","1/1 [==============================] - 0s 298ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/116\n","1/1 [==============================] - 0s 142ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/116\n","1/1 [==============================] - 0s 158ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/116\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/116\n","1/1 [==============================] - 0s 199ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/116\n","1/1 [==============================] - 0s 199ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b1c300310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:55:53,089]\u001b[0m Trial 13 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 166, 'num_epochs': 116}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/82\n","1/1 [==============================] - 6s 6s/step - loss: 14.6889 - accuracy: 0.0159\n","Epoch 2/82\n","1/1 [==============================] - 0s 415ms/step - loss: 10.4693 - accuracy: 0.0139\n","Epoch 3/82\n","1/1 [==============================] - 0s 301ms/step - loss: 1.6247 - accuracy: 0.6706\n","Epoch 4/82\n","1/1 [==============================] - 0s 263ms/step - loss: 1.5307 - accuracy: 0.9067\n","Epoch 5/82\n","1/1 [==============================] - 0s 325ms/step - loss: 1.5465 - accuracy: 0.9067\n","Epoch 6/82\n","1/1 [==============================] - 0s 397ms/step - loss: 1.5500 - accuracy: 0.9067\n","Epoch 7/82\n","1/1 [==============================] - 0s 216ms/step - loss: 1.5313 - accuracy: 0.9067\n","Epoch 8/82\n","1/1 [==============================] - 0s 305ms/step - loss: 1.5040 - accuracy: 0.9067\n","Epoch 9/82\n","1/1 [==============================] - 0s 241ms/step - loss: 1.4732 - accuracy: 0.9067\n","Epoch 10/82\n","1/1 [==============================] - 0s 404ms/step - loss: 1.4458 - accuracy: 0.9067\n","Epoch 11/82\n","1/1 [==============================] - 0s 332ms/step - loss: 1.3989 - accuracy: 0.9067\n","Epoch 12/82\n","1/1 [==============================] - 0s 190ms/step - loss: 1.3598 - accuracy: 0.9067\n","Epoch 13/82\n","1/1 [==============================] - 0s 325ms/step - loss: 1.3157 - accuracy: 0.9067\n","Epoch 14/82\n","1/1 [==============================] - 0s 297ms/step - loss: 1.2671 - accuracy: 0.9067\n","Epoch 15/82\n","1/1 [==============================] - 0s 389ms/step - loss: 1.2160 - accuracy: 0.9067\n","Epoch 16/82\n","1/1 [==============================] - 0s 353ms/step - loss: 1.1605 - accuracy: 0.9067\n","Epoch 17/82\n","1/1 [==============================] - 0s 374ms/step - loss: 1.1151 - accuracy: 0.9067\n","Epoch 18/82\n","1/1 [==============================] - 0s 363ms/step - loss: 1.0466 - accuracy: 0.9067\n","Epoch 19/82\n","1/1 [==============================] - 0s 414ms/step - loss: 0.9786 - accuracy: 0.9067\n","Epoch 20/82\n","1/1 [==============================] - 0s 382ms/step - loss: 0.9166 - accuracy: 0.9067\n","Epoch 21/82\n","1/1 [==============================] - 0s 295ms/step - loss: 0.8666 - accuracy: 0.9067\n","Epoch 22/82\n","1/1 [==============================] - 0s 319ms/step - loss: 0.8288 - accuracy: 0.9067\n","Epoch 23/82\n","1/1 [==============================] - 0s 336ms/step - loss: 0.8466 - accuracy: 0.9067\n","Epoch 24/82\n","1/1 [==============================] - 0s 299ms/step - loss: 0.8732 - accuracy: 0.9067\n","Epoch 25/82\n","1/1 [==============================] - 0s 446ms/step - loss: 0.9131 - accuracy: 0.9067\n","Epoch 26/82\n","1/1 [==============================] - 0s 352ms/step - loss: 0.9830 - accuracy: 0.9067\n","Epoch 27/82\n","1/1 [==============================] - 0s 278ms/step - loss: 1.0050 - accuracy: 0.9067\n","Epoch 28/82\n","1/1 [==============================] - 0s 301ms/step - loss: 1.0045 - accuracy: 0.9067\n","Epoch 29/82\n","1/1 [==============================] - 1s 662ms/step - loss: 1.0026 - accuracy: 0.9067\n","Epoch 30/82\n","1/1 [==============================] - 0s 343ms/step - loss: 0.9997 - accuracy: 0.9067\n","Epoch 31/82\n","1/1 [==============================] - 1s 643ms/step - loss: 0.9978 - accuracy: 0.9067\n","Epoch 32/82\n","1/1 [==============================] - 1s 658ms/step - loss: 0.9964 - accuracy: 0.9067\n","Epoch 33/82\n","1/1 [==============================] - 0s 338ms/step - loss: 0.9936 - accuracy: 0.9067\n","Epoch 34/82\n","1/1 [==============================] - 0s 429ms/step - loss: 0.9712 - accuracy: 0.9067\n","Epoch 35/82\n","1/1 [==============================] - 0s 392ms/step - loss: 0.9873 - accuracy: 0.9067\n","Epoch 36/82\n","1/1 [==============================] - 0s 298ms/step - loss: 0.9754 - accuracy: 0.9067\n","Epoch 37/82\n","1/1 [==============================] - 0s 336ms/step - loss: 0.9896 - accuracy: 0.9067\n","Epoch 38/82\n","1/1 [==============================] - 0s 384ms/step - loss: 0.9873 - accuracy: 0.9067\n","Epoch 39/82\n","1/1 [==============================] - 0s 351ms/step - loss: 0.9852 - accuracy: 0.9067\n","Epoch 40/82\n","1/1 [==============================] - 0s 397ms/step - loss: 0.9845 - accuracy: 0.9028\n","Epoch 41/82\n","1/1 [==============================] - 1s 639ms/step - loss: 0.9847 - accuracy: 0.9028\n","Epoch 42/82\n","1/1 [==============================] - 1s 1s/step - loss: 0.9831 - accuracy: 0.9028\n","Epoch 43/82\n","1/1 [==============================] - 1s 521ms/step - loss: 0.9816 - accuracy: 0.9048\n","Epoch 44/82\n","1/1 [==============================] - 0s 247ms/step - loss: 0.9806 - accuracy: 0.9067\n","Epoch 45/82\n","1/1 [==============================] - 0s 450ms/step - loss: 0.9793 - accuracy: 0.9067\n","Epoch 46/82\n","1/1 [==============================] - 1s 1s/step - loss: 0.9778 - accuracy: 0.9048\n","Epoch 47/82\n","1/1 [==============================] - 0s 489ms/step - loss: 0.9765 - accuracy: 0.9028\n","Epoch 48/82\n","1/1 [==============================] - 0s 400ms/step - loss: 0.9755 - accuracy: 0.9028\n","Epoch 49/82\n","1/1 [==============================] - 0s 330ms/step - loss: 0.9740 - accuracy: 0.9028\n","Epoch 50/82\n","1/1 [==============================] - 0s 286ms/step - loss: 0.9511 - accuracy: 0.9048\n","Epoch 51/82\n","1/1 [==============================] - 0s 352ms/step - loss: 1.0909 - accuracy: 0.8849\n","Epoch 52/82\n","1/1 [==============================] - 0s 297ms/step - loss: 1.0129 - accuracy: 0.8730\n","Epoch 53/82\n","1/1 [==============================] - 0s 320ms/step - loss: 0.7037 - accuracy: 0.8869\n","Epoch 54/82\n","1/1 [==============================] - 0s 350ms/step - loss: 0.6831 - accuracy: 0.9067\n","Epoch 55/82\n","1/1 [==============================] - 0s 462ms/step - loss: 0.6880 - accuracy: 0.9067\n","Epoch 56/82\n","1/1 [==============================] - 1s 757ms/step - loss: 0.6852 - accuracy: 0.9067\n","Epoch 57/82\n","1/1 [==============================] - 0s 493ms/step - loss: 0.6790 - accuracy: 0.9067\n","Epoch 58/82\n","1/1 [==============================] - 0s 261ms/step - loss: 0.6748 - accuracy: 0.9067\n","Epoch 59/82\n","1/1 [==============================] - 1s 548ms/step - loss: 0.6720 - accuracy: 0.9067\n","Epoch 60/82\n","1/1 [==============================] - 0s 448ms/step - loss: 0.6733 - accuracy: 0.9067\n","Epoch 61/82\n","1/1 [==============================] - 1s 665ms/step - loss: 0.6741 - accuracy: 0.9067\n","Epoch 62/82\n","1/1 [==============================] - 0s 366ms/step - loss: 0.6635 - accuracy: 0.9067\n","Epoch 63/82\n","1/1 [==============================] - 0s 300ms/step - loss: 0.6772 - accuracy: 0.9067\n","Epoch 64/82\n","1/1 [==============================] - 0s 410ms/step - loss: 0.6828 - accuracy: 0.9067\n","Epoch 65/82\n","1/1 [==============================] - 0s 317ms/step - loss: 0.6780 - accuracy: 0.9067\n","Epoch 66/82\n","1/1 [==============================] - 0s 232ms/step - loss: 0.6694 - accuracy: 0.9067\n","Epoch 67/82\n","1/1 [==============================] - 0s 272ms/step - loss: 0.6621 - accuracy: 0.9067\n","Epoch 68/82\n","1/1 [==============================] - 0s 259ms/step - loss: 0.6781 - accuracy: 0.9067\n","Epoch 69/82\n","1/1 [==============================] - 0s 257ms/step - loss: 0.6713 - accuracy: 0.9067\n","Epoch 70/82\n","1/1 [==============================] - 1s 501ms/step - loss: 0.6663 - accuracy: 0.9067\n","Epoch 71/82\n","1/1 [==============================] - 0s 370ms/step - loss: 0.6635 - accuracy: 0.9067\n","Epoch 72/82\n","1/1 [==============================] - 0s 240ms/step - loss: 0.6636 - accuracy: 0.9067\n","Epoch 73/82\n","1/1 [==============================] - 0s 382ms/step - loss: 0.6676 - accuracy: 0.9067\n","Epoch 74/82\n","1/1 [==============================] - 0s 315ms/step - loss: 0.6671 - accuracy: 0.9067\n","Epoch 75/82\n","1/1 [==============================] - 0s 289ms/step - loss: 0.6612 - accuracy: 0.9067\n","Epoch 76/82\n","1/1 [==============================] - 0s 367ms/step - loss: 0.6594 - accuracy: 0.9067\n","Epoch 77/82\n","1/1 [==============================] - 0s 369ms/step - loss: 0.6591 - accuracy: 0.9067\n","Epoch 78/82\n","1/1 [==============================] - 0s 383ms/step - loss: 0.6589 - accuracy: 0.9067\n","Epoch 79/82\n","1/1 [==============================] - 0s 255ms/step - loss: 0.6588 - accuracy: 0.9067\n","Epoch 80/82\n","1/1 [==============================] - 0s 330ms/step - loss: 0.6586 - accuracy: 0.9067\n","Epoch 81/82\n","1/1 [==============================] - 0s 388ms/step - loss: 0.6582 - accuracy: 0.9067\n","Epoch 82/82\n","1/1 [==============================] - 1s 583ms/step - loss: 0.6576 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b273789d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1311 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:56:32,999]\u001b[0m Trial 14 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 225, 'num_epochs': 82}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/53\n","1/1 [==============================] - 5s 5s/step - loss: 4.8488 - accuracy: 0.0060\n","Epoch 2/53\n","1/1 [==============================] - 0s 341ms/step - loss: 0.9906 - accuracy: 0.9067\n","Epoch 3/53\n","1/1 [==============================] - 0s 343ms/step - loss: 0.8678 - accuracy: 0.9067\n","Epoch 4/53\n","1/1 [==============================] - 0s 382ms/step - loss: 0.8022 - accuracy: 0.9067\n","Epoch 5/53\n","1/1 [==============================] - 0s 378ms/step - loss: 0.7634 - accuracy: 0.9067\n","Epoch 6/53\n","1/1 [==============================] - 0s 268ms/step - loss: 0.7451 - accuracy: 0.9067\n","Epoch 7/53\n","1/1 [==============================] - 0s 304ms/step - loss: 0.7021 - accuracy: 0.9067\n","Epoch 8/53\n","1/1 [==============================] - 0s 376ms/step - loss: 0.7462 - accuracy: 0.9067\n","Epoch 9/53\n","1/1 [==============================] - 0s 445ms/step - loss: 0.7014 - accuracy: 0.9067\n","Epoch 10/53\n","1/1 [==============================] - 0s 375ms/step - loss: 0.6974 - accuracy: 0.9067\n","Epoch 11/53\n","1/1 [==============================] - 0s 320ms/step - loss: 0.6900 - accuracy: 0.9067\n","Epoch 12/53\n","1/1 [==============================] - 0s 307ms/step - loss: 0.6801 - accuracy: 0.9067\n","Epoch 13/53\n","1/1 [==============================] - 0s 314ms/step - loss: 0.6710 - accuracy: 0.9067\n","Epoch 14/53\n","1/1 [==============================] - 0s 338ms/step - loss: 0.6649 - accuracy: 0.9067\n","Epoch 15/53\n","1/1 [==============================] - 0s 396ms/step - loss: 0.6670 - accuracy: 0.9067\n","Epoch 16/53\n","1/1 [==============================] - 0s 362ms/step - loss: 0.6627 - accuracy: 0.9067\n","Epoch 17/53\n","1/1 [==============================] - 1s 519ms/step - loss: 0.6543 - accuracy: 0.9067\n","Epoch 18/53\n","1/1 [==============================] - 0s 359ms/step - loss: 0.6496 - accuracy: 0.9067\n","Epoch 19/53\n","1/1 [==============================] - 0s 380ms/step - loss: 0.6489 - accuracy: 0.9067\n","Epoch 20/53\n","1/1 [==============================] - 1s 506ms/step - loss: 0.6500 - accuracy: 0.9067\n","Epoch 21/53\n","1/1 [==============================] - 0s 345ms/step - loss: 0.6507 - accuracy: 0.9067\n","Epoch 22/53\n","1/1 [==============================] - 0s 466ms/step - loss: 0.6498 - accuracy: 0.9067\n","Epoch 23/53\n","1/1 [==============================] - 1s 516ms/step - loss: 0.6471 - accuracy: 0.9067\n","Epoch 24/53\n","1/1 [==============================] - 0s 407ms/step - loss: 0.6433 - accuracy: 0.9067\n","Epoch 25/53\n","1/1 [==============================] - 0s 245ms/step - loss: 0.6391 - accuracy: 0.9067\n","Epoch 26/53\n","1/1 [==============================] - 0s 303ms/step - loss: 0.6146 - accuracy: 0.9067\n","Epoch 27/53\n","1/1 [==============================] - 0s 430ms/step - loss: 0.5865 - accuracy: 0.9067\n","Epoch 28/53\n","1/1 [==============================] - 1s 549ms/step - loss: 0.5961 - accuracy: 0.9067\n","Epoch 29/53\n","1/1 [==============================] - 0s 443ms/step - loss: 0.5943 - accuracy: 0.9067\n","Epoch 30/53\n","1/1 [==============================] - 0s 299ms/step - loss: 0.5829 - accuracy: 0.9067\n","Epoch 31/53\n","1/1 [==============================] - 0s 318ms/step - loss: 0.5746 - accuracy: 0.9067\n","Epoch 32/53\n","1/1 [==============================] - 0s 293ms/step - loss: 0.5724 - accuracy: 0.9067\n","Epoch 33/53\n","1/1 [==============================] - 0s 307ms/step - loss: 0.5468 - accuracy: 0.9087\n","Epoch 34/53\n","1/1 [==============================] - 0s 301ms/step - loss: 0.5376 - accuracy: 0.9107\n","Epoch 35/53\n","1/1 [==============================] - 0s 273ms/step - loss: 0.5057 - accuracy: 0.9147\n","Epoch 36/53\n","1/1 [==============================] - 1s 636ms/step - loss: 0.4814 - accuracy: 0.9187\n","Epoch 37/53\n","1/1 [==============================] - 1s 526ms/step - loss: 0.4748 - accuracy: 0.9107\n","Epoch 38/53\n","1/1 [==============================] - 1s 536ms/step - loss: 0.4657 - accuracy: 0.9107\n","Epoch 39/53\n","1/1 [==============================] - 0s 287ms/step - loss: 0.4492 - accuracy: 0.9187\n","Epoch 40/53\n","1/1 [==============================] - 0s 366ms/step - loss: 0.4929 - accuracy: 0.9167\n","Epoch 41/53\n","1/1 [==============================] - 0s 421ms/step - loss: 0.5124 - accuracy: 0.9167\n","Epoch 42/53\n","1/1 [==============================] - 0s 452ms/step - loss: 0.5091 - accuracy: 0.9206\n","Epoch 43/53\n","1/1 [==============================] - 1s 801ms/step - loss: 0.5084 - accuracy: 0.9226\n","Epoch 44/53\n","1/1 [==============================] - 1s 535ms/step - loss: 0.5008 - accuracy: 0.9187\n","Epoch 45/53\n","1/1 [==============================] - 0s 383ms/step - loss: 0.5017 - accuracy: 0.9226\n","Epoch 46/53\n","1/1 [==============================] - 0s 407ms/step - loss: 0.5253 - accuracy: 0.9226\n","Epoch 47/53\n","1/1 [==============================] - 0s 305ms/step - loss: 0.5136 - accuracy: 0.9226\n","Epoch 48/53\n","1/1 [==============================] - 0s 356ms/step - loss: 0.5500 - accuracy: 0.9286\n","Epoch 49/53\n","1/1 [==============================] - 0s 394ms/step - loss: 0.5662 - accuracy: 0.9067\n","Epoch 50/53\n","1/1 [==============================] - 1s 715ms/step - loss: 0.6208 - accuracy: 0.9067\n","Epoch 51/53\n","1/1 [==============================] - 0s 382ms/step - loss: 0.6296 - accuracy: 0.9067\n","Epoch 52/53\n","1/1 [==============================] - 1s 593ms/step - loss: 0.6316 - accuracy: 0.9067\n","Epoch 53/53\n","1/1 [==============================] - 1s 634ms/step - loss: 0.6283 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b244d8790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1587 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:57:01,590]\u001b[0m Trial 15 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 253, 'num_epochs': 53}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/133\n","1/1 [==============================] - 5s 5s/step - loss: 5.2913 - accuracy: 0.0079\n","Epoch 2/133\n","1/1 [==============================] - 0s 166ms/step - loss: 1.0956 - accuracy: 0.9067\n","Epoch 3/133\n","1/1 [==============================] - 0s 250ms/step - loss: 0.8593 - accuracy: 0.9067\n","Epoch 4/133\n","1/1 [==============================] - 0s 200ms/step - loss: 0.7222 - accuracy: 0.9067\n","Epoch 5/133\n","1/1 [==============================] - 0s 189ms/step - loss: 0.6561 - accuracy: 0.9067\n","Epoch 6/133\n","1/1 [==============================] - 0s 195ms/step - loss: 0.6597 - accuracy: 0.9067\n","Epoch 7/133\n","1/1 [==============================] - 0s 201ms/step - loss: 0.6346 - accuracy: 0.9067\n","Epoch 8/133\n","1/1 [==============================] - 0s 213ms/step - loss: 0.7178 - accuracy: 0.9067\n","Epoch 9/133\n","1/1 [==============================] - 0s 196ms/step - loss: 0.7116 - accuracy: 0.9067\n","Epoch 10/133\n","1/1 [==============================] - 0s 174ms/step - loss: 0.7081 - accuracy: 0.9067\n","Epoch 11/133\n","1/1 [==============================] - 0s 178ms/step - loss: 0.7042 - accuracy: 0.9067\n","Epoch 12/133\n","1/1 [==============================] - 0s 189ms/step - loss: 0.7059 - accuracy: 0.9067\n","Epoch 13/133\n","1/1 [==============================] - 0s 181ms/step - loss: 0.7031 - accuracy: 0.9067\n","Epoch 14/133\n","1/1 [==============================] - 0s 188ms/step - loss: 0.6971 - accuracy: 0.9067\n","Epoch 15/133\n","1/1 [==============================] - 0s 163ms/step - loss: 0.6963 - accuracy: 0.9067\n","Epoch 16/133\n","1/1 [==============================] - 0s 147ms/step - loss: 0.6968 - accuracy: 0.9067\n","Epoch 17/133\n","1/1 [==============================] - 0s 145ms/step - loss: 0.6897 - accuracy: 0.9067\n","Epoch 18/133\n","1/1 [==============================] - 0s 163ms/step - loss: 0.6895 - accuracy: 0.9067\n","Epoch 19/133\n","1/1 [==============================] - 0s 171ms/step - loss: 0.6896 - accuracy: 0.9067\n","Epoch 20/133\n","1/1 [==============================] - 0s 168ms/step - loss: 0.6854 - accuracy: 0.9067\n","Epoch 21/133\n","1/1 [==============================] - 0s 151ms/step - loss: 0.6799 - accuracy: 0.9067\n","Epoch 22/133\n","1/1 [==============================] - 0s 149ms/step - loss: 0.6777 - accuracy: 0.9067\n","Epoch 23/133\n","1/1 [==============================] - 0s 231ms/step - loss: 0.6760 - accuracy: 0.9067\n","Epoch 24/133\n","1/1 [==============================] - 0s 148ms/step - loss: 0.6720 - accuracy: 0.9067\n","Epoch 25/133\n","1/1 [==============================] - 0s 190ms/step - loss: 0.6672 - accuracy: 0.9067\n","Epoch 26/133\n","1/1 [==============================] - 0s 216ms/step - loss: 0.6631 - accuracy: 0.9067\n","Epoch 27/133\n","1/1 [==============================] - 0s 226ms/step - loss: 0.6589 - accuracy: 0.9067\n","Epoch 28/133\n","1/1 [==============================] - 0s 235ms/step - loss: 0.6538 - accuracy: 0.9087\n","Epoch 29/133\n","1/1 [==============================] - 0s 156ms/step - loss: 0.6476 - accuracy: 0.9147\n","Epoch 30/133\n","1/1 [==============================] - 0s 183ms/step - loss: 0.6387 - accuracy: 0.9167\n","Epoch 31/133\n","1/1 [==============================] - 0s 199ms/step - loss: 0.6891 - accuracy: 0.9187\n","Epoch 32/133\n","1/1 [==============================] - 0s 169ms/step - loss: nan - accuracy: 0.9187\n","Epoch 33/133\n","1/1 [==============================] - 0s 196ms/step - loss: nan - accuracy: 0.9067\n","Epoch 34/133\n","1/1 [==============================] - 0s 177ms/step - loss: nan - accuracy: 0.9067\n","Epoch 35/133\n","1/1 [==============================] - 0s 187ms/step - loss: nan - accuracy: 0.9067\n","Epoch 36/133\n","1/1 [==============================] - 0s 155ms/step - loss: nan - accuracy: 0.9067\n","Epoch 37/133\n","1/1 [==============================] - 0s 183ms/step - loss: nan - accuracy: 0.9067\n","Epoch 38/133\n","1/1 [==============================] - 0s 179ms/step - loss: nan - accuracy: 0.9067\n","Epoch 39/133\n","1/1 [==============================] - 0s 188ms/step - loss: nan - accuracy: 0.9067\n","Epoch 40/133\n","1/1 [==============================] - 0s 178ms/step - loss: nan - accuracy: 0.9067\n","Epoch 41/133\n","1/1 [==============================] - 0s 154ms/step - loss: nan - accuracy: 0.9067\n","Epoch 42/133\n","1/1 [==============================] - 0s 153ms/step - loss: nan - accuracy: 0.9067\n","Epoch 43/133\n","1/1 [==============================] - 0s 152ms/step - loss: nan - accuracy: 0.9067\n","Epoch 44/133\n","1/1 [==============================] - 0s 163ms/step - loss: nan - accuracy: 0.9067\n","Epoch 45/133\n","1/1 [==============================] - 0s 149ms/step - loss: nan - accuracy: 0.9067\n","Epoch 46/133\n","1/1 [==============================] - 0s 137ms/step - loss: nan - accuracy: 0.9067\n","Epoch 47/133\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 48/133\n","1/1 [==============================] - 0s 170ms/step - loss: nan - accuracy: 0.9067\n","Epoch 49/133\n","1/1 [==============================] - 0s 160ms/step - loss: nan - accuracy: 0.9067\n","Epoch 50/133\n","1/1 [==============================] - 0s 154ms/step - loss: nan - accuracy: 0.9067\n","Epoch 51/133\n","1/1 [==============================] - 0s 157ms/step - loss: nan - accuracy: 0.9067\n","Epoch 52/133\n","1/1 [==============================] - 0s 171ms/step - loss: nan - accuracy: 0.9067\n","Epoch 53/133\n","1/1 [==============================] - 0s 159ms/step - loss: nan - accuracy: 0.9067\n","Epoch 54/133\n","1/1 [==============================] - 0s 168ms/step - loss: nan - accuracy: 0.9067\n","Epoch 55/133\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 56/133\n","1/1 [==============================] - 0s 172ms/step - loss: nan - accuracy: 0.9067\n","Epoch 57/133\n","1/1 [==============================] - 0s 160ms/step - loss: nan - accuracy: 0.9067\n","Epoch 58/133\n","1/1 [==============================] - 0s 198ms/step - loss: nan - accuracy: 0.9067\n","Epoch 59/133\n","1/1 [==============================] - 0s 211ms/step - loss: nan - accuracy: 0.9067\n","Epoch 60/133\n","1/1 [==============================] - 0s 205ms/step - loss: nan - accuracy: 0.9067\n","Epoch 61/133\n","1/1 [==============================] - 0s 185ms/step - loss: nan - accuracy: 0.9067\n","Epoch 62/133\n","1/1 [==============================] - 0s 169ms/step - loss: nan - accuracy: 0.9067\n","Epoch 63/133\n","1/1 [==============================] - 0s 196ms/step - loss: nan - accuracy: 0.9067\n","Epoch 64/133\n","1/1 [==============================] - 0s 203ms/step - loss: nan - accuracy: 0.9067\n","Epoch 65/133\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","Epoch 66/133\n","1/1 [==============================] - 0s 171ms/step - loss: nan - accuracy: 0.9067\n","Epoch 67/133\n","1/1 [==============================] - 0s 155ms/step - loss: nan - accuracy: 0.9067\n","Epoch 68/133\n","1/1 [==============================] - 0s 179ms/step - loss: nan - accuracy: 0.9067\n","Epoch 69/133\n","1/1 [==============================] - 0s 167ms/step - loss: nan - accuracy: 0.9067\n","Epoch 70/133\n","1/1 [==============================] - 0s 272ms/step - loss: nan - accuracy: 0.9067\n","Epoch 71/133\n","1/1 [==============================] - 0s 177ms/step - loss: nan - accuracy: 0.9067\n","Epoch 72/133\n","1/1 [==============================] - 0s 221ms/step - loss: nan - accuracy: 0.9067\n","Epoch 73/133\n","1/1 [==============================] - 0s 241ms/step - loss: nan - accuracy: 0.9067\n","Epoch 74/133\n","1/1 [==============================] - 0s 209ms/step - loss: nan - accuracy: 0.9067\n","Epoch 75/133\n","1/1 [==============================] - 0s 237ms/step - loss: nan - accuracy: 0.9067\n","Epoch 76/133\n","1/1 [==============================] - 0s 234ms/step - loss: nan - accuracy: 0.9067\n","Epoch 77/133\n","1/1 [==============================] - 0s 190ms/step - loss: nan - accuracy: 0.9067\n","Epoch 78/133\n","1/1 [==============================] - 0s 228ms/step - loss: nan - accuracy: 0.9067\n","Epoch 79/133\n","1/1 [==============================] - 0s 188ms/step - loss: nan - accuracy: 0.9067\n","Epoch 80/133\n","1/1 [==============================] - 0s 243ms/step - loss: nan - accuracy: 0.9067\n","Epoch 81/133\n","1/1 [==============================] - 0s 209ms/step - loss: nan - accuracy: 0.9067\n","Epoch 82/133\n","1/1 [==============================] - 0s 230ms/step - loss: nan - accuracy: 0.9067\n","Epoch 83/133\n","1/1 [==============================] - 0s 192ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/133\n","1/1 [==============================] - 0s 243ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/133\n","1/1 [==============================] - 0s 242ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/133\n","1/1 [==============================] - 0s 271ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/133\n","1/1 [==============================] - 0s 259ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/133\n","1/1 [==============================] - 0s 244ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/133\n","1/1 [==============================] - 0s 262ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/133\n","1/1 [==============================] - 0s 236ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/133\n","1/1 [==============================] - 0s 237ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/133\n","1/1 [==============================] - 0s 246ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/133\n","1/1 [==============================] - 0s 258ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/133\n","1/1 [==============================] - 0s 214ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/133\n","1/1 [==============================] - 0s 275ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/133\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/133\n","1/1 [==============================] - 0s 213ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/133\n","1/1 [==============================] - 0s 190ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/133\n","1/1 [==============================] - 0s 227ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/133\n","1/1 [==============================] - 0s 236ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/133\n","1/1 [==============================] - 0s 274ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/133\n","1/1 [==============================] - 0s 155ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/133\n","1/1 [==============================] - 0s 184ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/133\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/133\n","1/1 [==============================] - 0s 222ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/133\n","1/1 [==============================] - 0s 196ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/133\n","1/1 [==============================] - 0s 240ms/step - loss: nan - accuracy: 0.9067\n","Epoch 108/133\n","1/1 [==============================] - 0s 240ms/step - loss: nan - accuracy: 0.9067\n","Epoch 109/133\n","1/1 [==============================] - 0s 241ms/step - loss: nan - accuracy: 0.9067\n","Epoch 110/133\n","1/1 [==============================] - 0s 254ms/step - loss: nan - accuracy: 0.9067\n","Epoch 111/133\n","1/1 [==============================] - 0s 236ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/133\n","1/1 [==============================] - 0s 405ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/133\n","1/1 [==============================] - 0s 187ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/133\n","1/1 [==============================] - 0s 244ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/133\n","1/1 [==============================] - 0s 202ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/133\n","1/1 [==============================] - 0s 193ms/step - loss: nan - accuracy: 0.9067\n","Epoch 117/133\n","1/1 [==============================] - 0s 258ms/step - loss: nan - accuracy: 0.9067\n","Epoch 118/133\n","1/1 [==============================] - 0s 253ms/step - loss: nan - accuracy: 0.9067\n","Epoch 119/133\n","1/1 [==============================] - 0s 278ms/step - loss: nan - accuracy: 0.9067\n","Epoch 120/133\n","1/1 [==============================] - 0s 227ms/step - loss: nan - accuracy: 0.9067\n","Epoch 121/133\n","1/1 [==============================] - 0s 230ms/step - loss: nan - accuracy: 0.9067\n","Epoch 122/133\n","1/1 [==============================] - 0s 222ms/step - loss: nan - accuracy: 0.9067\n","Epoch 123/133\n","1/1 [==============================] - 0s 257ms/step - loss: nan - accuracy: 0.9067\n","Epoch 124/133\n","1/1 [==============================] - 0s 214ms/step - loss: nan - accuracy: 0.9067\n","Epoch 125/133\n","1/1 [==============================] - 0s 274ms/step - loss: nan - accuracy: 0.9067\n","Epoch 126/133\n","1/1 [==============================] - 0s 234ms/step - loss: nan - accuracy: 0.9067\n","Epoch 127/133\n","1/1 [==============================] - 0s 241ms/step - loss: nan - accuracy: 0.9067\n","Epoch 128/133\n","1/1 [==============================] - 0s 213ms/step - loss: nan - accuracy: 0.9067\n","Epoch 129/133\n","1/1 [==============================] - 0s 213ms/step - loss: nan - accuracy: 0.9067\n","Epoch 130/133\n","1/1 [==============================] - 0s 284ms/step - loss: nan - accuracy: 0.9067\n","Epoch 131/133\n","1/1 [==============================] - 0s 249ms/step - loss: nan - accuracy: 0.9067\n","Epoch 132/133\n","1/1 [==============================] - 0s 261ms/step - loss: nan - accuracy: 0.9067\n","Epoch 133/133\n","1/1 [==============================] - 0s 199ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b4544a820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:57:36,834]\u001b[0m Trial 16 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 190, 'num_epochs': 133}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/87\n","1/1 [==============================] - 6s 6s/step - loss: 5.0956 - accuracy: 0.0774\n","Epoch 2/87\n","1/1 [==============================] - 0s 153ms/step - loss: 1.3292 - accuracy: 0.9067\n","Epoch 3/87\n","1/1 [==============================] - 0s 229ms/step - loss: 1.1457 - accuracy: 0.9067\n","Epoch 4/87\n","1/1 [==============================] - 0s 207ms/step - loss: 1.0877 - accuracy: 0.9067\n","Epoch 5/87\n","1/1 [==============================] - 0s 315ms/step - loss: 1.0632 - accuracy: 0.9067\n","Epoch 6/87\n","1/1 [==============================] - 0s 202ms/step - loss: 1.1228 - accuracy: 0.9067\n","Epoch 7/87\n","1/1 [==============================] - 0s 251ms/step - loss: 1.1524 - accuracy: 0.9067\n","Epoch 8/87\n","1/1 [==============================] - 0s 210ms/step - loss: 1.1608 - accuracy: 0.9067\n","Epoch 9/87\n","1/1 [==============================] - 0s 189ms/step - loss: 1.1369 - accuracy: 0.9067\n","Epoch 10/87\n","1/1 [==============================] - 0s 247ms/step - loss: 1.0974 - accuracy: 0.9067\n","Epoch 11/87\n","1/1 [==============================] - 0s 224ms/step - loss: 1.0871 - accuracy: 0.9067\n","Epoch 12/87\n","1/1 [==============================] - 0s 247ms/step - loss: 1.0712 - accuracy: 0.9067\n","Epoch 13/87\n","1/1 [==============================] - 0s 289ms/step - loss: 1.0477 - accuracy: 0.9067\n","Epoch 14/87\n","1/1 [==============================] - 0s 422ms/step - loss: 1.0198 - accuracy: 0.9067\n","Epoch 15/87\n","1/1 [==============================] - 0s 242ms/step - loss: 0.9936 - accuracy: 0.9067\n","Epoch 16/87\n","1/1 [==============================] - 0s 286ms/step - loss: 0.9723 - accuracy: 0.9067\n","Epoch 17/87\n","1/1 [==============================] - 0s 269ms/step - loss: 0.9589 - accuracy: 0.9067\n","Epoch 18/87\n","1/1 [==============================] - 0s 373ms/step - loss: 0.9727 - accuracy: 0.9067\n","Epoch 19/87\n","1/1 [==============================] - 0s 348ms/step - loss: 0.9954 - accuracy: 0.9067\n","Epoch 20/87\n","1/1 [==============================] - 0s 385ms/step - loss: 1.0187 - accuracy: 0.9067\n","Epoch 21/87\n","1/1 [==============================] - 0s 345ms/step - loss: 1.0327 - accuracy: 0.9067\n","Epoch 22/87\n","1/1 [==============================] - 0s 198ms/step - loss: 0.9817 - accuracy: 0.9067\n","Epoch 23/87\n","1/1 [==============================] - 0s 226ms/step - loss: 1.0014 - accuracy: 0.9067\n","Epoch 24/87\n","1/1 [==============================] - 0s 272ms/step - loss: 1.0495 - accuracy: 0.9087\n","Epoch 25/87\n","1/1 [==============================] - 0s 345ms/step - loss: 1.0937 - accuracy: 0.9028\n","Epoch 26/87\n","1/1 [==============================] - 1s 503ms/step - loss: 1.1103 - accuracy: 0.8869\n","Epoch 27/87\n","1/1 [==============================] - 0s 248ms/step - loss: 1.1355 - accuracy: 0.8452\n","Epoch 28/87\n","1/1 [==============================] - 0s 439ms/step - loss: 1.1513 - accuracy: 0.8095\n","Epoch 29/87\n","1/1 [==============================] - 1s 526ms/step - loss: 1.1516 - accuracy: 0.7956\n","Epoch 30/87\n","1/1 [==============================] - 0s 275ms/step - loss: 1.1428 - accuracy: 0.7976\n","Epoch 31/87\n","1/1 [==============================] - 0s 413ms/step - loss: 1.1269 - accuracy: 0.8075\n","Epoch 32/87\n","1/1 [==============================] - 0s 361ms/step - loss: 1.1029 - accuracy: 0.8294\n","Epoch 33/87\n","1/1 [==============================] - 0s 363ms/step - loss: 1.1042 - accuracy: 0.8492\n","Epoch 34/87\n","1/1 [==============================] - 0s 340ms/step - loss: 1.1612 - accuracy: 0.8810\n","Epoch 35/87\n","1/1 [==============================] - 0s 368ms/step - loss: 1.1168 - accuracy: 0.8492\n","Epoch 36/87\n","1/1 [==============================] - 0s 262ms/step - loss: 1.2254 - accuracy: 0.8333\n","Epoch 37/87\n","1/1 [==============================] - 0s 286ms/step - loss: 1.3048 - accuracy: 0.8214\n","Epoch 38/87\n","1/1 [==============================] - 0s 272ms/step - loss: 1.3574 - accuracy: 0.8254\n","Epoch 39/87\n","1/1 [==============================] - 0s 313ms/step - loss: 1.3920 - accuracy: 0.8274\n","Epoch 40/87\n","1/1 [==============================] - 0s 347ms/step - loss: 1.4130 - accuracy: 0.8433\n","Epoch 41/87\n","1/1 [==============================] - 0s 350ms/step - loss: 1.4236 - accuracy: 0.8552\n","Epoch 42/87\n","1/1 [==============================] - 1s 618ms/step - loss: 1.4239 - accuracy: 0.8790\n","Epoch 43/87\n","1/1 [==============================] - 0s 270ms/step - loss: 1.4168 - accuracy: 0.8909\n","Epoch 44/87\n","1/1 [==============================] - 0s 259ms/step - loss: 1.4044 - accuracy: 0.9028\n","Epoch 45/87\n","1/1 [==============================] - 0s 358ms/step - loss: 1.3873 - accuracy: 0.9048\n","Epoch 46/87\n","1/1 [==============================] - 0s 284ms/step - loss: 1.3663 - accuracy: 0.9067\n","Epoch 47/87\n","1/1 [==============================] - 0s 250ms/step - loss: 1.3427 - accuracy: 0.9087\n","Epoch 48/87\n","1/1 [==============================] - 0s 199ms/step - loss: 1.3160 - accuracy: 0.9067\n","Epoch 49/87\n","1/1 [==============================] - 0s 230ms/step - loss: 1.2874 - accuracy: 0.9067\n","Epoch 50/87\n","1/1 [==============================] - 0s 224ms/step - loss: 1.2567 - accuracy: 0.9067\n","Epoch 51/87\n","1/1 [==============================] - 0s 457ms/step - loss: 1.2246 - accuracy: 0.9067\n","Epoch 52/87\n","1/1 [==============================] - 0s 311ms/step - loss: 1.1918 - accuracy: 0.9067\n","Epoch 53/87\n","1/1 [==============================] - 0s 366ms/step - loss: 1.1588 - accuracy: 0.9067\n","Epoch 54/87\n","1/1 [==============================] - 0s 254ms/step - loss: 1.1265 - accuracy: 0.9067\n","Epoch 55/87\n","1/1 [==============================] - 0s 226ms/step - loss: 1.0954 - accuracy: 0.9067\n","Epoch 56/87\n","1/1 [==============================] - 0s 487ms/step - loss: 1.0660 - accuracy: 0.9067\n","Epoch 57/87\n","1/1 [==============================] - 0s 302ms/step - loss: 1.0388 - accuracy: 0.9067\n","Epoch 58/87\n","1/1 [==============================] - 1s 646ms/step - loss: 1.0142 - accuracy: 0.9087\n","Epoch 59/87\n","1/1 [==============================] - 0s 228ms/step - loss: 0.9927 - accuracy: 0.9067\n","Epoch 60/87\n","1/1 [==============================] - 0s 229ms/step - loss: 0.9760 - accuracy: 0.9067\n","Epoch 61/87\n","1/1 [==============================] - 0s 268ms/step - loss: 0.9843 - accuracy: 0.9067\n","Epoch 62/87\n","1/1 [==============================] - 0s 349ms/step - loss: 0.9679 - accuracy: 0.9067\n","Epoch 63/87\n","1/1 [==============================] - 0s 201ms/step - loss: 0.9857 - accuracy: 0.9067\n","Epoch 64/87\n","1/1 [==============================] - 0s 244ms/step - loss: 0.9807 - accuracy: 0.9067\n","Epoch 65/87\n","1/1 [==============================] - 0s 258ms/step - loss: 1.0031 - accuracy: 0.9067\n","Epoch 66/87\n","1/1 [==============================] - 0s 254ms/step - loss: 1.0238 - accuracy: 0.9067\n","Epoch 67/87\n","1/1 [==============================] - 0s 259ms/step - loss: 1.0253 - accuracy: 0.9067\n","Epoch 68/87\n","1/1 [==============================] - 0s 241ms/step - loss: 1.0268 - accuracy: 0.9067\n","Epoch 69/87\n","1/1 [==============================] - 0s 276ms/step - loss: 1.0221 - accuracy: 0.9067\n","Epoch 70/87\n","1/1 [==============================] - 0s 302ms/step - loss: 1.0194 - accuracy: 0.9067\n","Epoch 71/87\n","1/1 [==============================] - 0s 269ms/step - loss: 1.0183 - accuracy: 0.9067\n","Epoch 72/87\n","1/1 [==============================] - 0s 285ms/step - loss: 1.0183 - accuracy: 0.9067\n","Epoch 73/87\n","1/1 [==============================] - 0s 285ms/step - loss: 1.0186 - accuracy: 0.9067\n","Epoch 74/87\n","1/1 [==============================] - 0s 357ms/step - loss: 0.9986 - accuracy: 0.9067\n","Epoch 75/87\n","1/1 [==============================] - 0s 278ms/step - loss: 0.9707 - accuracy: 0.9067\n","Epoch 76/87\n","1/1 [==============================] - 0s 318ms/step - loss: 0.9690 - accuracy: 0.9067\n","Epoch 77/87\n","1/1 [==============================] - 0s 366ms/step - loss: 0.9686 - accuracy: 0.9067\n","Epoch 78/87\n","1/1 [==============================] - 0s 395ms/step - loss: 0.9686 - accuracy: 0.9067\n","Epoch 79/87\n","1/1 [==============================] - 0s 350ms/step - loss: 0.9687 - accuracy: 0.9067\n","Epoch 80/87\n","1/1 [==============================] - 0s 361ms/step - loss: 0.9686 - accuracy: 0.9067\n","Epoch 81/87\n","1/1 [==============================] - 0s 394ms/step - loss: 0.9684 - accuracy: 0.9067\n","Epoch 82/87\n","1/1 [==============================] - 0s 386ms/step - loss: 0.9680 - accuracy: 0.9067\n","Epoch 83/87\n","1/1 [==============================] - 0s 418ms/step - loss: 0.9674 - accuracy: 0.9067\n","Epoch 84/87\n","1/1 [==============================] - 0s 369ms/step - loss: 0.9668 - accuracy: 0.9067\n","Epoch 85/87\n","1/1 [==============================] - 0s 310ms/step - loss: 0.9661 - accuracy: 0.9067\n","Epoch 86/87\n","1/1 [==============================] - 0s 293ms/step - loss: 0.9653 - accuracy: 0.9067\n","Epoch 87/87\n","1/1 [==============================] - 0s 318ms/step - loss: 0.9642 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b44d5e700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1364 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:58:11,723]\u001b[0m Trial 17 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 217, 'num_epochs': 87}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/58\n","1/1 [==============================] - 5s 5s/step - loss: 6.9402 - accuracy: 0.0060\n","Epoch 2/58\n","1/1 [==============================] - 0s 152ms/step - loss: 1.5726 - accuracy: 0.9067\n","Epoch 3/58\n","1/1 [==============================] - 0s 173ms/step - loss: 1.4452 - accuracy: 0.9067\n","Epoch 4/58\n","1/1 [==============================] - 0s 210ms/step - loss: 1.3488 - accuracy: 0.9067\n","Epoch 5/58\n","1/1 [==============================] - 0s 205ms/step - loss: 1.2510 - accuracy: 0.9067\n","Epoch 6/58\n","1/1 [==============================] - 0s 220ms/step - loss: 1.1754 - accuracy: 0.9067\n","Epoch 7/58\n","1/1 [==============================] - 0s 179ms/step - loss: 1.0943 - accuracy: 0.9067\n","Epoch 8/58\n","1/1 [==============================] - 0s 206ms/step - loss: 1.0580 - accuracy: 0.9067\n","Epoch 9/58\n","1/1 [==============================] - 0s 192ms/step - loss: 1.0237 - accuracy: 0.9067\n","Epoch 10/58\n","1/1 [==============================] - 0s 213ms/step - loss: 1.0484 - accuracy: 0.9067\n","Epoch 11/58\n","1/1 [==============================] - 0s 193ms/step - loss: 1.0390 - accuracy: 0.9067\n","Epoch 12/58\n","1/1 [==============================] - 0s 161ms/step - loss: 1.0514 - accuracy: 0.9067\n","Epoch 13/58\n","1/1 [==============================] - 0s 172ms/step - loss: 1.0207 - accuracy: 0.9067\n","Epoch 14/58\n","1/1 [==============================] - 0s 172ms/step - loss: 1.0225 - accuracy: 0.9067\n","Epoch 15/58\n","1/1 [==============================] - 0s 190ms/step - loss: 1.0220 - accuracy: 0.9067\n","Epoch 16/58\n","1/1 [==============================] - 0s 151ms/step - loss: 1.0178 - accuracy: 0.9067\n","Epoch 17/58\n","1/1 [==============================] - 0s 183ms/step - loss: 1.0103 - accuracy: 0.9067\n","Epoch 18/58\n","1/1 [==============================] - 0s 178ms/step - loss: 1.0031 - accuracy: 0.9067\n","Epoch 19/58\n","1/1 [==============================] - 0s 175ms/step - loss: 1.0299 - accuracy: 0.9067\n","Epoch 20/58\n","1/1 [==============================] - 0s 183ms/step - loss: 1.0145 - accuracy: 0.9067\n","Epoch 21/58\n","1/1 [==============================] - 0s 183ms/step - loss: 0.9974 - accuracy: 0.9067\n","Epoch 22/58\n","1/1 [==============================] - 0s 188ms/step - loss: 0.9579 - accuracy: 0.9067\n","Epoch 23/58\n","1/1 [==============================] - 0s 187ms/step - loss: 0.9378 - accuracy: 0.9067\n","Epoch 24/58\n","1/1 [==============================] - 0s 182ms/step - loss: 0.8987 - accuracy: 0.9067\n","Epoch 25/58\n","1/1 [==============================] - 0s 266ms/step - loss: 0.8812 - accuracy: 0.9067\n","Epoch 26/58\n","1/1 [==============================] - 0s 185ms/step - loss: 0.8668 - accuracy: 0.9067\n","Epoch 27/58\n","1/1 [==============================] - 0s 159ms/step - loss: 0.8549 - accuracy: 0.9067\n","Epoch 28/58\n","1/1 [==============================] - 0s 152ms/step - loss: 0.8455 - accuracy: 0.9067\n","Epoch 29/58\n","1/1 [==============================] - 0s 151ms/step - loss: 0.8578 - accuracy: 0.9067\n","Epoch 30/58\n","1/1 [==============================] - 0s 145ms/step - loss: 0.8514 - accuracy: 0.9067\n","Epoch 31/58\n","1/1 [==============================] - 0s 160ms/step - loss: 0.8140 - accuracy: 0.9067\n","Epoch 32/58\n","1/1 [==============================] - 0s 143ms/step - loss: 0.7999 - accuracy: 0.9067\n","Epoch 33/58\n","1/1 [==============================] - 0s 175ms/step - loss: 0.7912 - accuracy: 0.9067\n","Epoch 34/58\n","1/1 [==============================] - 0s 172ms/step - loss: 0.7856 - accuracy: 0.9067\n","Epoch 35/58\n","1/1 [==============================] - 0s 166ms/step - loss: 0.7828 - accuracy: 0.9067\n","Epoch 36/58\n","1/1 [==============================] - 0s 160ms/step - loss: 0.7827 - accuracy: 0.9067\n","Epoch 37/58\n","1/1 [==============================] - 0s 176ms/step - loss: 0.7826 - accuracy: 0.9067\n","Epoch 38/58\n","1/1 [==============================] - 0s 154ms/step - loss: 0.7761 - accuracy: 0.9067\n","Epoch 39/58\n","1/1 [==============================] - 0s 167ms/step - loss: 0.7706 - accuracy: 0.9067\n","Epoch 40/58\n","1/1 [==============================] - 0s 141ms/step - loss: 0.7662 - accuracy: 0.9067\n","Epoch 41/58\n","1/1 [==============================] - 0s 159ms/step - loss: 0.7628 - accuracy: 0.9067\n","Epoch 42/58\n","1/1 [==============================] - 0s 139ms/step - loss: 0.7600 - accuracy: 0.9067\n","Epoch 43/58\n","1/1 [==============================] - 0s 235ms/step - loss: 0.7576 - accuracy: 0.9067\n","Epoch 44/58\n","1/1 [==============================] - 0s 232ms/step - loss: 0.7557 - accuracy: 0.9067\n","Epoch 45/58\n","1/1 [==============================] - 0s 173ms/step - loss: 0.7543 - accuracy: 0.9067\n","Epoch 46/58\n","1/1 [==============================] - 0s 202ms/step - loss: 0.7531 - accuracy: 0.9067\n","Epoch 47/58\n","1/1 [==============================] - 0s 230ms/step - loss: 0.7519 - accuracy: 0.9067\n","Epoch 48/58\n","1/1 [==============================] - 0s 186ms/step - loss: 0.7509 - accuracy: 0.9067\n","Epoch 49/58\n","1/1 [==============================] - 0s 148ms/step - loss: 0.7499 - accuracy: 0.9067\n","Epoch 50/58\n","1/1 [==============================] - 0s 163ms/step - loss: 0.7491 - accuracy: 0.9067\n","Epoch 51/58\n","1/1 [==============================] - 0s 171ms/step - loss: 0.7483 - accuracy: 0.9067\n","Epoch 52/58\n","1/1 [==============================] - 0s 155ms/step - loss: 0.7476 - accuracy: 0.9067\n","Epoch 53/58\n","1/1 [==============================] - 0s 143ms/step - loss: 0.7469 - accuracy: 0.9067\n","Epoch 54/58\n","1/1 [==============================] - 0s 216ms/step - loss: 0.7462 - accuracy: 0.9067\n","Epoch 55/58\n","1/1 [==============================] - 0s 234ms/step - loss: 0.7452 - accuracy: 0.9067\n","Epoch 56/58\n","1/1 [==============================] - 0s 236ms/step - loss: 0.7441 - accuracy: 0.9067\n","Epoch 57/58\n","1/1 [==============================] - 0s 231ms/step - loss: 0.7428 - accuracy: 0.9067\n","Epoch 58/58\n","1/1 [==============================] - 0s 233ms/step - loss: 0.7415 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b242f7430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1250 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:58:29,111]\u001b[0m Trial 18 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 147, 'num_epochs': 58}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/133\n","1/1 [==============================] - 6s 6s/step - loss: 3.2596 - accuracy: 0.0218\n","Epoch 2/133\n","1/1 [==============================] - 0s 273ms/step - loss: 1.2623 - accuracy: 0.9067\n","Epoch 3/133\n","1/1 [==============================] - 0s 285ms/step - loss: 1.0473 - accuracy: 0.9067\n","Epoch 4/133\n","1/1 [==============================] - 0s 134ms/step - loss: 1.0909 - accuracy: 0.9067\n","Epoch 5/133\n","1/1 [==============================] - 0s 160ms/step - loss: 1.1192 - accuracy: 0.9067\n","Epoch 6/133\n","1/1 [==============================] - 0s 194ms/step - loss: 1.1263 - accuracy: 0.9067\n","Epoch 7/133\n","1/1 [==============================] - 0s 198ms/step - loss: 1.1154 - accuracy: 0.9067\n","Epoch 8/133\n","1/1 [==============================] - 0s 241ms/step - loss: 1.0947 - accuracy: 0.9067\n","Epoch 9/133\n","1/1 [==============================] - 0s 168ms/step - loss: 1.0619 - accuracy: 0.9067\n","Epoch 10/133\n","1/1 [==============================] - 0s 287ms/step - loss: 1.0233 - accuracy: 0.9067\n","Epoch 11/133\n","1/1 [==============================] - 0s 170ms/step - loss: 0.9834 - accuracy: 0.9067\n","Epoch 12/133\n","1/1 [==============================] - 0s 249ms/step - loss: 0.9422 - accuracy: 0.9067\n","Epoch 13/133\n","1/1 [==============================] - 0s 212ms/step - loss: 0.8988 - accuracy: 0.9067\n","Epoch 14/133\n","1/1 [==============================] - 0s 251ms/step - loss: 0.8540 - accuracy: 0.9067\n","Epoch 15/133\n","1/1 [==============================] - 0s 231ms/step - loss: 0.8094 - accuracy: 0.9067\n","Epoch 16/133\n","1/1 [==============================] - 0s 200ms/step - loss: 0.7677 - accuracy: 0.9067\n","Epoch 17/133\n","1/1 [==============================] - 0s 218ms/step - loss: 0.7320 - accuracy: 0.9067\n","Epoch 18/133\n","1/1 [==============================] - 0s 182ms/step - loss: 0.7054 - accuracy: 0.9067\n","Epoch 19/133\n","1/1 [==============================] - 0s 210ms/step - loss: 0.6954 - accuracy: 0.9067\n","Epoch 20/133\n","1/1 [==============================] - 0s 174ms/step - loss: 0.6779 - accuracy: 0.9067\n","Epoch 21/133\n","1/1 [==============================] - 0s 173ms/step - loss: 0.6840 - accuracy: 0.9067\n","Epoch 22/133\n","1/1 [==============================] - 0s 216ms/step - loss: 0.7283 - accuracy: 0.9067\n","Epoch 23/133\n","1/1 [==============================] - 0s 176ms/step - loss: 0.7302 - accuracy: 0.9067\n","Epoch 24/133\n","1/1 [==============================] - 0s 173ms/step - loss: 0.7087 - accuracy: 0.9067\n","Epoch 25/133\n","1/1 [==============================] - 0s 175ms/step - loss: 0.6795 - accuracy: 0.9067\n","Epoch 26/133\n","1/1 [==============================] - 0s 172ms/step - loss: 0.6739 - accuracy: 0.9067\n","Epoch 27/133\n","1/1 [==============================] - 0s 156ms/step - loss: 0.6689 - accuracy: 0.9067\n","Epoch 28/133\n","1/1 [==============================] - 0s 163ms/step - loss: 0.6643 - accuracy: 0.9067\n","Epoch 29/133\n","1/1 [==============================] - 0s 162ms/step - loss: 0.6610 - accuracy: 0.9067\n","Epoch 30/133\n","1/1 [==============================] - 0s 194ms/step - loss: 0.6593 - accuracy: 0.9067\n","Epoch 31/133\n","1/1 [==============================] - 0s 167ms/step - loss: 0.6591 - accuracy: 0.9067\n","Epoch 32/133\n","1/1 [==============================] - 0s 140ms/step - loss: 0.6590 - accuracy: 0.9067\n","Epoch 33/133\n","1/1 [==============================] - 0s 128ms/step - loss: 0.6578 - accuracy: 0.9067\n","Epoch 34/133\n","1/1 [==============================] - 0s 137ms/step - loss: 0.6556 - accuracy: 0.9067\n","Epoch 35/133\n","1/1 [==============================] - 0s 193ms/step - loss: 0.6527 - accuracy: 0.9067\n","Epoch 36/133\n","1/1 [==============================] - 0s 161ms/step - loss: 0.6497 - accuracy: 0.9067\n","Epoch 37/133\n","1/1 [==============================] - 0s 155ms/step - loss: 0.6469 - accuracy: 0.9067\n","Epoch 38/133\n","1/1 [==============================] - 0s 142ms/step - loss: 0.6443 - accuracy: 0.9067\n","Epoch 39/133\n","1/1 [==============================] - 0s 142ms/step - loss: 0.6418 - accuracy: 0.9067\n","Epoch 40/133\n","1/1 [==============================] - 0s 177ms/step - loss: 0.6394 - accuracy: 0.9067\n","Epoch 41/133\n","1/1 [==============================] - 0s 175ms/step - loss: 0.6371 - accuracy: 0.9067\n","Epoch 42/133\n","1/1 [==============================] - 0s 176ms/step - loss: 0.6349 - accuracy: 0.9067\n","Epoch 43/133\n","1/1 [==============================] - 0s 171ms/step - loss: 0.6328 - accuracy: 0.9067\n","Epoch 44/133\n","1/1 [==============================] - 0s 168ms/step - loss: 0.6308 - accuracy: 0.9067\n","Epoch 45/133\n","1/1 [==============================] - 0s 173ms/step - loss: 0.6290 - accuracy: 0.9067\n","Epoch 46/133\n","1/1 [==============================] - 0s 171ms/step - loss: 0.6271 - accuracy: 0.9067\n","Epoch 47/133\n","1/1 [==============================] - 0s 162ms/step - loss: 0.6251 - accuracy: 0.9067\n","Epoch 48/133\n","1/1 [==============================] - 0s 167ms/step - loss: 0.6227 - accuracy: 0.9067\n","Epoch 49/133\n","1/1 [==============================] - 0s 166ms/step - loss: 0.6201 - accuracy: 0.9067\n","Epoch 50/133\n","1/1 [==============================] - 0s 179ms/step - loss: 0.6174 - accuracy: 0.9067\n","Epoch 51/133\n","1/1 [==============================] - 0s 177ms/step - loss: 0.6146 - accuracy: 0.9067\n","Epoch 52/133\n","1/1 [==============================] - 0s 168ms/step - loss: 0.6117 - accuracy: 0.9127\n","Epoch 53/133\n","1/1 [==============================] - 0s 186ms/step - loss: 0.6085 - accuracy: 0.9127\n","Epoch 54/133\n","1/1 [==============================] - 0s 194ms/step - loss: 0.6053 - accuracy: 0.9187\n","Epoch 55/133\n","1/1 [==============================] - 0s 195ms/step - loss: 0.6020 - accuracy: 0.9147\n","Epoch 56/133\n","1/1 [==============================] - 0s 202ms/step - loss: 0.5986 - accuracy: 0.9167\n","Epoch 57/133\n","1/1 [==============================] - 0s 187ms/step - loss: 0.5955 - accuracy: 0.9187\n","Epoch 58/133\n","1/1 [==============================] - 0s 194ms/step - loss: 0.5940 - accuracy: 0.9187\n","Epoch 59/133\n","1/1 [==============================] - 0s 170ms/step - loss: 0.5960 - accuracy: 0.9187\n","Epoch 60/133\n","1/1 [==============================] - 0s 214ms/step - loss: 0.5918 - accuracy: 0.9187\n","Epoch 61/133\n","1/1 [==============================] - 0s 211ms/step - loss: 0.5937 - accuracy: 0.9167\n","Epoch 62/133\n","1/1 [==============================] - 0s 230ms/step - loss: 0.5952 - accuracy: 0.9167\n","Epoch 63/133\n","1/1 [==============================] - 0s 180ms/step - loss: 0.5958 - accuracy: 0.9187\n","Epoch 64/133\n","1/1 [==============================] - 0s 183ms/step - loss: 0.5954 - accuracy: 0.9206\n","Epoch 65/133\n","1/1 [==============================] - 0s 183ms/step - loss: 0.5939 - accuracy: 0.9226\n","Epoch 66/133\n","1/1 [==============================] - 0s 181ms/step - loss: 0.5912 - accuracy: 0.9226\n","Epoch 67/133\n","1/1 [==============================] - 0s 179ms/step - loss: 0.5874 - accuracy: 0.9206\n","Epoch 68/133\n","1/1 [==============================] - 0s 207ms/step - loss: 0.5833 - accuracy: 0.9246\n","Epoch 69/133\n","1/1 [==============================] - 0s 180ms/step - loss: 0.5808 - accuracy: 0.9246\n","Epoch 70/133\n","1/1 [==============================] - 0s 203ms/step - loss: 0.5827 - accuracy: 0.9246\n","Epoch 71/133\n","1/1 [==============================] - 0s 183ms/step - loss: 0.5794 - accuracy: 0.9206\n","Epoch 72/133\n","1/1 [==============================] - 0s 183ms/step - loss: 0.5746 - accuracy: 0.9246\n","Epoch 73/133\n","1/1 [==============================] - 0s 181ms/step - loss: 0.5738 - accuracy: 0.9246\n","Epoch 74/133\n","1/1 [==============================] - 0s 175ms/step - loss: 0.5744 - accuracy: 0.9266\n","Epoch 75/133\n","1/1 [==============================] - 0s 159ms/step - loss: 0.5712 - accuracy: 0.9266\n","Epoch 76/133\n","1/1 [==============================] - 0s 207ms/step - loss: 0.5676 - accuracy: 0.9286\n","Epoch 77/133\n","1/1 [==============================] - 0s 235ms/step - loss: 0.5635 - accuracy: 0.9306\n","Epoch 78/133\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9266\n","Epoch 79/133\n","1/1 [==============================] - 0s 223ms/step - loss: nan - accuracy: 0.9067\n","Epoch 80/133\n","1/1 [==============================] - 0s 220ms/step - loss: nan - accuracy: 0.9067\n","Epoch 81/133\n","1/1 [==============================] - 0s 190ms/step - loss: nan - accuracy: 0.9067\n","Epoch 82/133\n","1/1 [==============================] - 0s 201ms/step - loss: nan - accuracy: 0.9067\n","Epoch 83/133\n","1/1 [==============================] - 0s 210ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/133\n","1/1 [==============================] - 0s 237ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/133\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/133\n","1/1 [==============================] - 0s 221ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/133\n","1/1 [==============================] - 0s 192ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/133\n","1/1 [==============================] - 0s 214ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/133\n","1/1 [==============================] - 0s 209ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/133\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/133\n","1/1 [==============================] - 0s 235ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/133\n","1/1 [==============================] - 0s 230ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/133\n","1/1 [==============================] - 0s 228ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/133\n","1/1 [==============================] - 0s 233ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/133\n","1/1 [==============================] - 0s 227ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/133\n","1/1 [==============================] - 0s 204ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/133\n","1/1 [==============================] - 0s 253ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/133\n","1/1 [==============================] - 0s 297ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/133\n","1/1 [==============================] - 0s 194ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/133\n","1/1 [==============================] - 0s 220ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/133\n","1/1 [==============================] - 0s 185ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/133\n","1/1 [==============================] - 0s 222ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/133\n","1/1 [==============================] - 0s 198ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/133\n","1/1 [==============================] - 0s 202ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/133\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/133\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/133\n","1/1 [==============================] - 0s 184ms/step - loss: nan - accuracy: 0.9067\n","Epoch 108/133\n","1/1 [==============================] - 0s 178ms/step - loss: nan - accuracy: 0.9067\n","Epoch 109/133\n","1/1 [==============================] - 0s 187ms/step - loss: nan - accuracy: 0.9067\n","Epoch 110/133\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 111/133\n","1/1 [==============================] - 0s 177ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/133\n","1/1 [==============================] - 0s 173ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/133\n","1/1 [==============================] - 0s 183ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/133\n","1/1 [==============================] - 0s 177ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/133\n","1/1 [==============================] - 0s 169ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/133\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","Epoch 117/133\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 118/133\n","1/1 [==============================] - 0s 180ms/step - loss: nan - accuracy: 0.9067\n","Epoch 119/133\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 120/133\n","1/1 [==============================] - 0s 178ms/step - loss: nan - accuracy: 0.9067\n","Epoch 121/133\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 122/133\n","1/1 [==============================] - 0s 179ms/step - loss: nan - accuracy: 0.9067\n","Epoch 123/133\n","1/1 [==============================] - 0s 167ms/step - loss: nan - accuracy: 0.9067\n","Epoch 124/133\n","1/1 [==============================] - 0s 195ms/step - loss: nan - accuracy: 0.9067\n","Epoch 125/133\n","1/1 [==============================] - 0s 172ms/step - loss: nan - accuracy: 0.9067\n","Epoch 126/133\n","1/1 [==============================] - 0s 177ms/step - loss: nan - accuracy: 0.9067\n","Epoch 127/133\n","1/1 [==============================] - 0s 177ms/step - loss: nan - accuracy: 0.9067\n","Epoch 128/133\n","1/1 [==============================] - 0s 195ms/step - loss: nan - accuracy: 0.9067\n","Epoch 129/133\n","1/1 [==============================] - 0s 191ms/step - loss: nan - accuracy: 0.9067\n","Epoch 130/133\n","1/1 [==============================] - 0s 184ms/step - loss: nan - accuracy: 0.9067\n","Epoch 131/133\n","1/1 [==============================] - 0s 171ms/step - loss: nan - accuracy: 0.9067\n","Epoch 132/133\n","1/1 [==============================] - 0s 187ms/step - loss: nan - accuracy: 0.9067\n","Epoch 133/133\n","1/1 [==============================] - 0s 170ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b12efa040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:59:02,610]\u001b[0m Trial 19 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 175, 'num_epochs': 133}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/94\n","1/1 [==============================] - 5s 5s/step - loss: 11.7255 - accuracy: 0.0099\n","Epoch 2/94\n","1/1 [==============================] - 0s 138ms/step - loss: 1.5586 - accuracy: 0.9067\n","Epoch 3/94\n","1/1 [==============================] - 0s 175ms/step - loss: 1.2651 - accuracy: 0.9067\n","Epoch 4/94\n","1/1 [==============================] - 0s 173ms/step - loss: 1.1424 - accuracy: 0.9067\n","Epoch 5/94\n","1/1 [==============================] - 0s 144ms/step - loss: 1.0435 - accuracy: 0.9067\n","Epoch 6/94\n","1/1 [==============================] - 0s 164ms/step - loss: 1.0096 - accuracy: 0.9067\n","Epoch 7/94\n","1/1 [==============================] - 0s 148ms/step - loss: 0.9698 - accuracy: 0.9067\n","Epoch 8/94\n","1/1 [==============================] - 0s 147ms/step - loss: 0.9598 - accuracy: 0.9067\n","Epoch 9/94\n","1/1 [==============================] - 0s 139ms/step - loss: 0.9459 - accuracy: 0.9067\n","Epoch 10/94\n","1/1 [==============================] - 0s 138ms/step - loss: 0.9301 - accuracy: 0.9067\n","Epoch 11/94\n","1/1 [==============================] - 0s 167ms/step - loss: 0.9148 - accuracy: 0.9067\n","Epoch 12/94\n","1/1 [==============================] - 0s 119ms/step - loss: 0.9022 - accuracy: 0.9067\n","Epoch 13/94\n","1/1 [==============================] - 0s 133ms/step - loss: 0.8942 - accuracy: 0.9067\n","Epoch 14/94\n","1/1 [==============================] - 0s 152ms/step - loss: 0.9146 - accuracy: 0.9067\n","Epoch 15/94\n","1/1 [==============================] - 0s 154ms/step - loss: 0.9342 - accuracy: 0.9067\n","Epoch 16/94\n","1/1 [==============================] - 0s 181ms/step - loss: 0.9327 - accuracy: 0.9067\n","Epoch 17/94\n","1/1 [==============================] - 0s 145ms/step - loss: 0.9328 - accuracy: 0.9067\n","Epoch 18/94\n","1/1 [==============================] - 0s 147ms/step - loss: 0.9133 - accuracy: 0.9067\n","Epoch 19/94\n","1/1 [==============================] - 0s 123ms/step - loss: 0.8908 - accuracy: 0.9067\n","Epoch 20/94\n","1/1 [==============================] - 0s 140ms/step - loss: 0.8849 - accuracy: 0.9067\n","Epoch 21/94\n","1/1 [==============================] - 0s 115ms/step - loss: 0.8894 - accuracy: 0.9067\n","Epoch 22/94\n","1/1 [==============================] - 0s 160ms/step - loss: 0.8935 - accuracy: 0.9067\n","Epoch 23/94\n","1/1 [==============================] - 0s 208ms/step - loss: 0.8939 - accuracy: 0.9067\n","Epoch 24/94\n","1/1 [==============================] - 0s 227ms/step - loss: 0.8908 - accuracy: 0.9067\n","Epoch 25/94\n","1/1 [==============================] - 0s 166ms/step - loss: 0.8861 - accuracy: 0.9067\n","Epoch 26/94\n","1/1 [==============================] - 0s 165ms/step - loss: 0.8816 - accuracy: 0.9067\n","Epoch 27/94\n","1/1 [==============================] - 0s 152ms/step - loss: 0.8777 - accuracy: 0.9067\n","Epoch 28/94\n","1/1 [==============================] - 0s 142ms/step - loss: 0.8746 - accuracy: 0.9067\n","Epoch 29/94\n","1/1 [==============================] - 0s 137ms/step - loss: 0.8729 - accuracy: 0.9067\n","Epoch 30/94\n","1/1 [==============================] - 0s 126ms/step - loss: 0.8730 - accuracy: 0.9067\n","Epoch 31/94\n","1/1 [==============================] - 0s 122ms/step - loss: 0.8748 - accuracy: 0.9067\n","Epoch 32/94\n","1/1 [==============================] - 0s 112ms/step - loss: 0.8714 - accuracy: 0.9067\n","Epoch 33/94\n","1/1 [==============================] - 0s 122ms/step - loss: 0.8692 - accuracy: 0.9067\n","Epoch 34/94\n","1/1 [==============================] - 0s 122ms/step - loss: 0.8676 - accuracy: 0.9067\n","Epoch 35/94\n","1/1 [==============================] - 0s 121ms/step - loss: 0.8663 - accuracy: 0.9067\n","Epoch 36/94\n","1/1 [==============================] - 0s 160ms/step - loss: 0.8654 - accuracy: 0.9067\n","Epoch 37/94\n","1/1 [==============================] - 0s 153ms/step - loss: 0.8649 - accuracy: 0.9067\n","Epoch 38/94\n","1/1 [==============================] - 0s 144ms/step - loss: 0.8647 - accuracy: 0.9067\n","Epoch 39/94\n","1/1 [==============================] - 0s 148ms/step - loss: 0.8642 - accuracy: 0.9067\n","Epoch 40/94\n","1/1 [==============================] - 0s 147ms/step - loss: 0.8632 - accuracy: 0.9067\n","Epoch 41/94\n","1/1 [==============================] - 0s 144ms/step - loss: 0.8619 - accuracy: 0.9067\n","Epoch 42/94\n","1/1 [==============================] - 0s 214ms/step - loss: 0.8607 - accuracy: 0.9067\n","Epoch 43/94\n","1/1 [==============================] - 0s 178ms/step - loss: 0.8597 - accuracy: 0.9067\n","Epoch 44/94\n","1/1 [==============================] - 0s 187ms/step - loss: 0.8591 - accuracy: 0.9067\n","Epoch 45/94\n","1/1 [==============================] - 0s 248ms/step - loss: 0.8584 - accuracy: 0.9067\n","Epoch 46/94\n","1/1 [==============================] - 0s 157ms/step - loss: 0.8576 - accuracy: 0.9067\n","Epoch 47/94\n","1/1 [==============================] - 0s 151ms/step - loss: 0.8569 - accuracy: 0.9067\n","Epoch 48/94\n","1/1 [==============================] - 0s 124ms/step - loss: 0.8561 - accuracy: 0.9067\n","Epoch 49/94\n","1/1 [==============================] - 0s 159ms/step - loss: 0.8555 - accuracy: 0.9067\n","Epoch 50/94\n","1/1 [==============================] - 0s 170ms/step - loss: 0.8548 - accuracy: 0.9067\n","Epoch 51/94\n","1/1 [==============================] - 0s 174ms/step - loss: 0.8542 - accuracy: 0.9067\n","Epoch 52/94\n","1/1 [==============================] - 0s 144ms/step - loss: 0.8534 - accuracy: 0.9067\n","Epoch 53/94\n","1/1 [==============================] - 0s 141ms/step - loss: 0.8525 - accuracy: 0.9067\n","Epoch 54/94\n","1/1 [==============================] - 0s 154ms/step - loss: 0.8517 - accuracy: 0.9067\n","Epoch 55/94\n","1/1 [==============================] - 0s 171ms/step - loss: 0.8509 - accuracy: 0.9067\n","Epoch 56/94\n","1/1 [==============================] - 0s 173ms/step - loss: 0.8501 - accuracy: 0.9067\n","Epoch 57/94\n","1/1 [==============================] - 0s 197ms/step - loss: 0.8493 - accuracy: 0.9067\n","Epoch 58/94\n","1/1 [==============================] - 0s 150ms/step - loss: 0.8483 - accuracy: 0.9087\n","Epoch 59/94\n","1/1 [==============================] - 0s 162ms/step - loss: 0.8474 - accuracy: 0.9087\n","Epoch 60/94\n","1/1 [==============================] - 0s 151ms/step - loss: 0.8466 - accuracy: 0.9087\n","Epoch 61/94\n","1/1 [==============================] - 0s 180ms/step - loss: 0.8458 - accuracy: 0.9087\n","Epoch 62/94\n","1/1 [==============================] - 0s 158ms/step - loss: 0.8449 - accuracy: 0.9087\n","Epoch 63/94\n","1/1 [==============================] - 0s 156ms/step - loss: 0.8440 - accuracy: 0.9087\n","Epoch 64/94\n","1/1 [==============================] - 0s 138ms/step - loss: 0.8431 - accuracy: 0.9087\n","Epoch 65/94\n","1/1 [==============================] - 0s 131ms/step - loss: 0.8422 - accuracy: 0.9087\n","Epoch 66/94\n","1/1 [==============================] - 0s 128ms/step - loss: 0.8413 - accuracy: 0.9087\n","Epoch 67/94\n","1/1 [==============================] - 0s 132ms/step - loss: 0.8404 - accuracy: 0.9107\n","Epoch 68/94\n","1/1 [==============================] - 0s 141ms/step - loss: 0.8394 - accuracy: 0.9127\n","Epoch 69/94\n","1/1 [==============================] - 0s 141ms/step - loss: 0.8384 - accuracy: 0.9127\n","Epoch 70/94\n","1/1 [==============================] - 0s 123ms/step - loss: 0.8373 - accuracy: 0.9127\n","Epoch 71/94\n","1/1 [==============================] - 0s 135ms/step - loss: 0.8362 - accuracy: 0.9127\n","Epoch 72/94\n","1/1 [==============================] - 0s 138ms/step - loss: 0.8350 - accuracy: 0.9127\n","Epoch 73/94\n","1/1 [==============================] - 0s 128ms/step - loss: 0.8338 - accuracy: 0.9107\n","Epoch 74/94\n","1/1 [==============================] - 0s 123ms/step - loss: 0.8327 - accuracy: 0.9107\n","Epoch 75/94\n","1/1 [==============================] - 0s 128ms/step - loss: 0.8315 - accuracy: 0.9107\n","Epoch 76/94\n","1/1 [==============================] - 0s 142ms/step - loss: 0.8302 - accuracy: 0.9107\n","Epoch 77/94\n","1/1 [==============================] - 0s 131ms/step - loss: 0.8290 - accuracy: 0.9107\n","Epoch 78/94\n","1/1 [==============================] - 0s 123ms/step - loss: 0.8274 - accuracy: 0.9107\n","Epoch 79/94\n","1/1 [==============================] - 0s 135ms/step - loss: 0.8257 - accuracy: 0.9107\n","Epoch 80/94\n","1/1 [==============================] - 0s 131ms/step - loss: 0.8237 - accuracy: 0.9107\n","Epoch 81/94\n","1/1 [==============================] - 0s 127ms/step - loss: 0.8215 - accuracy: 0.9127\n","Epoch 82/94\n","1/1 [==============================] - 0s 134ms/step - loss: 0.8197 - accuracy: 0.9147\n","Epoch 83/94\n","1/1 [==============================] - 0s 168ms/step - loss: 0.8501 - accuracy: 0.9127\n","Epoch 84/94\n","1/1 [==============================] - 0s 236ms/step - loss: 0.8488 - accuracy: 0.9127\n","Epoch 85/94\n","1/1 [==============================] - 0s 160ms/step - loss: 0.8463 - accuracy: 0.9167\n","Epoch 86/94\n","1/1 [==============================] - 0s 144ms/step - loss: nan - accuracy: 0.9167\n","Epoch 87/94\n","1/1 [==============================] - 0s 150ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/94\n","1/1 [==============================] - 0s 154ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/94\n","1/1 [==============================] - 0s 148ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/94\n","1/1 [==============================] - 0s 119ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/94\n","1/1 [==============================] - 0s 122ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/94\n","1/1 [==============================] - 0s 154ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/94\n","1/1 [==============================] - 0s 172ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/94\n","1/1 [==============================] - 0s 156ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b2439c9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:59:24,016]\u001b[0m Trial 20 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 118, 'num_epochs': 94}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/65\n","1/1 [==============================] - 5s 5s/step - loss: 14.5046 - accuracy: 0.0079\n","Epoch 2/65\n","1/1 [==============================] - 0s 140ms/step - loss: 2.6580 - accuracy: 0.1131\n","Epoch 3/65\n","1/1 [==============================] - 0s 170ms/step - loss: 1.2846 - accuracy: 0.9067\n","Epoch 4/65\n","1/1 [==============================] - 0s 203ms/step - loss: 1.0138 - accuracy: 0.9067\n","Epoch 5/65\n","1/1 [==============================] - 0s 159ms/step - loss: 0.8209 - accuracy: 0.9067\n","Epoch 6/65\n","1/1 [==============================] - 0s 190ms/step - loss: 0.6905 - accuracy: 0.9067\n","Epoch 7/65\n","1/1 [==============================] - 0s 183ms/step - loss: 0.5715 - accuracy: 0.9067\n","Epoch 8/65\n","1/1 [==============================] - 0s 169ms/step - loss: 0.5637 - accuracy: 0.9067\n","Epoch 9/65\n","1/1 [==============================] - 0s 165ms/step - loss: 0.5888 - accuracy: 0.9067\n","Epoch 10/65\n","1/1 [==============================] - 0s 177ms/step - loss: 0.6649 - accuracy: 0.9067\n","Epoch 11/65\n","1/1 [==============================] - 0s 166ms/step - loss: 0.6684 - accuracy: 0.9067\n","Epoch 12/65\n","1/1 [==============================] - 0s 166ms/step - loss: 0.5814 - accuracy: 0.9067\n","Epoch 13/65\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5763 - accuracy: 0.9067\n","Epoch 14/65\n","1/1 [==============================] - 0s 135ms/step - loss: 0.5635 - accuracy: 0.9067\n","Epoch 15/65\n","1/1 [==============================] - 0s 136ms/step - loss: 0.5417 - accuracy: 0.9067\n","Epoch 16/65\n","1/1 [==============================] - 0s 129ms/step - loss: 0.5491 - accuracy: 0.9067\n","Epoch 17/65\n","1/1 [==============================] - 0s 162ms/step - loss: 0.5551 - accuracy: 0.9067\n","Epoch 18/65\n","1/1 [==============================] - 0s 160ms/step - loss: 0.5557 - accuracy: 0.9067\n","Epoch 19/65\n","1/1 [==============================] - 0s 156ms/step - loss: 0.5485 - accuracy: 0.9067\n","Epoch 20/65\n","1/1 [==============================] - 0s 129ms/step - loss: 0.5357 - accuracy: 0.9067\n","Epoch 21/65\n","1/1 [==============================] - 0s 141ms/step - loss: 0.5209 - accuracy: 0.9067\n","Epoch 22/65\n","1/1 [==============================] - 0s 141ms/step - loss: 0.5080 - accuracy: 0.9067\n","Epoch 23/65\n","1/1 [==============================] - 0s 131ms/step - loss: 0.4989 - accuracy: 0.9067\n","Epoch 24/65\n","1/1 [==============================] - 0s 181ms/step - loss: 0.4944 - accuracy: 0.9067\n","Epoch 25/65\n","1/1 [==============================] - 0s 224ms/step - loss: 0.4941 - accuracy: 0.9067\n","Epoch 26/65\n","1/1 [==============================] - 0s 158ms/step - loss: 0.5179 - accuracy: 0.9067\n","Epoch 27/65\n","1/1 [==============================] - 0s 162ms/step - loss: 0.5185 - accuracy: 0.9067\n","Epoch 28/65\n","1/1 [==============================] - 0s 172ms/step - loss: 0.5119 - accuracy: 0.9067\n","Epoch 29/65\n","1/1 [==============================] - 0s 190ms/step - loss: 0.5097 - accuracy: 0.9067\n","Epoch 30/65\n","1/1 [==============================] - 0s 171ms/step - loss: 0.4915 - accuracy: 0.9067\n","Epoch 31/65\n","1/1 [==============================] - 0s 205ms/step - loss: 0.5520 - accuracy: 0.9067\n","Epoch 32/65\n","1/1 [==============================] - 0s 207ms/step - loss: 0.6191 - accuracy: 0.9067\n","Epoch 33/65\n","1/1 [==============================] - 0s 358ms/step - loss: 0.6541 - accuracy: 0.9067\n","Epoch 34/65\n","1/1 [==============================] - 0s 325ms/step - loss: 0.6609 - accuracy: 0.9067\n","Epoch 35/65\n","1/1 [==============================] - 0s 279ms/step - loss: 0.6442 - accuracy: 0.9067\n","Epoch 36/65\n","1/1 [==============================] - 0s 171ms/step - loss: 0.6087 - accuracy: 0.9067\n","Epoch 37/65\n","1/1 [==============================] - 0s 181ms/step - loss: 0.5604 - accuracy: 0.9067\n","Epoch 38/65\n","1/1 [==============================] - 0s 183ms/step - loss: 0.4839 - accuracy: 0.9067\n","Epoch 39/65\n","1/1 [==============================] - 0s 177ms/step - loss: 0.4452 - accuracy: 0.9067\n","Epoch 40/65\n","1/1 [==============================] - 0s 209ms/step - loss: 0.4431 - accuracy: 0.9067\n","Epoch 41/65\n","1/1 [==============================] - 0s 159ms/step - loss: 0.5223 - accuracy: 0.9067\n","Epoch 42/65\n","1/1 [==============================] - 0s 140ms/step - loss: 0.5484 - accuracy: 0.9067\n","Epoch 43/65\n","1/1 [==============================] - 0s 165ms/step - loss: 0.5736 - accuracy: 0.9067\n","Epoch 44/65\n","1/1 [==============================] - 0s 172ms/step - loss: 0.5719 - accuracy: 0.9067\n","Epoch 45/65\n","1/1 [==============================] - 0s 161ms/step - loss: 0.5685 - accuracy: 0.9067\n","Epoch 46/65\n","1/1 [==============================] - 0s 133ms/step - loss: 0.5655 - accuracy: 0.9067\n","Epoch 47/65\n","1/1 [==============================] - 0s 135ms/step - loss: 0.5636 - accuracy: 0.9067\n","Epoch 48/65\n","1/1 [==============================] - 0s 130ms/step - loss: 0.5622 - accuracy: 0.9087\n","Epoch 49/65\n","1/1 [==============================] - 0s 131ms/step - loss: 0.5606 - accuracy: 0.9107\n","Epoch 50/65\n","1/1 [==============================] - 0s 134ms/step - loss: 0.5585 - accuracy: 0.9107\n","Epoch 51/65\n","1/1 [==============================] - 0s 176ms/step - loss: 0.5560 - accuracy: 0.9127\n","Epoch 52/65\n","1/1 [==============================] - 0s 190ms/step - loss: 0.5538 - accuracy: 0.9147\n","Epoch 53/65\n","1/1 [==============================] - 0s 194ms/step - loss: 0.5520 - accuracy: 0.9167\n","Epoch 54/65\n","1/1 [==============================] - 0s 145ms/step - loss: 0.5502 - accuracy: 0.9187\n","Epoch 55/65\n","1/1 [==============================] - 0s 166ms/step - loss: 0.5473 - accuracy: 0.9187\n","Epoch 56/65\n","1/1 [==============================] - 0s 187ms/step - loss: 0.5441 - accuracy: 0.9187\n","Epoch 57/65\n","1/1 [==============================] - 0s 168ms/step - loss: 0.5418 - accuracy: 0.9187\n","Epoch 58/65\n","1/1 [==============================] - 0s 180ms/step - loss: 0.5399 - accuracy: 0.9206\n","Epoch 59/65\n","1/1 [==============================] - 0s 158ms/step - loss: 0.5160 - accuracy: 0.9206\n","Epoch 60/65\n","1/1 [==============================] - 0s 157ms/step - loss: 0.5108 - accuracy: 0.9226\n","Epoch 61/65\n","1/1 [==============================] - 0s 161ms/step - loss: 0.5133 - accuracy: 0.9226\n","Epoch 62/65\n","1/1 [==============================] - 0s 143ms/step - loss: 0.5075 - accuracy: 0.9226\n","Epoch 63/65\n","1/1 [==============================] - 0s 196ms/step - loss: 0.5100 - accuracy: 0.9226\n","Epoch 64/65\n","1/1 [==============================] - 0s 198ms/step - loss: 0.5123 - accuracy: 0.9226\n","Epoch 65/65\n","1/1 [==============================] - 0s 189ms/step - loss: 0.5128 - accuracy: 0.9187\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b457a7d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1057 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 13:59:43,158]\u001b[0m Trial 21 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 154, 'num_epochs': 65}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/146\n","1/1 [==============================] - 6s 6s/step - loss: 3.4505 - accuracy: 0.0278\n","Epoch 2/146\n","1/1 [==============================] - 0s 127ms/step - loss: 1.0093 - accuracy: 0.9067\n","Epoch 3/146\n","1/1 [==============================] - 0s 168ms/step - loss: 0.6977 - accuracy: 0.9067\n","Epoch 4/146\n","1/1 [==============================] - 0s 239ms/step - loss: 0.6570 - accuracy: 0.9067\n","Epoch 5/146\n","1/1 [==============================] - 0s 154ms/step - loss: 0.5950 - accuracy: 0.9067\n","Epoch 6/146\n","1/1 [==============================] - 0s 154ms/step - loss: 0.6131 - accuracy: 0.9067\n","Epoch 7/146\n","1/1 [==============================] - 0s 178ms/step - loss: 0.5785 - accuracy: 0.9067\n","Epoch 8/146\n","1/1 [==============================] - 0s 162ms/step - loss: 0.5445 - accuracy: 0.9067\n","Epoch 9/146\n","1/1 [==============================] - 0s 165ms/step - loss: 0.5138 - accuracy: 0.9067\n","Epoch 10/146\n","1/1 [==============================] - 0s 145ms/step - loss: 0.5450 - accuracy: 0.9067\n","Epoch 11/146\n","1/1 [==============================] - 0s 151ms/step - loss: 0.5632 - accuracy: 0.9067\n","Epoch 12/146\n","1/1 [==============================] - 0s 157ms/step - loss: 0.5626 - accuracy: 0.9067\n","Epoch 13/146\n","1/1 [==============================] - 0s 186ms/step - loss: 0.5308 - accuracy: 0.9067\n","Epoch 14/146\n","1/1 [==============================] - 0s 149ms/step - loss: 0.5422 - accuracy: 0.9067\n","Epoch 15/146\n","1/1 [==============================] - 0s 139ms/step - loss: 0.5243 - accuracy: 0.9067\n","Epoch 16/146\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5004 - accuracy: 0.9067\n","Epoch 17/146\n","1/1 [==============================] - 0s 196ms/step - loss: 0.4962 - accuracy: 0.9067\n","Epoch 18/146\n","1/1 [==============================] - 0s 159ms/step - loss: 0.4921 - accuracy: 0.9067\n","Epoch 19/146\n","1/1 [==============================] - 0s 131ms/step - loss: 0.4827 - accuracy: 0.9067\n","Epoch 20/146\n","1/1 [==============================] - 0s 156ms/step - loss: 0.4705 - accuracy: 0.9067\n","Epoch 21/146\n","1/1 [==============================] - 0s 154ms/step - loss: 0.4622 - accuracy: 0.9067\n","Epoch 22/146\n","1/1 [==============================] - 0s 161ms/step - loss: 0.4577 - accuracy: 0.9067\n","Epoch 23/146\n","1/1 [==============================] - 0s 170ms/step - loss: 0.4585 - accuracy: 0.9067\n","Epoch 24/146\n","1/1 [==============================] - 0s 137ms/step - loss: 0.4641 - accuracy: 0.9067\n","Epoch 25/146\n","1/1 [==============================] - 0s 129ms/step - loss: 0.4601 - accuracy: 0.9067\n","Epoch 26/146\n","1/1 [==============================] - 0s 140ms/step - loss: 0.5168 - accuracy: 0.9067\n","Epoch 27/146\n","1/1 [==============================] - 0s 135ms/step - loss: 0.6520 - accuracy: 0.9067\n","Epoch 28/146\n","1/1 [==============================] - 0s 137ms/step - loss: 0.7619 - accuracy: 0.9067\n","Epoch 29/146\n","1/1 [==============================] - 0s 147ms/step - loss: 0.8441 - accuracy: 0.9028\n","Epoch 30/146\n","1/1 [==============================] - 0s 167ms/step - loss: 0.9027 - accuracy: 0.8909\n","Epoch 31/146\n","1/1 [==============================] - 0s 170ms/step - loss: 0.9422 - accuracy: 0.8631\n","Epoch 32/146\n","1/1 [==============================] - 0s 166ms/step - loss: 0.9671 - accuracy: 0.8532\n","Epoch 33/146\n","1/1 [==============================] - 0s 179ms/step - loss: 0.9810 - accuracy: 0.8492\n","Epoch 34/146\n","1/1 [==============================] - 0s 161ms/step - loss: 0.9844 - accuracy: 0.8571\n","Epoch 35/146\n","1/1 [==============================] - 0s 148ms/step - loss: 0.9783 - accuracy: 0.8710\n","Epoch 36/146\n","1/1 [==============================] - 0s 147ms/step - loss: 0.9684 - accuracy: 0.8829\n","Epoch 37/146\n","1/1 [==============================] - 0s 174ms/step - loss: 0.9567 - accuracy: 0.9008\n","Epoch 38/146\n","1/1 [==============================] - 0s 165ms/step - loss: 0.9440 - accuracy: 0.9028\n","Epoch 39/146\n","1/1 [==============================] - 0s 186ms/step - loss: 0.9298 - accuracy: 0.9067\n","Epoch 40/146\n","1/1 [==============================] - 0s 169ms/step - loss: 0.9141 - accuracy: 0.9067\n","Epoch 41/146\n","1/1 [==============================] - 0s 164ms/step - loss: 0.8969 - accuracy: 0.9067\n","Epoch 42/146\n","1/1 [==============================] - 0s 168ms/step - loss: 0.8784 - accuracy: 0.9067\n","Epoch 43/146\n","1/1 [==============================] - 0s 176ms/step - loss: 0.8591 - accuracy: 0.9067\n","Epoch 44/146\n","1/1 [==============================] - 0s 168ms/step - loss: 0.8396 - accuracy: 0.9067\n","Epoch 45/146\n","1/1 [==============================] - 0s 198ms/step - loss: 0.8202 - accuracy: 0.9067\n","Epoch 46/146\n","1/1 [==============================] - 0s 197ms/step - loss: 0.8012 - accuracy: 0.9067\n","Epoch 47/146\n","1/1 [==============================] - 0s 157ms/step - loss: 0.7826 - accuracy: 0.9067\n","Epoch 48/146\n","1/1 [==============================] - 0s 159ms/step - loss: 0.7630 - accuracy: 0.9067\n","Epoch 49/146\n","1/1 [==============================] - 0s 163ms/step - loss: 0.7439 - accuracy: 0.9067\n","Epoch 50/146\n","1/1 [==============================] - 0s 155ms/step - loss: 0.7253 - accuracy: 0.9067\n","Epoch 51/146\n","1/1 [==============================] - 0s 174ms/step - loss: 0.7065 - accuracy: 0.9067\n","Epoch 52/146\n","1/1 [==============================] - 0s 163ms/step - loss: 0.6871 - accuracy: 0.9067\n","Epoch 53/146\n","1/1 [==============================] - 0s 167ms/step - loss: 0.6672 - accuracy: 0.9067\n","Epoch 54/146\n","1/1 [==============================] - 0s 166ms/step - loss: 0.6472 - accuracy: 0.9067\n","Epoch 55/146\n","1/1 [==============================] - 0s 156ms/step - loss: 0.6269 - accuracy: 0.9067\n","Epoch 56/146\n","1/1 [==============================] - 0s 171ms/step - loss: 0.6064 - accuracy: 0.9067\n","Epoch 57/146\n","1/1 [==============================] - 0s 188ms/step - loss: 0.5858 - accuracy: 0.9067\n","Epoch 58/146\n","1/1 [==============================] - 0s 164ms/step - loss: 0.5647 - accuracy: 0.9067\n","Epoch 59/146\n","1/1 [==============================] - 0s 144ms/step - loss: 0.5434 - accuracy: 0.9067\n","Epoch 60/146\n","1/1 [==============================] - 0s 142ms/step - loss: 0.5225 - accuracy: 0.9067\n","Epoch 61/146\n","1/1 [==============================] - 0s 168ms/step - loss: 0.5032 - accuracy: 0.9067\n","Epoch 62/146\n","1/1 [==============================] - 0s 269ms/step - loss: 0.4921 - accuracy: 0.9067\n","Epoch 63/146\n","1/1 [==============================] - 0s 155ms/step - loss: 0.5267 - accuracy: 0.9067\n","Epoch 64/146\n","1/1 [==============================] - 0s 161ms/step - loss: 0.5498 - accuracy: 0.9067\n","Epoch 65/146\n","1/1 [==============================] - 0s 156ms/step - loss: 0.5675 - accuracy: 0.9067\n","Epoch 66/146\n","1/1 [==============================] - 0s 237ms/step - loss: 0.5654 - accuracy: 0.9067\n","Epoch 67/146\n","1/1 [==============================] - 0s 241ms/step - loss: 0.5634 - accuracy: 0.9067\n","Epoch 68/146\n","1/1 [==============================] - 0s 228ms/step - loss: 0.5613 - accuracy: 0.9067\n","Epoch 69/146\n","1/1 [==============================] - 0s 223ms/step - loss: 0.5595 - accuracy: 0.9067\n","Epoch 70/146\n","1/1 [==============================] - 0s 201ms/step - loss: 0.5580 - accuracy: 0.9067\n","Epoch 71/146\n","1/1 [==============================] - 0s 185ms/step - loss: 0.5566 - accuracy: 0.9067\n","Epoch 72/146\n","1/1 [==============================] - 0s 230ms/step - loss: 0.5553 - accuracy: 0.9067\n","Epoch 73/146\n","1/1 [==============================] - 0s 230ms/step - loss: 0.5539 - accuracy: 0.9067\n","Epoch 74/146\n","1/1 [==============================] - 0s 252ms/step - loss: 0.5524 - accuracy: 0.9067\n","Epoch 75/146\n","1/1 [==============================] - 0s 184ms/step - loss: 0.5508 - accuracy: 0.9067\n","Epoch 76/146\n","1/1 [==============================] - 0s 186ms/step - loss: 0.5490 - accuracy: 0.9067\n","Epoch 77/146\n","1/1 [==============================] - 0s 196ms/step - loss: 0.5473 - accuracy: 0.9067\n","Epoch 78/146\n","1/1 [==============================] - 0s 183ms/step - loss: 0.5456 - accuracy: 0.9067\n","Epoch 79/146\n","1/1 [==============================] - 0s 163ms/step - loss: 0.5439 - accuracy: 0.9067\n","Epoch 80/146\n","1/1 [==============================] - 0s 189ms/step - loss: 0.5423 - accuracy: 0.9067\n","Epoch 81/146\n","1/1 [==============================] - 0s 191ms/step - loss: 0.5406 - accuracy: 0.9067\n","Epoch 82/146\n","1/1 [==============================] - 0s 196ms/step - loss: 0.5389 - accuracy: 0.9067\n","Epoch 83/146\n","1/1 [==============================] - 0s 178ms/step - loss: 0.5373 - accuracy: 0.9067\n","Epoch 84/146\n","1/1 [==============================] - 0s 161ms/step - loss: 0.5357 - accuracy: 0.9067\n","Epoch 85/146\n","1/1 [==============================] - 0s 169ms/step - loss: 0.5341 - accuracy: 0.9067\n","Epoch 86/146\n","1/1 [==============================] - 0s 172ms/step - loss: 0.5326 - accuracy: 0.9067\n","Epoch 87/146\n","1/1 [==============================] - 0s 178ms/step - loss: 0.5310 - accuracy: 0.9067\n","Epoch 88/146\n","1/1 [==============================] - 0s 182ms/step - loss: 0.5293 - accuracy: 0.9067\n","Epoch 89/146\n","1/1 [==============================] - 0s 196ms/step - loss: 0.5276 - accuracy: 0.9067\n","Epoch 90/146\n","1/1 [==============================] - 0s 226ms/step - loss: 0.5259 - accuracy: 0.9067\n","Epoch 91/146\n","1/1 [==============================] - 0s 166ms/step - loss: 0.5242 - accuracy: 0.9067\n","Epoch 92/146\n","1/1 [==============================] - 0s 169ms/step - loss: 0.5226 - accuracy: 0.9067\n","Epoch 93/146\n","1/1 [==============================] - 0s 160ms/step - loss: 0.5210 - accuracy: 0.9127\n","Epoch 94/146\n","1/1 [==============================] - 0s 160ms/step - loss: 0.5195 - accuracy: 0.9127\n","Epoch 95/146\n","1/1 [==============================] - 0s 157ms/step - loss: 0.5179 - accuracy: 0.9107\n","Epoch 96/146\n","1/1 [==============================] - 0s 156ms/step - loss: 0.5163 - accuracy: 0.9107\n","Epoch 97/146\n","1/1 [==============================] - 0s 174ms/step - loss: 0.5149 - accuracy: 0.9167\n","Epoch 98/146\n","1/1 [==============================] - 0s 183ms/step - loss: 0.5135 - accuracy: 0.9187\n","Epoch 99/146\n","1/1 [==============================] - 0s 206ms/step - loss: 0.5121 - accuracy: 0.9206\n","Epoch 100/146\n","1/1 [==============================] - 0s 295ms/step - loss: 0.5107 - accuracy: 0.9206\n","Epoch 101/146\n","1/1 [==============================] - 0s 246ms/step - loss: 0.5093 - accuracy: 0.9206\n","Epoch 102/146\n","1/1 [==============================] - 0s 184ms/step - loss: 0.5078 - accuracy: 0.9226\n","Epoch 103/146\n","1/1 [==============================] - 0s 276ms/step - loss: 0.5065 - accuracy: 0.9226\n","Epoch 104/146\n","1/1 [==============================] - 0s 208ms/step - loss: 0.5052 - accuracy: 0.9226\n","Epoch 105/146\n","1/1 [==============================] - 0s 210ms/step - loss: 0.5041 - accuracy: 0.9226\n","Epoch 106/146\n","1/1 [==============================] - 0s 199ms/step - loss: 0.5030 - accuracy: 0.9226\n","Epoch 107/146\n","1/1 [==============================] - 0s 246ms/step - loss: 0.5019 - accuracy: 0.9226\n","Epoch 108/146\n","1/1 [==============================] - 0s 259ms/step - loss: 0.5007 - accuracy: 0.9246\n","Epoch 109/146\n","1/1 [==============================] - 0s 209ms/step - loss: 0.4995 - accuracy: 0.9246\n","Epoch 110/146\n","1/1 [==============================] - 0s 210ms/step - loss: 0.4982 - accuracy: 0.9226\n","Epoch 111/146\n","1/1 [==============================] - 0s 234ms/step - loss: 0.4970 - accuracy: 0.9226\n","Epoch 112/146\n","1/1 [==============================] - 0s 243ms/step - loss: 0.4954 - accuracy: 0.9206\n","Epoch 113/146\n","1/1 [==============================] - 0s 236ms/step - loss: 0.4938 - accuracy: 0.9206\n","Epoch 114/146\n","1/1 [==============================] - 0s 281ms/step - loss: 0.4917 - accuracy: 0.9187\n","Epoch 115/146\n","1/1 [==============================] - 0s 165ms/step - loss: 0.4900 - accuracy: 0.9187\n","Epoch 116/146\n","1/1 [==============================] - 0s 185ms/step - loss: 0.4885 - accuracy: 0.9206\n","Epoch 117/146\n","1/1 [==============================] - 0s 220ms/step - loss: 0.4870 - accuracy: 0.9206\n","Epoch 118/146\n","1/1 [==============================] - 0s 272ms/step - loss: nan - accuracy: 0.9187\n","Epoch 119/146\n","1/1 [==============================] - 0s 249ms/step - loss: nan - accuracy: 0.9067\n","Epoch 120/146\n","1/1 [==============================] - 0s 247ms/step - loss: nan - accuracy: 0.9067\n","Epoch 121/146\n","1/1 [==============================] - 0s 330ms/step - loss: nan - accuracy: 0.9067\n","Epoch 122/146\n","1/1 [==============================] - 0s 279ms/step - loss: nan - accuracy: 0.9067\n","Epoch 123/146\n","1/1 [==============================] - 0s 262ms/step - loss: nan - accuracy: 0.9067\n","Epoch 124/146\n","1/1 [==============================] - 0s 266ms/step - loss: nan - accuracy: 0.9067\n","Epoch 125/146\n","1/1 [==============================] - 0s 257ms/step - loss: nan - accuracy: 0.9067\n","Epoch 126/146\n","1/1 [==============================] - 0s 230ms/step - loss: nan - accuracy: 0.9067\n","Epoch 127/146\n","1/1 [==============================] - 0s 252ms/step - loss: nan - accuracy: 0.9067\n","Epoch 128/146\n","1/1 [==============================] - 0s 253ms/step - loss: nan - accuracy: 0.9067\n","Epoch 129/146\n","1/1 [==============================] - 0s 225ms/step - loss: nan - accuracy: 0.9067\n","Epoch 130/146\n","1/1 [==============================] - 0s 171ms/step - loss: nan - accuracy: 0.9067\n","Epoch 131/146\n","1/1 [==============================] - 0s 203ms/step - loss: nan - accuracy: 0.9067\n","Epoch 132/146\n","1/1 [==============================] - 0s 225ms/step - loss: nan - accuracy: 0.9067\n","Epoch 133/146\n","1/1 [==============================] - 0s 170ms/step - loss: nan - accuracy: 0.9067\n","Epoch 134/146\n","1/1 [==============================] - 0s 226ms/step - loss: nan - accuracy: 0.9067\n","Epoch 135/146\n","1/1 [==============================] - 0s 258ms/step - loss: nan - accuracy: 0.9067\n","Epoch 136/146\n","1/1 [==============================] - 0s 383ms/step - loss: nan - accuracy: 0.9067\n","Epoch 137/146\n","1/1 [==============================] - 0s 206ms/step - loss: nan - accuracy: 0.9067\n","Epoch 138/146\n","1/1 [==============================] - 0s 191ms/step - loss: nan - accuracy: 0.9067\n","Epoch 139/146\n","1/1 [==============================] - 0s 169ms/step - loss: nan - accuracy: 0.9067\n","Epoch 140/146\n","1/1 [==============================] - 0s 212ms/step - loss: nan - accuracy: 0.9067\n","Epoch 141/146\n","1/1 [==============================] - 0s 168ms/step - loss: nan - accuracy: 0.9067\n","Epoch 142/146\n","1/1 [==============================] - 0s 153ms/step - loss: nan - accuracy: 0.9067\n","Epoch 143/146\n","1/1 [==============================] - 0s 211ms/step - loss: nan - accuracy: 0.9067\n","Epoch 144/146\n","1/1 [==============================] - 0s 169ms/step - loss: nan - accuracy: 0.9067\n","Epoch 145/146\n","1/1 [==============================] - 0s 164ms/step - loss: nan - accuracy: 0.9067\n","Epoch 146/146\n","1/1 [==============================] - 0s 157ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b27ae23a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:00:18,898]\u001b[0m Trial 22 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 169, 'num_epochs': 146}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/98\n","1/1 [==============================] - 5s 5s/step - loss: 14.9425 - accuracy: 0.0179\n","Epoch 2/98\n","1/1 [==============================] - 0s 125ms/step - loss: 1.8567 - accuracy: 0.5218\n","Epoch 3/98\n","1/1 [==============================] - 0s 166ms/step - loss: 1.0871 - accuracy: 0.9067\n","Epoch 4/98\n","1/1 [==============================] - 0s 190ms/step - loss: 1.2297 - accuracy: 0.9067\n","Epoch 5/98\n","1/1 [==============================] - 0s 186ms/step - loss: 1.3016 - accuracy: 0.4246\n","Epoch 6/98\n","1/1 [==============================] - 0s 201ms/step - loss: 1.3299 - accuracy: 0.2798\n","Epoch 7/98\n","1/1 [==============================] - 0s 138ms/step - loss: 1.3051 - accuracy: 0.2421\n","Epoch 8/98\n","1/1 [==============================] - 0s 186ms/step - loss: 1.2732 - accuracy: 0.2579\n","Epoch 9/98\n","1/1 [==============================] - 0s 228ms/step - loss: 1.2423 - accuracy: 0.2659\n","Epoch 10/98\n","1/1 [==============================] - 0s 211ms/step - loss: 1.2067 - accuracy: 0.2917\n","Epoch 11/98\n","1/1 [==============================] - 0s 180ms/step - loss: 1.1657 - accuracy: 0.3333\n","Epoch 12/98\n","1/1 [==============================] - 0s 187ms/step - loss: 1.1241 - accuracy: 0.3829\n","Epoch 13/98\n","1/1 [==============================] - 0s 185ms/step - loss: 1.0853 - accuracy: 0.4544\n","Epoch 14/98\n","1/1 [==============================] - 0s 217ms/step - loss: 1.0521 - accuracy: 0.5476\n","Epoch 15/98\n","1/1 [==============================] - 0s 144ms/step - loss: 1.0270 - accuracy: 0.6528\n","Epoch 16/98\n","1/1 [==============================] - 0s 161ms/step - loss: 1.0294 - accuracy: 0.7718\n","Epoch 17/98\n","1/1 [==============================] - 0s 142ms/step - loss: 1.0025 - accuracy: 0.8651\n","Epoch 18/98\n","1/1 [==============================] - 0s 145ms/step - loss: 0.9707 - accuracy: 0.9048\n","Epoch 19/98\n","1/1 [==============================] - 0s 153ms/step - loss: 0.9195 - accuracy: 0.9067\n","Epoch 20/98\n","1/1 [==============================] - 0s 307ms/step - loss: 0.8888 - accuracy: 0.9067\n","Epoch 21/98\n","1/1 [==============================] - 0s 275ms/step - loss: 0.8628 - accuracy: 0.9067\n","Epoch 22/98\n","1/1 [==============================] - 0s 262ms/step - loss: 0.8386 - accuracy: 0.9067\n","Epoch 23/98\n","1/1 [==============================] - 0s 203ms/step - loss: 0.8142 - accuracy: 0.9067\n","Epoch 24/98\n","1/1 [==============================] - 0s 226ms/step - loss: 0.7894 - accuracy: 0.9067\n","Epoch 25/98\n","1/1 [==============================] - 0s 171ms/step - loss: 0.7637 - accuracy: 0.9067\n","Epoch 26/98\n","1/1 [==============================] - 0s 166ms/step - loss: 0.7373 - accuracy: 0.9067\n","Epoch 27/98\n","1/1 [==============================] - 0s 187ms/step - loss: 0.7106 - accuracy: 0.9067\n","Epoch 28/98\n","1/1 [==============================] - 0s 142ms/step - loss: 0.6837 - accuracy: 0.9067\n","Epoch 29/98\n","1/1 [==============================] - 0s 172ms/step - loss: 0.6560 - accuracy: 0.9067\n","Epoch 30/98\n","1/1 [==============================] - 0s 166ms/step - loss: 0.6276 - accuracy: 0.9067\n","Epoch 31/98\n","1/1 [==============================] - 0s 138ms/step - loss: 0.5986 - accuracy: 0.9067\n","Epoch 32/98\n","1/1 [==============================] - 0s 228ms/step - loss: 0.5699 - accuracy: 0.9067\n","Epoch 33/98\n","1/1 [==============================] - 0s 144ms/step - loss: 0.5427 - accuracy: 0.9067\n","Epoch 34/98\n","1/1 [==============================] - 0s 185ms/step - loss: 0.5021 - accuracy: 0.9067\n","Epoch 35/98\n","1/1 [==============================] - 0s 231ms/step - loss: 0.4017 - accuracy: 0.9067\n","Epoch 36/98\n","1/1 [==============================] - 0s 172ms/step - loss: 0.4017 - accuracy: 0.9067\n","Epoch 37/98\n","1/1 [==============================] - 0s 150ms/step - loss: 0.4077 - accuracy: 0.9067\n","Epoch 38/98\n","1/1 [==============================] - 0s 148ms/step - loss: 0.4357 - accuracy: 0.9067\n","Epoch 39/98\n","1/1 [==============================] - 0s 145ms/step - loss: 0.4667 - accuracy: 0.9067\n","Epoch 40/98\n","1/1 [==============================] - 0s 170ms/step - loss: 0.4268 - accuracy: 0.9067\n","Epoch 41/98\n","1/1 [==============================] - 0s 199ms/step - loss: 0.3937 - accuracy: 0.9067\n","Epoch 42/98\n","1/1 [==============================] - 0s 208ms/step - loss: 0.3814 - accuracy: 0.9067\n","Epoch 43/98\n","1/1 [==============================] - 0s 247ms/step - loss: 0.3728 - accuracy: 0.9067\n","Epoch 44/98\n","1/1 [==============================] - 0s 212ms/step - loss: 0.3653 - accuracy: 0.9067\n","Epoch 45/98\n","1/1 [==============================] - 0s 211ms/step - loss: 0.3599 - accuracy: 0.9067\n","Epoch 46/98\n","1/1 [==============================] - 0s 225ms/step - loss: 0.3763 - accuracy: 0.9067\n","Epoch 47/98\n","1/1 [==============================] - 0s 194ms/step - loss: 0.3929 - accuracy: 0.9067\n","Epoch 48/98\n","1/1 [==============================] - 0s 196ms/step - loss: 0.3878 - accuracy: 0.9067\n","Epoch 49/98\n","1/1 [==============================] - 0s 164ms/step - loss: 0.3870 - accuracy: 0.9067\n","Epoch 50/98\n","1/1 [==============================] - 0s 167ms/step - loss: 0.3786 - accuracy: 0.9067\n","Epoch 51/98\n","1/1 [==============================] - 0s 139ms/step - loss: 0.3703 - accuracy: 0.9067\n","Epoch 52/98\n","1/1 [==============================] - 0s 154ms/step - loss: 0.3650 - accuracy: 0.9067\n","Epoch 53/98\n","1/1 [==============================] - 0s 140ms/step - loss: 0.3619 - accuracy: 0.9067\n","Epoch 54/98\n","1/1 [==============================] - 0s 141ms/step - loss: 0.3607 - accuracy: 0.9067\n","Epoch 55/98\n","1/1 [==============================] - 0s 143ms/step - loss: 0.3609 - accuracy: 0.9067\n","Epoch 56/98\n","1/1 [==============================] - 0s 141ms/step - loss: 0.3613 - accuracy: 0.9067\n","Epoch 57/98\n","1/1 [==============================] - 0s 154ms/step - loss: 0.3595 - accuracy: 0.9067\n","Epoch 58/98\n","1/1 [==============================] - 0s 187ms/step - loss: 0.3561 - accuracy: 0.9067\n","Epoch 59/98\n","1/1 [==============================] - 0s 156ms/step - loss: 0.3525 - accuracy: 0.9067\n","Epoch 60/98\n","1/1 [==============================] - 0s 158ms/step - loss: 0.3490 - accuracy: 0.9067\n","Epoch 61/98\n","1/1 [==============================] - 0s 171ms/step - loss: 0.3460 - accuracy: 0.9067\n","Epoch 62/98\n","1/1 [==============================] - 0s 181ms/step - loss: 0.3435 - accuracy: 0.9067\n","Epoch 63/98\n","1/1 [==============================] - 0s 176ms/step - loss: 0.3416 - accuracy: 0.9067\n","Epoch 64/98\n","1/1 [==============================] - 0s 168ms/step - loss: 0.3399 - accuracy: 0.9067\n","Epoch 65/98\n","1/1 [==============================] - 0s 221ms/step - loss: 0.3384 - accuracy: 0.9067\n","Epoch 66/98\n","1/1 [==============================] - 0s 168ms/step - loss: 0.3368 - accuracy: 0.9067\n","Epoch 67/98\n","1/1 [==============================] - 0s 223ms/step - loss: 0.3349 - accuracy: 0.9087\n","Epoch 68/98\n","1/1 [==============================] - 0s 205ms/step - loss: 0.3328 - accuracy: 0.9087\n","Epoch 69/98\n","1/1 [==============================] - 0s 143ms/step - loss: 0.3306 - accuracy: 0.9107\n","Epoch 70/98\n","1/1 [==============================] - 0s 151ms/step - loss: 0.3284 - accuracy: 0.9107\n","Epoch 71/98\n","1/1 [==============================] - 0s 160ms/step - loss: 0.3262 - accuracy: 0.9107\n","Epoch 72/98\n","1/1 [==============================] - 0s 142ms/step - loss: 0.3242 - accuracy: 0.9127\n","Epoch 73/98\n","1/1 [==============================] - 0s 133ms/step - loss: 0.3223 - accuracy: 0.9167\n","Epoch 74/98\n","1/1 [==============================] - 0s 192ms/step - loss: 0.3206 - accuracy: 0.9187\n","Epoch 75/98\n","1/1 [==============================] - 0s 207ms/step - loss: 0.3189 - accuracy: 0.9187\n","Epoch 76/98\n","1/1 [==============================] - 0s 168ms/step - loss: 0.3172 - accuracy: 0.9187\n","Epoch 77/98\n","1/1 [==============================] - 0s 261ms/step - loss: 0.3154 - accuracy: 0.9187\n","Epoch 78/98\n","1/1 [==============================] - 0s 159ms/step - loss: 0.3137 - accuracy: 0.9187\n","Epoch 79/98\n","1/1 [==============================] - 0s 137ms/step - loss: 0.3119 - accuracy: 0.9187\n","Epoch 80/98\n","1/1 [==============================] - 0s 146ms/step - loss: 0.3101 - accuracy: 0.9187\n","Epoch 81/98\n","1/1 [==============================] - 0s 151ms/step - loss: 0.3083 - accuracy: 0.9206\n","Epoch 82/98\n","1/1 [==============================] - 0s 138ms/step - loss: 0.3066 - accuracy: 0.9206\n","Epoch 83/98\n","1/1 [==============================] - 0s 147ms/step - loss: 0.3049 - accuracy: 0.9206\n","Epoch 84/98\n","1/1 [==============================] - 0s 144ms/step - loss: 0.3034 - accuracy: 0.9206\n","Epoch 85/98\n","1/1 [==============================] - 0s 145ms/step - loss: 0.3019 - accuracy: 0.9226\n","Epoch 86/98\n","1/1 [==============================] - 0s 159ms/step - loss: 0.3004 - accuracy: 0.9187\n","Epoch 87/98\n","1/1 [==============================] - 0s 156ms/step - loss: 0.2990 - accuracy: 0.9187\n","Epoch 88/98\n","1/1 [==============================] - 0s 189ms/step - loss: 0.2975 - accuracy: 0.9187\n","Epoch 89/98\n","1/1 [==============================] - 0s 178ms/step - loss: 0.2959 - accuracy: 0.9187\n","Epoch 90/98\n","1/1 [==============================] - 0s 162ms/step - loss: 0.2944 - accuracy: 0.9187\n","Epoch 91/98\n","1/1 [==============================] - 0s 180ms/step - loss: 0.2930 - accuracy: 0.9226\n","Epoch 92/98\n","1/1 [==============================] - 0s 218ms/step - loss: 0.2916 - accuracy: 0.9226\n","Epoch 93/98\n","1/1 [==============================] - 0s 185ms/step - loss: 0.2902 - accuracy: 0.9226\n","Epoch 94/98\n","1/1 [==============================] - 0s 194ms/step - loss: 0.2887 - accuracy: 0.9226\n","Epoch 95/98\n","1/1 [==============================] - 0s 152ms/step - loss: 0.2872 - accuracy: 0.9246\n","Epoch 96/98\n","1/1 [==============================] - 0s 149ms/step - loss: 0.2856 - accuracy: 0.9246\n","Epoch 97/98\n","1/1 [==============================] - 0s 156ms/step - loss: 0.2840 - accuracy: 0.9246\n","Epoch 98/98\n","1/1 [==============================] - 0s 139ms/step - loss: 0.2827 - accuracy: 0.9246\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b246d8ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.0417 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:00:43,436]\u001b[0m Trial 23 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 113, 'num_epochs': 98}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/66\n","1/1 [==============================] - 6s 6s/step - loss: 3.2146 - accuracy: 0.0615\n","Epoch 2/66\n","1/1 [==============================] - 0s 170ms/step - loss: 1.9681 - accuracy: 0.9067\n","Epoch 3/66\n","1/1 [==============================] - 0s 220ms/step - loss: 1.5977 - accuracy: 0.9067\n","Epoch 4/66\n","1/1 [==============================] - 0s 152ms/step - loss: 1.2526 - accuracy: 0.9067\n","Epoch 5/66\n","1/1 [==============================] - 0s 198ms/step - loss: 1.0848 - accuracy: 0.9067\n","Epoch 6/66\n","1/1 [==============================] - 0s 191ms/step - loss: 1.0224 - accuracy: 0.9067\n","Epoch 7/66\n","1/1 [==============================] - 0s 181ms/step - loss: 0.9960 - accuracy: 0.9067\n","Epoch 8/66\n","1/1 [==============================] - 0s 176ms/step - loss: 0.9789 - accuracy: 0.9067\n","Epoch 9/66\n","1/1 [==============================] - 0s 172ms/step - loss: 0.9747 - accuracy: 0.9067\n","Epoch 10/66\n","1/1 [==============================] - 0s 191ms/step - loss: 0.9785 - accuracy: 0.9067\n","Epoch 11/66\n","1/1 [==============================] - 0s 189ms/step - loss: 0.9676 - accuracy: 0.9067\n","Epoch 12/66\n","1/1 [==============================] - 0s 181ms/step - loss: 0.9663 - accuracy: 0.9067\n","Epoch 13/66\n","1/1 [==============================] - 0s 232ms/step - loss: 0.9683 - accuracy: 0.9067\n","Epoch 14/66\n","1/1 [==============================] - 0s 152ms/step - loss: 0.9557 - accuracy: 0.9067\n","Epoch 15/66\n","1/1 [==============================] - 0s 190ms/step - loss: 0.9502 - accuracy: 0.9067\n","Epoch 16/66\n","1/1 [==============================] - 0s 128ms/step - loss: 0.9466 - accuracy: 0.9067\n","Epoch 17/66\n","1/1 [==============================] - 0s 170ms/step - loss: 0.9436 - accuracy: 0.9067\n","Epoch 18/66\n","1/1 [==============================] - 0s 188ms/step - loss: 0.9411 - accuracy: 0.9067\n","Epoch 19/66\n","1/1 [==============================] - 0s 166ms/step - loss: 0.9389 - accuracy: 0.9067\n","Epoch 20/66\n","1/1 [==============================] - 0s 154ms/step - loss: 0.9370 - accuracy: 0.9067\n","Epoch 21/66\n","1/1 [==============================] - 0s 154ms/step - loss: 0.9352 - accuracy: 0.9067\n","Epoch 22/66\n","1/1 [==============================] - 0s 126ms/step - loss: 0.9332 - accuracy: 0.9067\n","Epoch 23/66\n","1/1 [==============================] - 0s 136ms/step - loss: 0.9312 - accuracy: 0.9067\n","Epoch 24/66\n","1/1 [==============================] - 0s 129ms/step - loss: 0.9292 - accuracy: 0.9067\n","Epoch 25/66\n","1/1 [==============================] - 0s 115ms/step - loss: 0.9273 - accuracy: 0.9067\n","Epoch 26/66\n","1/1 [==============================] - 0s 125ms/step - loss: 0.9254 - accuracy: 0.9067\n","Epoch 27/66\n","1/1 [==============================] - 0s 133ms/step - loss: 0.9237 - accuracy: 0.9067\n","Epoch 28/66\n","1/1 [==============================] - 0s 134ms/step - loss: 0.9221 - accuracy: 0.9067\n","Epoch 29/66\n","1/1 [==============================] - 0s 137ms/step - loss: 0.9208 - accuracy: 0.9067\n","Epoch 30/66\n","1/1 [==============================] - 0s 164ms/step - loss: 0.9196 - accuracy: 0.9067\n","Epoch 31/66\n","1/1 [==============================] - 0s 116ms/step - loss: 0.9188 - accuracy: 0.9067\n","Epoch 32/66\n","1/1 [==============================] - 0s 133ms/step - loss: 0.9181 - accuracy: 0.9067\n","Epoch 33/66\n","1/1 [==============================] - 0s 158ms/step - loss: 0.9173 - accuracy: 0.9067\n","Epoch 34/66\n","1/1 [==============================] - 0s 144ms/step - loss: 0.9163 - accuracy: 0.9067\n","Epoch 35/66\n","1/1 [==============================] - 0s 162ms/step - loss: 0.9152 - accuracy: 0.9067\n","Epoch 36/66\n","1/1 [==============================] - 0s 177ms/step - loss: 0.9139 - accuracy: 0.9067\n","Epoch 37/66\n","1/1 [==============================] - 0s 150ms/step - loss: 0.9127 - accuracy: 0.9067\n","Epoch 38/66\n","1/1 [==============================] - 0s 230ms/step - loss: 0.9117 - accuracy: 0.9067\n","Epoch 39/66\n","1/1 [==============================] - 0s 126ms/step - loss: 0.9107 - accuracy: 0.9067\n","Epoch 40/66\n","1/1 [==============================] - 0s 144ms/step - loss: 0.9098 - accuracy: 0.9067\n","Epoch 41/66\n","1/1 [==============================] - 0s 157ms/step - loss: 0.9090 - accuracy: 0.9067\n","Epoch 42/66\n","1/1 [==============================] - 0s 156ms/step - loss: 0.9082 - accuracy: 0.9067\n","Epoch 43/66\n","1/1 [==============================] - 0s 147ms/step - loss: 0.9075 - accuracy: 0.9067\n","Epoch 44/66\n","1/1 [==============================] - 0s 139ms/step - loss: 0.9068 - accuracy: 0.9067\n","Epoch 45/66\n","1/1 [==============================] - 0s 135ms/step - loss: 0.9061 - accuracy: 0.9067\n","Epoch 46/66\n","1/1 [==============================] - 0s 140ms/step - loss: 0.9053 - accuracy: 0.9067\n","Epoch 47/66\n","1/1 [==============================] - 0s 172ms/step - loss: 0.9046 - accuracy: 0.9067\n","Epoch 48/66\n","1/1 [==============================] - 0s 143ms/step - loss: 0.9039 - accuracy: 0.9067\n","Epoch 49/66\n","1/1 [==============================] - 0s 138ms/step - loss: 0.9031 - accuracy: 0.9067\n","Epoch 50/66\n","1/1 [==============================] - 0s 131ms/step - loss: 0.9024 - accuracy: 0.9067\n","Epoch 51/66\n","1/1 [==============================] - 0s 148ms/step - loss: 0.9017 - accuracy: 0.9067\n","Epoch 52/66\n","1/1 [==============================] - 0s 111ms/step - loss: 0.9010 - accuracy: 0.9067\n","Epoch 53/66\n","1/1 [==============================] - 0s 125ms/step - loss: 0.9003 - accuracy: 0.9067\n","Epoch 54/66\n","1/1 [==============================] - 0s 128ms/step - loss: 0.8996 - accuracy: 0.9067\n","Epoch 55/66\n","1/1 [==============================] - 0s 145ms/step - loss: 0.8989 - accuracy: 0.9067\n","Epoch 56/66\n","1/1 [==============================] - 0s 142ms/step - loss: 0.8983 - accuracy: 0.9067\n","Epoch 57/66\n","1/1 [==============================] - 0s 128ms/step - loss: 0.8976 - accuracy: 0.9067\n","Epoch 58/66\n","1/1 [==============================] - 0s 136ms/step - loss: 0.8969 - accuracy: 0.9067\n","Epoch 59/66\n","1/1 [==============================] - 0s 149ms/step - loss: 0.8962 - accuracy: 0.9067\n","Epoch 60/66\n","1/1 [==============================] - 0s 97ms/step - loss: 0.8955 - accuracy: 0.9087\n","Epoch 61/66\n","1/1 [==============================] - 0s 65ms/step - loss: 0.8948 - accuracy: 0.9087\n","Epoch 62/66\n","1/1 [==============================] - 0s 109ms/step - loss: 0.8940 - accuracy: 0.9087\n","Epoch 63/66\n","1/1 [==============================] - 0s 107ms/step - loss: 0.8933 - accuracy: 0.9107\n","Epoch 64/66\n","1/1 [==============================] - 0s 115ms/step - loss: 0.8927 - accuracy: 0.9107\n","Epoch 65/66\n","1/1 [==============================] - 0s 105ms/step - loss: 0.8919 - accuracy: 0.9127\n","Epoch 66/66\n","1/1 [==============================] - 0s 87ms/step - loss: 0.8912 - accuracy: 0.9147\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b457a7310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1766 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:01:01,255]\u001b[0m Trial 24 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 28, 'num_epochs': 66}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/147\n","1/1 [==============================] - 5s 5s/step - loss: 3.4354 - accuracy: 0.0635\n","Epoch 2/147\n","1/1 [==============================] - 0s 184ms/step - loss: 1.2454 - accuracy: 0.9067\n","Epoch 3/147\n","1/1 [==============================] - 0s 206ms/step - loss: 1.1721 - accuracy: 0.9067\n","Epoch 4/147\n","1/1 [==============================] - 0s 156ms/step - loss: 1.1740 - accuracy: 0.9067\n","Epoch 5/147\n","1/1 [==============================] - 0s 205ms/step - loss: 1.2184 - accuracy: 0.9067\n","Epoch 6/147\n","1/1 [==============================] - 0s 176ms/step - loss: 1.2527 - accuracy: 0.9067\n","Epoch 7/147\n","1/1 [==============================] - 0s 242ms/step - loss: 1.2761 - accuracy: 0.9067\n","Epoch 8/147\n","1/1 [==============================] - 0s 154ms/step - loss: 1.2775 - accuracy: 0.9067\n","Epoch 9/147\n","1/1 [==============================] - 0s 189ms/step - loss: 1.2616 - accuracy: 0.9067\n","Epoch 10/147\n","1/1 [==============================] - 0s 201ms/step - loss: 1.2360 - accuracy: 0.9067\n","Epoch 11/147\n","1/1 [==============================] - 0s 174ms/step - loss: 1.2060 - accuracy: 0.9067\n","Epoch 12/147\n","1/1 [==============================] - 0s 207ms/step - loss: 1.1752 - accuracy: 0.9067\n","Epoch 13/147\n","1/1 [==============================] - 0s 175ms/step - loss: 1.1477 - accuracy: 0.9067\n","Epoch 14/147\n","1/1 [==============================] - 0s 231ms/step - loss: 1.1270 - accuracy: 0.9067\n","Epoch 15/147\n","1/1 [==============================] - 0s 192ms/step - loss: 1.1139 - accuracy: 0.9067\n","Epoch 16/147\n","1/1 [==============================] - 0s 154ms/step - loss: 1.1313 - accuracy: 0.9067\n","Epoch 17/147\n","1/1 [==============================] - 0s 174ms/step - loss: 1.1785 - accuracy: 0.9067\n","Epoch 18/147\n","1/1 [==============================] - 0s 179ms/step - loss: 1.2013 - accuracy: 0.9067\n","Epoch 19/147\n","1/1 [==============================] - 0s 159ms/step - loss: 1.2000 - accuracy: 0.9067\n","Epoch 20/147\n","1/1 [==============================] - 0s 159ms/step - loss: 1.1991 - accuracy: 0.9067\n","Epoch 21/147\n","1/1 [==============================] - 0s 157ms/step - loss: 1.1986 - accuracy: 0.9067\n","Epoch 22/147\n","1/1 [==============================] - 0s 166ms/step - loss: 1.1983 - accuracy: 0.9067\n","Epoch 23/147\n","1/1 [==============================] - 0s 156ms/step - loss: 1.1981 - accuracy: 0.9067\n","Epoch 24/147\n","1/1 [==============================] - 0s 171ms/step - loss: 1.1977 - accuracy: 0.9067\n","Epoch 25/147\n","1/1 [==============================] - 0s 169ms/step - loss: 1.1972 - accuracy: 0.9067\n","Epoch 26/147\n","1/1 [==============================] - 0s 163ms/step - loss: 1.1966 - accuracy: 0.9067\n","Epoch 27/147\n","1/1 [==============================] - 0s 189ms/step - loss: 1.1959 - accuracy: 0.9067\n","Epoch 28/147\n","1/1 [==============================] - 0s 165ms/step - loss: 1.1952 - accuracy: 0.9067\n","Epoch 29/147\n","1/1 [==============================] - 0s 160ms/step - loss: 1.1946 - accuracy: 0.9067\n","Epoch 30/147\n","1/1 [==============================] - 0s 159ms/step - loss: 1.1940 - accuracy: 0.9067\n","Epoch 31/147\n","1/1 [==============================] - 0s 170ms/step - loss: 1.1934 - accuracy: 0.9067\n","Epoch 32/147\n","1/1 [==============================] - 0s 169ms/step - loss: 1.1929 - accuracy: 0.9067\n","Epoch 33/147\n","1/1 [==============================] - 0s 176ms/step - loss: 1.1923 - accuracy: 0.9067\n","Epoch 34/147\n","1/1 [==============================] - 0s 157ms/step - loss: 1.1917 - accuracy: 0.9067\n","Epoch 35/147\n","1/1 [==============================] - 0s 167ms/step - loss: 1.1910 - accuracy: 0.9067\n","Epoch 36/147\n","1/1 [==============================] - 0s 147ms/step - loss: 1.1904 - accuracy: 0.9067\n","Epoch 37/147\n","1/1 [==============================] - 0s 141ms/step - loss: 1.1897 - accuracy: 0.9067\n","Epoch 38/147\n","1/1 [==============================] - 0s 156ms/step - loss: 1.1891 - accuracy: 0.9067\n","Epoch 39/147\n","1/1 [==============================] - 0s 163ms/step - loss: 1.1884 - accuracy: 0.9067\n","Epoch 40/147\n","1/1 [==============================] - 0s 188ms/step - loss: 1.1877 - accuracy: 0.9067\n","Epoch 41/147\n","1/1 [==============================] - 0s 187ms/step - loss: 1.1871 - accuracy: 0.9067\n","Epoch 42/147\n","1/1 [==============================] - 0s 176ms/step - loss: 1.1865 - accuracy: 0.9067\n","Epoch 43/147\n","1/1 [==============================] - 0s 169ms/step - loss: 1.1858 - accuracy: 0.9067\n","Epoch 44/147\n","1/1 [==============================] - 0s 166ms/step - loss: 1.1852 - accuracy: 0.9067\n","Epoch 45/147\n","1/1 [==============================] - 0s 140ms/step - loss: 1.1845 - accuracy: 0.9067\n","Epoch 46/147\n","1/1 [==============================] - 0s 220ms/step - loss: 1.1838 - accuracy: 0.9067\n","Epoch 47/147\n","1/1 [==============================] - 0s 150ms/step - loss: 1.1831 - accuracy: 0.9067\n","Epoch 48/147\n","1/1 [==============================] - 0s 174ms/step - loss: 1.1823 - accuracy: 0.9067\n","Epoch 49/147\n","1/1 [==============================] - 0s 173ms/step - loss: 1.1815 - accuracy: 0.9087\n","Epoch 50/147\n","1/1 [==============================] - 0s 172ms/step - loss: 1.1808 - accuracy: 0.9107\n","Epoch 51/147\n","1/1 [==============================] - 0s 182ms/step - loss: 1.1799 - accuracy: 0.9127\n","Epoch 52/147\n","1/1 [==============================] - 0s 208ms/step - loss: 1.1789 - accuracy: 0.9147\n","Epoch 53/147\n","1/1 [==============================] - 0s 169ms/step - loss: 1.1778 - accuracy: 0.9167\n","Epoch 54/147\n","1/1 [==============================] - 0s 228ms/step - loss: 1.1766 - accuracy: 0.9167\n","Epoch 55/147\n","1/1 [==============================] - 0s 193ms/step - loss: 1.1753 - accuracy: 0.9167\n","Epoch 56/147\n","1/1 [==============================] - 0s 197ms/step - loss: 1.1736 - accuracy: 0.9167\n","Epoch 57/147\n","1/1 [==============================] - 0s 161ms/step - loss: 1.1714 - accuracy: 0.9187\n","Epoch 58/147\n","1/1 [==============================] - 0s 224ms/step - loss: 1.1699 - accuracy: 0.9167\n","Epoch 59/147\n","1/1 [==============================] - 0s 240ms/step - loss: 1.1962 - accuracy: 0.9187\n","Epoch 60/147\n","1/1 [==============================] - 0s 185ms/step - loss: 1.1969 - accuracy: 0.9187\n","Epoch 61/147\n","1/1 [==============================] - 0s 215ms/step - loss: 1.2304 - accuracy: 0.9147\n","Epoch 62/147\n","1/1 [==============================] - 0s 196ms/step - loss: 1.2363 - accuracy: 0.9147\n","Epoch 63/147\n","1/1 [==============================] - 0s 217ms/step - loss: 1.1807 - accuracy: 0.9127\n","Epoch 64/147\n","1/1 [==============================] - 0s 215ms/step - loss: 1.1747 - accuracy: 0.9127\n","Epoch 65/147\n","1/1 [==============================] - 0s 233ms/step - loss: 1.1754 - accuracy: 0.9206\n","Epoch 66/147\n","1/1 [==============================] - 0s 165ms/step - loss: 1.1773 - accuracy: 0.9187\n","Epoch 67/147\n","1/1 [==============================] - 0s 229ms/step - loss: 1.1790 - accuracy: 0.9226\n","Epoch 68/147\n","1/1 [==============================] - 0s 237ms/step - loss: 1.1805 - accuracy: 0.9226\n","Epoch 69/147\n","1/1 [==============================] - 0s 175ms/step - loss: 1.1808 - accuracy: 0.9226\n","Epoch 70/147\n","1/1 [==============================] - 0s 190ms/step - loss: 1.1801 - accuracy: 0.9226\n","Epoch 71/147\n","1/1 [==============================] - 0s 227ms/step - loss: 1.1787 - accuracy: 0.9226\n","Epoch 72/147\n","1/1 [==============================] - 0s 253ms/step - loss: 1.1776 - accuracy: 0.9187\n","Epoch 73/147\n","1/1 [==============================] - 0s 236ms/step - loss: 1.1765 - accuracy: 0.9206\n","Epoch 74/147\n","1/1 [==============================] - 0s 180ms/step - loss: 1.1755 - accuracy: 0.9167\n","Epoch 75/147\n","1/1 [==============================] - 0s 231ms/step - loss: 1.1749 - accuracy: 0.9167\n","Epoch 76/147\n","1/1 [==============================] - 0s 162ms/step - loss: 1.1746 - accuracy: 0.9167\n","Epoch 77/147\n","1/1 [==============================] - 0s 171ms/step - loss: 1.1745 - accuracy: 0.9147\n","Epoch 78/147\n","1/1 [==============================] - 0s 195ms/step - loss: 1.1745 - accuracy: 0.9147\n","Epoch 79/147\n","1/1 [==============================] - 0s 185ms/step - loss: 1.1742 - accuracy: 0.9147\n","Epoch 80/147\n","1/1 [==============================] - 0s 208ms/step - loss: 1.1737 - accuracy: 0.9167\n","Epoch 81/147\n","1/1 [==============================] - 0s 243ms/step - loss: 1.1730 - accuracy: 0.9167\n","Epoch 82/147\n","1/1 [==============================] - 0s 176ms/step - loss: 1.1721 - accuracy: 0.9167\n","Epoch 83/147\n","1/1 [==============================] - 0s 252ms/step - loss: 1.1712 - accuracy: 0.9187\n","Epoch 84/147\n","1/1 [==============================] - 0s 253ms/step - loss: 1.1705 - accuracy: 0.9187\n","Epoch 85/147\n","1/1 [==============================] - 0s 232ms/step - loss: 1.1701 - accuracy: 0.9246\n","Epoch 86/147\n","1/1 [==============================] - 0s 195ms/step - loss: 1.1701 - accuracy: 0.9226\n","Epoch 87/147\n","1/1 [==============================] - 0s 206ms/step - loss: 1.1703 - accuracy: 0.9226\n","Epoch 88/147\n","1/1 [==============================] - 0s 312ms/step - loss: 1.1691 - accuracy: 0.9226\n","Epoch 89/147\n","1/1 [==============================] - 0s 198ms/step - loss: 1.1680 - accuracy: 0.9226\n","Epoch 90/147\n","1/1 [==============================] - 0s 186ms/step - loss: 1.1673 - accuracy: 0.9226\n","Epoch 91/147\n","1/1 [==============================] - 0s 176ms/step - loss: 1.1670 - accuracy: 0.9226\n","Epoch 92/147\n","1/1 [==============================] - 0s 176ms/step - loss: 1.1668 - accuracy: 0.9226\n","Epoch 93/147\n","1/1 [==============================] - 0s 182ms/step - loss: 1.1664 - accuracy: 0.9226\n","Epoch 94/147\n","1/1 [==============================] - 0s 183ms/step - loss: 1.1657 - accuracy: 0.9226\n","Epoch 95/147\n","1/1 [==============================] - 0s 181ms/step - loss: 1.1645 - accuracy: 0.9226\n","Epoch 96/147\n","1/1 [==============================] - 0s 159ms/step - loss: 1.1637 - accuracy: 0.9226\n","Epoch 97/147\n","1/1 [==============================] - 0s 152ms/step - loss: 1.1630 - accuracy: 0.9206\n","Epoch 98/147\n","1/1 [==============================] - 0s 154ms/step - loss: 1.1629 - accuracy: 0.9226\n","Epoch 99/147\n","1/1 [==============================] - 0s 176ms/step - loss: 1.1630 - accuracy: 0.9226\n","Epoch 100/147\n","1/1 [==============================] - 0s 176ms/step - loss: 1.1618 - accuracy: 0.9226\n","Epoch 101/147\n","1/1 [==============================] - 0s 215ms/step - loss: 1.1615 - accuracy: 0.9206\n","Epoch 102/147\n","1/1 [==============================] - 0s 183ms/step - loss: 1.1613 - accuracy: 0.9206\n","Epoch 103/147\n","1/1 [==============================] - 0s 186ms/step - loss: 1.1610 - accuracy: 0.9206\n","Epoch 104/147\n","1/1 [==============================] - 0s 177ms/step - loss: 1.1607 - accuracy: 0.9206\n","Epoch 105/147\n","1/1 [==============================] - 0s 184ms/step - loss: 1.1604 - accuracy: 0.9206\n","Epoch 106/147\n","1/1 [==============================] - 0s 174ms/step - loss: 1.1599 - accuracy: 0.9206\n","Epoch 107/147\n","1/1 [==============================] - 0s 160ms/step - loss: 1.1595 - accuracy: 0.9226\n","Epoch 108/147\n","1/1 [==============================] - 0s 196ms/step - loss: 1.1592 - accuracy: 0.9226\n","Epoch 109/147\n","1/1 [==============================] - 0s 214ms/step - loss: 1.1589 - accuracy: 0.9246\n","Epoch 110/147\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9246\n","Epoch 111/147\n","1/1 [==============================] - 0s 197ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/147\n","1/1 [==============================] - 0s 165ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/147\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/147\n","1/1 [==============================] - 0s 151ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/147\n","1/1 [==============================] - 0s 162ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/147\n","1/1 [==============================] - 0s 245ms/step - loss: nan - accuracy: 0.9067\n","Epoch 117/147\n","1/1 [==============================] - 0s 245ms/step - loss: nan - accuracy: 0.9067\n","Epoch 118/147\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 119/147\n","1/1 [==============================] - 0s 199ms/step - loss: nan - accuracy: 0.9067\n","Epoch 120/147\n","1/1 [==============================] - 0s 156ms/step - loss: nan - accuracy: 0.9067\n","Epoch 121/147\n","1/1 [==============================] - 0s 155ms/step - loss: nan - accuracy: 0.9067\n","Epoch 122/147\n","1/1 [==============================] - 0s 190ms/step - loss: nan - accuracy: 0.9067\n","Epoch 123/147\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 124/147\n","1/1 [==============================] - 0s 169ms/step - loss: nan - accuracy: 0.9067\n","Epoch 125/147\n","1/1 [==============================] - 0s 165ms/step - loss: nan - accuracy: 0.9067\n","Epoch 126/147\n","1/1 [==============================] - 0s 190ms/step - loss: nan - accuracy: 0.9067\n","Epoch 127/147\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 128/147\n","1/1 [==============================] - 0s 313ms/step - loss: nan - accuracy: 0.9067\n","Epoch 129/147\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 130/147\n","1/1 [==============================] - 0s 188ms/step - loss: nan - accuracy: 0.9067\n","Epoch 131/147\n","1/1 [==============================] - 0s 199ms/step - loss: nan - accuracy: 0.9067\n","Epoch 132/147\n","1/1 [==============================] - 0s 153ms/step - loss: nan - accuracy: 0.9067\n","Epoch 133/147\n","1/1 [==============================] - 0s 158ms/step - loss: nan - accuracy: 0.9067\n","Epoch 134/147\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 135/147\n","1/1 [==============================] - 0s 194ms/step - loss: nan - accuracy: 0.9067\n","Epoch 136/147\n","1/1 [==============================] - 0s 212ms/step - loss: nan - accuracy: 0.9067\n","Epoch 137/147\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 138/147\n","1/1 [==============================] - 0s 219ms/step - loss: nan - accuracy: 0.9067\n","Epoch 139/147\n","1/1 [==============================] - 0s 178ms/step - loss: nan - accuracy: 0.9067\n","Epoch 140/147\n","1/1 [==============================] - 0s 219ms/step - loss: nan - accuracy: 0.9067\n","Epoch 141/147\n","1/1 [==============================] - 0s 210ms/step - loss: nan - accuracy: 0.9067\n","Epoch 142/147\n","1/1 [==============================] - 0s 211ms/step - loss: nan - accuracy: 0.9067\n","Epoch 143/147\n","1/1 [==============================] - 0s 222ms/step - loss: nan - accuracy: 0.9067\n","Epoch 144/147\n","1/1 [==============================] - 0s 215ms/step - loss: nan - accuracy: 0.9067\n","Epoch 145/147\n","1/1 [==============================] - 0s 212ms/step - loss: nan - accuracy: 0.9067\n","Epoch 146/147\n","1/1 [==============================] - 0s 216ms/step - loss: nan - accuracy: 0.9067\n","Epoch 147/147\n","1/1 [==============================] - 0s 236ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b454d1550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:01:36,662]\u001b[0m Trial 25 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 162, 'num_epochs': 147}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/123\n","1/1 [==============================] - 6s 6s/step - loss: 3.6345 - accuracy: 0.0000e+00\n","Epoch 2/123\n","1/1 [==============================] - 0s 171ms/step - loss: 1.7201 - accuracy: 0.9067\n","Epoch 3/123\n","1/1 [==============================] - 0s 139ms/step - loss: 1.5508 - accuracy: 0.9067\n","Epoch 4/123\n","1/1 [==============================] - 0s 187ms/step - loss: 1.4369 - accuracy: 0.9067\n","Epoch 5/123\n","1/1 [==============================] - 0s 212ms/step - loss: 1.3211 - accuracy: 0.9067\n","Epoch 6/123\n","1/1 [==============================] - 0s 170ms/step - loss: 1.1953 - accuracy: 0.9067\n","Epoch 7/123\n","1/1 [==============================] - 0s 194ms/step - loss: 1.1570 - accuracy: 0.9067\n","Epoch 8/123\n","1/1 [==============================] - 0s 222ms/step - loss: 1.1525 - accuracy: 0.9067\n","Epoch 9/123\n","1/1 [==============================] - 0s 156ms/step - loss: 1.1291 - accuracy: 0.9067\n","Epoch 10/123\n","1/1 [==============================] - 0s 214ms/step - loss: 1.0985 - accuracy: 0.9067\n","Epoch 11/123\n","1/1 [==============================] - 0s 156ms/step - loss: 1.0617 - accuracy: 0.9067\n","Epoch 12/123\n","1/1 [==============================] - 0s 153ms/step - loss: 1.0462 - accuracy: 0.9067\n","Epoch 13/123\n","1/1 [==============================] - 0s 208ms/step - loss: 1.0353 - accuracy: 0.9067\n","Epoch 14/123\n","1/1 [==============================] - 0s 193ms/step - loss: 1.0826 - accuracy: 0.9067\n","Epoch 15/123\n","1/1 [==============================] - 0s 216ms/step - loss: 1.1519 - accuracy: 0.9067\n","Epoch 16/123\n","1/1 [==============================] - 0s 190ms/step - loss: 1.1768 - accuracy: 0.9067\n","Epoch 17/123\n","1/1 [==============================] - 0s 181ms/step - loss: 1.1804 - accuracy: 0.9067\n","Epoch 18/123\n","1/1 [==============================] - 0s 179ms/step - loss: 1.1395 - accuracy: 0.9067\n","Epoch 19/123\n","1/1 [==============================] - 0s 171ms/step - loss: 0.9945 - accuracy: 0.9067\n","Epoch 20/123\n","1/1 [==============================] - 0s 169ms/step - loss: 1.0378 - accuracy: 0.9067\n","Epoch 21/123\n","1/1 [==============================] - 0s 168ms/step - loss: 1.0625 - accuracy: 0.9067\n","Epoch 22/123\n","1/1 [==============================] - 0s 183ms/step - loss: 1.0951 - accuracy: 0.9008\n","Epoch 23/123\n","1/1 [==============================] - 0s 180ms/step - loss: 1.1201 - accuracy: 0.8968\n","Epoch 24/123\n","1/1 [==============================] - 0s 176ms/step - loss: 1.1380 - accuracy: 0.8929\n","Epoch 25/123\n","1/1 [==============================] - 0s 216ms/step - loss: 1.1495 - accuracy: 0.8929\n","Epoch 26/123\n","1/1 [==============================] - 0s 202ms/step - loss: 1.1556 - accuracy: 0.8929\n","Epoch 27/123\n","1/1 [==============================] - 0s 223ms/step - loss: 1.1526 - accuracy: 0.8948\n","Epoch 28/123\n","1/1 [==============================] - 0s 178ms/step - loss: 1.1437 - accuracy: 0.8909\n","Epoch 29/123\n","1/1 [==============================] - 0s 212ms/step - loss: 1.1300 - accuracy: 0.8948\n","Epoch 30/123\n","1/1 [==============================] - 0s 183ms/step - loss: 1.1133 - accuracy: 0.9008\n","Epoch 31/123\n","1/1 [==============================] - 0s 187ms/step - loss: 1.0954 - accuracy: 0.9048\n","Epoch 32/123\n","1/1 [==============================] - 0s 202ms/step - loss: 1.0772 - accuracy: 0.9067\n","Epoch 33/123\n","1/1 [==============================] - 0s 178ms/step - loss: 1.0594 - accuracy: 0.9067\n","Epoch 34/123\n","1/1 [==============================] - 0s 164ms/step - loss: 1.0243 - accuracy: 0.9067\n","Epoch 35/123\n","1/1 [==============================] - 0s 152ms/step - loss: 1.0021 - accuracy: 0.9067\n","Epoch 36/123\n","1/1 [==============================] - 0s 155ms/step - loss: 0.9907 - accuracy: 0.9067\n","Epoch 37/123\n","1/1 [==============================] - 0s 116ms/step - loss: 0.9812 - accuracy: 0.9067\n","Epoch 38/123\n","1/1 [==============================] - 0s 177ms/step - loss: 0.9719 - accuracy: 0.9067\n","Epoch 39/123\n","1/1 [==============================] - 0s 149ms/step - loss: 0.9622 - accuracy: 0.9067\n","Epoch 40/123\n","1/1 [==============================] - 0s 138ms/step - loss: 0.9527 - accuracy: 0.9067\n","Epoch 41/123\n","1/1 [==============================] - 0s 185ms/step - loss: 0.9438 - accuracy: 0.9067\n","Epoch 42/123\n","1/1 [==============================] - 0s 149ms/step - loss: 0.9367 - accuracy: 0.9067\n","Epoch 43/123\n","1/1 [==============================] - 0s 144ms/step - loss: 0.9548 - accuracy: 0.9067\n","Epoch 44/123\n","1/1 [==============================] - 0s 131ms/step - loss: 0.9730 - accuracy: 0.9067\n","Epoch 45/123\n","1/1 [==============================] - 0s 139ms/step - loss: 0.9695 - accuracy: 0.9067\n","Epoch 46/123\n","1/1 [==============================] - 0s 137ms/step - loss: 0.9719 - accuracy: 0.9067\n","Epoch 47/123\n","1/1 [==============================] - 0s 173ms/step - loss: 0.9960 - accuracy: 0.9067\n","Epoch 48/123\n","1/1 [==============================] - 0s 207ms/step - loss: 0.9707 - accuracy: 0.9067\n","Epoch 49/123\n","1/1 [==============================] - 0s 127ms/step - loss: 0.9637 - accuracy: 0.9067\n","Epoch 50/123\n","1/1 [==============================] - 0s 133ms/step - loss: 0.9629 - accuracy: 0.9067\n","Epoch 51/123\n","1/1 [==============================] - 0s 201ms/step - loss: 0.9636 - accuracy: 0.9067\n","Epoch 52/123\n","1/1 [==============================] - 0s 162ms/step - loss: 0.9645 - accuracy: 0.9067\n","Epoch 53/123\n","1/1 [==============================] - 0s 167ms/step - loss: 0.9442 - accuracy: 0.9067\n","Epoch 54/123\n","1/1 [==============================] - 0s 188ms/step - loss: 0.9424 - accuracy: 0.9067\n","Epoch 55/123\n","1/1 [==============================] - 0s 216ms/step - loss: 0.9423 - accuracy: 0.9067\n","Epoch 56/123\n","1/1 [==============================] - 0s 232ms/step - loss: 0.9422 - accuracy: 0.9067\n","Epoch 57/123\n","1/1 [==============================] - 0s 208ms/step - loss: 0.9419 - accuracy: 0.9067\n","Epoch 58/123\n","1/1 [==============================] - 0s 241ms/step - loss: 0.9214 - accuracy: 0.9067\n","Epoch 59/123\n","1/1 [==============================] - 0s 225ms/step - loss: 0.9180 - accuracy: 0.9067\n","Epoch 60/123\n","1/1 [==============================] - 0s 146ms/step - loss: 0.9177 - accuracy: 0.9067\n","Epoch 61/123\n","1/1 [==============================] - 0s 165ms/step - loss: 0.9176 - accuracy: 0.9067\n","Epoch 62/123\n","1/1 [==============================] - 0s 151ms/step - loss: 0.9174 - accuracy: 0.9067\n","Epoch 63/123\n","1/1 [==============================] - 0s 159ms/step - loss: 0.9170 - accuracy: 0.9067\n","Epoch 64/123\n","1/1 [==============================] - 0s 178ms/step - loss: 0.9164 - accuracy: 0.9067\n","Epoch 65/123\n","1/1 [==============================] - 0s 181ms/step - loss: 0.9157 - accuracy: 0.9067\n","Epoch 66/123\n","1/1 [==============================] - 0s 203ms/step - loss: 0.9148 - accuracy: 0.9067\n","Epoch 67/123\n","1/1 [==============================] - 0s 186ms/step - loss: 0.9137 - accuracy: 0.9067\n","Epoch 68/123\n","1/1 [==============================] - 0s 188ms/step - loss: 0.9124 - accuracy: 0.9067\n","Epoch 69/123\n","1/1 [==============================] - 0s 193ms/step - loss: 0.9109 - accuracy: 0.9067\n","Epoch 70/123\n","1/1 [==============================] - 0s 179ms/step - loss: 0.9095 - accuracy: 0.9067\n","Epoch 71/123\n","1/1 [==============================] - 0s 143ms/step - loss: 0.9081 - accuracy: 0.9067\n","Epoch 72/123\n","1/1 [==============================] - 0s 136ms/step - loss: 0.9067 - accuracy: 0.9067\n","Epoch 73/123\n","1/1 [==============================] - 0s 153ms/step - loss: 0.9053 - accuracy: 0.9067\n","Epoch 74/123\n","1/1 [==============================] - 0s 184ms/step - loss: 0.9040 - accuracy: 0.9067\n","Epoch 75/123\n","1/1 [==============================] - 0s 167ms/step - loss: 0.9029 - accuracy: 0.9067\n","Epoch 76/123\n","1/1 [==============================] - 0s 156ms/step - loss: 0.9019 - accuracy: 0.9067\n","Epoch 77/123\n","1/1 [==============================] - 0s 115ms/step - loss: 0.9012 - accuracy: 0.9067\n","Epoch 78/123\n","1/1 [==============================] - 0s 126ms/step - loss: 0.9006 - accuracy: 0.9067\n","Epoch 79/123\n","1/1 [==============================] - 0s 154ms/step - loss: 0.9001 - accuracy: 0.9067\n","Epoch 80/123\n","1/1 [==============================] - 0s 148ms/step - loss: 0.8997 - accuracy: 0.9067\n","Epoch 81/123\n","1/1 [==============================] - 0s 151ms/step - loss: 0.8992 - accuracy: 0.9067\n","Epoch 82/123\n","1/1 [==============================] - 0s 177ms/step - loss: 0.8987 - accuracy: 0.9067\n","Epoch 83/123\n","1/1 [==============================] - 0s 187ms/step - loss: 0.8982 - accuracy: 0.9067\n","Epoch 84/123\n","1/1 [==============================] - 0s 194ms/step - loss: 0.8977 - accuracy: 0.9067\n","Epoch 85/123\n","1/1 [==============================] - 0s 163ms/step - loss: 0.8971 - accuracy: 0.9067\n","Epoch 86/123\n","1/1 [==============================] - 0s 177ms/step - loss: 0.8966 - accuracy: 0.9067\n","Epoch 87/123\n","1/1 [==============================] - 0s 192ms/step - loss: 0.8960 - accuracy: 0.9067\n","Epoch 88/123\n","1/1 [==============================] - 0s 201ms/step - loss: 0.8955 - accuracy: 0.9067\n","Epoch 89/123\n","1/1 [==============================] - 0s 180ms/step - loss: 0.8950 - accuracy: 0.9067\n","Epoch 90/123\n","1/1 [==============================] - 0s 179ms/step - loss: 0.8784 - accuracy: 0.9067\n","Epoch 91/123\n","1/1 [==============================] - 0s 204ms/step - loss: 0.8835 - accuracy: 0.9067\n","Epoch 92/123\n","1/1 [==============================] - 0s 139ms/step - loss: 0.9158 - accuracy: 0.9067\n","Epoch 93/123\n","1/1 [==============================] - 0s 144ms/step - loss: 0.9480 - accuracy: 0.9067\n","Epoch 94/123\n","1/1 [==============================] - 0s 169ms/step - loss: 0.9729 - accuracy: 0.9067\n","Epoch 95/123\n","1/1 [==============================] - 0s 172ms/step - loss: 0.9891 - accuracy: 0.9067\n","Epoch 96/123\n","1/1 [==============================] - 0s 171ms/step - loss: 0.9969 - accuracy: 0.9067\n","Epoch 97/123\n","1/1 [==============================] - 0s 168ms/step - loss: 0.9972 - accuracy: 0.9067\n","Epoch 98/123\n","1/1 [==============================] - 0s 165ms/step - loss: 0.9911 - accuracy: 0.9067\n","Epoch 99/123\n","1/1 [==============================] - 0s 248ms/step - loss: 0.9798 - accuracy: 0.9067\n","Epoch 100/123\n","1/1 [==============================] - 0s 127ms/step - loss: 0.9647 - accuracy: 0.9067\n","Epoch 101/123\n","1/1 [==============================] - 0s 123ms/step - loss: 0.9472 - accuracy: 0.9067\n","Epoch 102/123\n","1/1 [==============================] - 0s 173ms/step - loss: 0.9289 - accuracy: 0.9067\n","Epoch 103/123\n","1/1 [==============================] - 0s 165ms/step - loss: 0.9117 - accuracy: 0.9067\n","Epoch 104/123\n","1/1 [==============================] - 0s 176ms/step - loss: 0.8976 - accuracy: 0.9067\n","Epoch 105/123\n","1/1 [==============================] - 0s 168ms/step - loss: 0.8861 - accuracy: 0.9067\n","Epoch 106/123\n","1/1 [==============================] - 0s 158ms/step - loss: 0.8779 - accuracy: 0.9067\n","Epoch 107/123\n","1/1 [==============================] - 0s 167ms/step - loss: 0.8717 - accuracy: 0.9067\n","Epoch 108/123\n","1/1 [==============================] - 0s 157ms/step - loss: 0.8673 - accuracy: 0.9067\n","Epoch 109/123\n","1/1 [==============================] - 0s 178ms/step - loss: 0.8653 - accuracy: 0.9067\n","Epoch 110/123\n","1/1 [==============================] - 0s 155ms/step - loss: 0.8652 - accuracy: 0.9067\n","Epoch 111/123\n","1/1 [==============================] - 0s 154ms/step - loss: 0.8679 - accuracy: 0.9067\n","Epoch 112/123\n","1/1 [==============================] - 0s 172ms/step - loss: 0.8907 - accuracy: 0.9067\n","Epoch 113/123\n","1/1 [==============================] - 0s 176ms/step - loss: 0.8911 - accuracy: 0.9067\n","Epoch 114/123\n","1/1 [==============================] - 0s 179ms/step - loss: 0.8912 - accuracy: 0.9067\n","Epoch 115/123\n","1/1 [==============================] - 0s 168ms/step - loss: 0.8906 - accuracy: 0.9067\n","Epoch 116/123\n","1/1 [==============================] - 0s 168ms/step - loss: 0.8893 - accuracy: 0.9067\n","Epoch 117/123\n","1/1 [==============================] - 0s 160ms/step - loss: 0.8690 - accuracy: 0.9067\n","Epoch 118/123\n","1/1 [==============================] - 0s 151ms/step - loss: 0.8598 - accuracy: 0.9067\n","Epoch 119/123\n","1/1 [==============================] - 0s 169ms/step - loss: 0.8590 - accuracy: 0.9067\n","Epoch 120/123\n","1/1 [==============================] - 0s 129ms/step - loss: 0.8598 - accuracy: 0.9067\n","Epoch 121/123\n","1/1 [==============================] - 0s 179ms/step - loss: 0.8611 - accuracy: 0.9067\n","Epoch 122/123\n","1/1 [==============================] - 0s 190ms/step - loss: 0.8624 - accuracy: 0.9067\n","Epoch 123/123\n","1/1 [==============================] - 0s 175ms/step - loss: 0.8634 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b24c06b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.0472 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:02:06,178]\u001b[0m Trial 26 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 112, 'num_epochs': 123}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/103\n","1/1 [==============================] - 5s 5s/step - loss: 14.0329 - accuracy: 0.0079\n","Epoch 2/103\n","1/1 [==============================] - 0s 179ms/step - loss: 2.6585 - accuracy: 0.1270\n","Epoch 3/103\n","1/1 [==============================] - 0s 149ms/step - loss: 2.0353 - accuracy: 0.2877\n","Epoch 4/103\n","1/1 [==============================] - 0s 181ms/step - loss: 1.7210 - accuracy: 0.4286\n","Epoch 5/103\n","1/1 [==============================] - 0s 180ms/step - loss: 1.5222 - accuracy: 0.8829\n","Epoch 6/103\n","1/1 [==============================] - 0s 158ms/step - loss: 1.3779 - accuracy: 0.9067\n","Epoch 7/103\n","1/1 [==============================] - 0s 140ms/step - loss: 1.3242 - accuracy: 0.9067\n","Epoch 8/103\n","1/1 [==============================] - 0s 122ms/step - loss: 1.3005 - accuracy: 0.9067\n","Epoch 9/103\n","1/1 [==============================] - 0s 167ms/step - loss: 1.2653 - accuracy: 0.9067\n","Epoch 10/103\n","1/1 [==============================] - 0s 137ms/step - loss: 1.1904 - accuracy: 0.9067\n","Epoch 11/103\n","1/1 [==============================] - 0s 157ms/step - loss: 1.1602 - accuracy: 0.9067\n","Epoch 12/103\n","1/1 [==============================] - 0s 128ms/step - loss: 1.1141 - accuracy: 0.9067\n","Epoch 13/103\n","1/1 [==============================] - 0s 126ms/step - loss: 1.0691 - accuracy: 0.9067\n","Epoch 14/103\n","1/1 [==============================] - 0s 140ms/step - loss: 1.0318 - accuracy: 0.9067\n","Epoch 15/103\n","1/1 [==============================] - 0s 136ms/step - loss: 0.9376 - accuracy: 0.9067\n","Epoch 16/103\n","1/1 [==============================] - 0s 138ms/step - loss: 0.8579 - accuracy: 0.9067\n","Epoch 17/103\n","1/1 [==============================] - 0s 120ms/step - loss: 0.8198 - accuracy: 0.9067\n","Epoch 18/103\n","1/1 [==============================] - 0s 133ms/step - loss: 0.7847 - accuracy: 0.9067\n","Epoch 19/103\n","1/1 [==============================] - 0s 126ms/step - loss: 0.7464 - accuracy: 0.9067\n","Epoch 20/103\n","1/1 [==============================] - 0s 146ms/step - loss: 0.7255 - accuracy: 0.9067\n","Epoch 21/103\n","1/1 [==============================] - 0s 138ms/step - loss: 0.7109 - accuracy: 0.9067\n","Epoch 22/103\n","1/1 [==============================] - 0s 223ms/step - loss: 0.7024 - accuracy: 0.9067\n","Epoch 23/103\n","1/1 [==============================] - 0s 142ms/step - loss: 0.6995 - accuracy: 0.9067\n","Epoch 24/103\n","1/1 [==============================] - 0s 164ms/step - loss: 0.7040 - accuracy: 0.9067\n","Epoch 25/103\n","1/1 [==============================] - 0s 131ms/step - loss: 0.7033 - accuracy: 0.9067\n","Epoch 26/103\n","1/1 [==============================] - 0s 149ms/step - loss: 0.6816 - accuracy: 0.9067\n","Epoch 27/103\n","1/1 [==============================] - 0s 132ms/step - loss: 0.6849 - accuracy: 0.9067\n","Epoch 28/103\n","1/1 [==============================] - 0s 100ms/step - loss: 0.6988 - accuracy: 0.9067\n","Epoch 29/103\n","1/1 [==============================] - 0s 141ms/step - loss: 0.7123 - accuracy: 0.9067\n","Epoch 30/103\n","1/1 [==============================] - 0s 130ms/step - loss: 0.7228 - accuracy: 0.9067\n","Epoch 31/103\n","1/1 [==============================] - 0s 131ms/step - loss: 0.7295 - accuracy: 0.9067\n","Epoch 32/103\n","1/1 [==============================] - 0s 133ms/step - loss: 0.7324 - accuracy: 0.9067\n","Epoch 33/103\n","1/1 [==============================] - 0s 152ms/step - loss: 0.7320 - accuracy: 0.9067\n","Epoch 34/103\n","1/1 [==============================] - 0s 131ms/step - loss: 0.7279 - accuracy: 0.9067\n","Epoch 35/103\n","1/1 [==============================] - 0s 127ms/step - loss: 0.7203 - accuracy: 0.9067\n","Epoch 36/103\n","1/1 [==============================] - 0s 109ms/step - loss: 0.7096 - accuracy: 0.9067\n","Epoch 37/103\n","1/1 [==============================] - 0s 92ms/step - loss: 0.6974 - accuracy: 0.9067\n","Epoch 38/103\n","1/1 [==============================] - 0s 94ms/step - loss: 0.6847 - accuracy: 0.9067\n","Epoch 39/103\n","1/1 [==============================] - 0s 109ms/step - loss: 0.6728 - accuracy: 0.9067\n","Epoch 40/103\n","1/1 [==============================] - 0s 93ms/step - loss: 0.6625 - accuracy: 0.9067\n","Epoch 41/103\n","1/1 [==============================] - 0s 80ms/step - loss: 0.6540 - accuracy: 0.9067\n","Epoch 42/103\n","1/1 [==============================] - 0s 125ms/step - loss: 0.6477 - accuracy: 0.9067\n","Epoch 43/103\n","1/1 [==============================] - 0s 132ms/step - loss: 0.6440 - accuracy: 0.9067\n","Epoch 44/103\n","1/1 [==============================] - 0s 133ms/step - loss: 0.6414 - accuracy: 0.9067\n","Epoch 45/103\n","1/1 [==============================] - 0s 112ms/step - loss: 0.6400 - accuracy: 0.9067\n","Epoch 46/103\n","1/1 [==============================] - 0s 146ms/step - loss: 0.6397 - accuracy: 0.9067\n","Epoch 47/103\n","1/1 [==============================] - 0s 175ms/step - loss: 0.6397 - accuracy: 0.9067\n","Epoch 48/103\n","1/1 [==============================] - 0s 177ms/step - loss: 0.6399 - accuracy: 0.9067\n","Epoch 49/103\n","1/1 [==============================] - 0s 182ms/step - loss: 0.6403 - accuracy: 0.9067\n","Epoch 50/103\n","1/1 [==============================] - 0s 125ms/step - loss: 0.6398 - accuracy: 0.9067\n","Epoch 51/103\n","1/1 [==============================] - 0s 121ms/step - loss: 0.6385 - accuracy: 0.9067\n","Epoch 52/103\n","1/1 [==============================] - 0s 148ms/step - loss: 0.6372 - accuracy: 0.9067\n","Epoch 53/103\n","1/1 [==============================] - 0s 138ms/step - loss: 0.6359 - accuracy: 0.9067\n","Epoch 54/103\n","1/1 [==============================] - 0s 149ms/step - loss: 0.6347 - accuracy: 0.9067\n","Epoch 55/103\n","1/1 [==============================] - 0s 148ms/step - loss: 0.6335 - accuracy: 0.9067\n","Epoch 56/103\n","1/1 [==============================] - 0s 113ms/step - loss: 0.6323 - accuracy: 0.9067\n","Epoch 57/103\n","1/1 [==============================] - 0s 117ms/step - loss: 0.6312 - accuracy: 0.9067\n","Epoch 58/103\n","1/1 [==============================] - 0s 89ms/step - loss: 0.6301 - accuracy: 0.9067\n","Epoch 59/103\n","1/1 [==============================] - 0s 97ms/step - loss: 0.6291 - accuracy: 0.9067\n","Epoch 60/103\n","1/1 [==============================] - 0s 101ms/step - loss: 0.6282 - accuracy: 0.9067\n","Epoch 61/103\n","1/1 [==============================] - 0s 83ms/step - loss: 0.6273 - accuracy: 0.9067\n","Epoch 62/103\n","1/1 [==============================] - 0s 111ms/step - loss: 0.6265 - accuracy: 0.9067\n","Epoch 63/103\n","1/1 [==============================] - 0s 106ms/step - loss: 0.6257 - accuracy: 0.9067\n","Epoch 64/103\n","1/1 [==============================] - 0s 173ms/step - loss: 0.6251 - accuracy: 0.9067\n","Epoch 65/103\n","1/1 [==============================] - 0s 179ms/step - loss: 0.6245 - accuracy: 0.9067\n","Epoch 66/103\n","1/1 [==============================] - 0s 120ms/step - loss: 0.6240 - accuracy: 0.9067\n","Epoch 67/103\n","1/1 [==============================] - 0s 106ms/step - loss: 0.6236 - accuracy: 0.9067\n","Epoch 68/103\n","1/1 [==============================] - 0s 177ms/step - loss: 0.6231 - accuracy: 0.9067\n","Epoch 69/103\n","1/1 [==============================] - 0s 144ms/step - loss: 0.6227 - accuracy: 0.9067\n","Epoch 70/103\n","1/1 [==============================] - 0s 107ms/step - loss: 0.6222 - accuracy: 0.9067\n","Epoch 71/103\n","1/1 [==============================] - 0s 121ms/step - loss: 0.6218 - accuracy: 0.9067\n","Epoch 72/103\n","1/1 [==============================] - 0s 139ms/step - loss: 0.6213 - accuracy: 0.9067\n","Epoch 73/103\n","1/1 [==============================] - 0s 120ms/step - loss: 0.6208 - accuracy: 0.9067\n","Epoch 74/103\n","1/1 [==============================] - 0s 120ms/step - loss: 0.6202 - accuracy: 0.9067\n","Epoch 75/103\n","1/1 [==============================] - 0s 130ms/step - loss: 0.6197 - accuracy: 0.9067\n","Epoch 76/103\n","1/1 [==============================] - 0s 125ms/step - loss: 0.6192 - accuracy: 0.9067\n","Epoch 77/103\n","1/1 [==============================] - 0s 153ms/step - loss: 0.6187 - accuracy: 0.9067\n","Epoch 78/103\n","1/1 [==============================] - 0s 123ms/step - loss: 0.6182 - accuracy: 0.9067\n","Epoch 79/103\n","1/1 [==============================] - 0s 186ms/step - loss: 0.6177 - accuracy: 0.9067\n","Epoch 80/103\n","1/1 [==============================] - 0s 160ms/step - loss: 0.6172 - accuracy: 0.9067\n","Epoch 81/103\n","1/1 [==============================] - 0s 156ms/step - loss: 0.6167 - accuracy: 0.9067\n","Epoch 82/103\n","1/1 [==============================] - 0s 167ms/step - loss: 0.6162 - accuracy: 0.9067\n","Epoch 83/103\n","1/1 [==============================] - 0s 210ms/step - loss: 0.6157 - accuracy: 0.9067\n","Epoch 84/103\n","1/1 [==============================] - 0s 194ms/step - loss: 0.6152 - accuracy: 0.9067\n","Epoch 85/103\n","1/1 [==============================] - 0s 152ms/step - loss: 0.6147 - accuracy: 0.9067\n","Epoch 86/103\n","1/1 [==============================] - 0s 157ms/step - loss: 0.6142 - accuracy: 0.9067\n","Epoch 87/103\n","1/1 [==============================] - 0s 141ms/step - loss: 0.6138 - accuracy: 0.9067\n","Epoch 88/103\n","1/1 [==============================] - 0s 202ms/step - loss: 0.6133 - accuracy: 0.9067\n","Epoch 89/103\n","1/1 [==============================] - 0s 171ms/step - loss: 0.6128 - accuracy: 0.9067\n","Epoch 90/103\n","1/1 [==============================] - 0s 162ms/step - loss: 0.6123 - accuracy: 0.9067\n","Epoch 91/103\n","1/1 [==============================] - 0s 163ms/step - loss: 0.6118 - accuracy: 0.9067\n","Epoch 92/103\n","1/1 [==============================] - 0s 170ms/step - loss: 0.6114 - accuracy: 0.9067\n","Epoch 93/103\n","1/1 [==============================] - 0s 196ms/step - loss: 0.6109 - accuracy: 0.9067\n","Epoch 94/103\n","1/1 [==============================] - 0s 191ms/step - loss: 0.6105 - accuracy: 0.9067\n","Epoch 95/103\n","1/1 [==============================] - 0s 205ms/step - loss: 0.6100 - accuracy: 0.9067\n","Epoch 96/103\n","1/1 [==============================] - 0s 190ms/step - loss: 0.6096 - accuracy: 0.9067\n","Epoch 97/103\n","1/1 [==============================] - 0s 198ms/step - loss: 0.6091 - accuracy: 0.9067\n","Epoch 98/103\n","1/1 [==============================] - 0s 198ms/step - loss: 0.6086 - accuracy: 0.9067\n","Epoch 99/103\n","1/1 [==============================] - 0s 209ms/step - loss: 0.6081 - accuracy: 0.9067\n","Epoch 100/103\n","1/1 [==============================] - 0s 182ms/step - loss: 0.6077 - accuracy: 0.9067\n","Epoch 101/103\n","1/1 [==============================] - 0s 212ms/step - loss: 0.6072 - accuracy: 0.9067\n","Epoch 102/103\n","1/1 [==============================] - 0s 150ms/step - loss: 0.6067 - accuracy: 0.9067\n","Epoch 103/103\n","1/1 [==============================] - 0s 163ms/step - loss: 0.6062 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b12eccca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 2s 2s/step - loss: 1.1131 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:02:28,738]\u001b[0m Trial 27 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 17, 'num_epochs': 103}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/36\n","1/1 [==============================] - 5s 5s/step - loss: 4.2457 - accuracy: 0.0099\n","Epoch 2/36\n","1/1 [==============================] - 0s 101ms/step - loss: 2.0094 - accuracy: 0.8433\n","Epoch 3/36\n","1/1 [==============================] - 0s 134ms/step - loss: 1.6959 - accuracy: 0.8988\n","Epoch 4/36\n","1/1 [==============================] - 0s 158ms/step - loss: 1.4334 - accuracy: 0.9067\n","Epoch 5/36\n","1/1 [==============================] - 0s 160ms/step - loss: 1.3083 - accuracy: 0.9067\n","Epoch 6/36\n","1/1 [==============================] - 0s 158ms/step - loss: 1.1956 - accuracy: 0.9087\n","Epoch 7/36\n","1/1 [==============================] - 0s 159ms/step - loss: 1.1003 - accuracy: 0.9107\n","Epoch 8/36\n","1/1 [==============================] - 0s 185ms/step - loss: 1.0924 - accuracy: 0.9127\n","Epoch 9/36\n","1/1 [==============================] - 0s 169ms/step - loss: 1.0695 - accuracy: 0.9127\n","Epoch 10/36\n","1/1 [==============================] - 0s 180ms/step - loss: 1.0624 - accuracy: 0.9107\n","Epoch 11/36\n","1/1 [==============================] - 0s 169ms/step - loss: 0.9315 - accuracy: 0.9087\n","Epoch 12/36\n","1/1 [==============================] - 0s 162ms/step - loss: 0.9379 - accuracy: 0.9067\n","Epoch 13/36\n","1/1 [==============================] - 0s 163ms/step - loss: 0.9597 - accuracy: 0.9067\n","Epoch 14/36\n","1/1 [==============================] - 0s 133ms/step - loss: 0.9703 - accuracy: 0.9067\n","Epoch 15/36\n","1/1 [==============================] - 0s 187ms/step - loss: 0.9756 - accuracy: 0.9067\n","Epoch 16/36\n","1/1 [==============================] - 0s 217ms/step - loss: 0.9710 - accuracy: 0.9067\n","Epoch 17/36\n","1/1 [==============================] - 0s 185ms/step - loss: 0.9646 - accuracy: 0.9067\n","Epoch 18/36\n","1/1 [==============================] - 0s 138ms/step - loss: 0.9523 - accuracy: 0.9067\n","Epoch 19/36\n","1/1 [==============================] - 0s 367ms/step - loss: 0.9372 - accuracy: 0.9067\n","Epoch 20/36\n","1/1 [==============================] - 0s 111ms/step - loss: 0.9185 - accuracy: 0.9067\n","Epoch 21/36\n","1/1 [==============================] - 0s 143ms/step - loss: 0.8995 - accuracy: 0.9067\n","Epoch 22/36\n","1/1 [==============================] - 0s 132ms/step - loss: 0.8787 - accuracy: 0.9067\n","Epoch 23/36\n","1/1 [==============================] - 0s 126ms/step - loss: 0.8587 - accuracy: 0.9067\n","Epoch 24/36\n","1/1 [==============================] - 0s 119ms/step - loss: 0.8408 - accuracy: 0.9067\n","Epoch 25/36\n","1/1 [==============================] - 0s 215ms/step - loss: 0.8249 - accuracy: 0.9067\n","Epoch 26/36\n","1/1 [==============================] - 0s 101ms/step - loss: 0.8105 - accuracy: 0.9067\n","Epoch 27/36\n","1/1 [==============================] - 0s 125ms/step - loss: 0.7983 - accuracy: 0.9067\n","Epoch 28/36\n","1/1 [==============================] - 0s 142ms/step - loss: 0.7880 - accuracy: 0.9067\n","Epoch 29/36\n","1/1 [==============================] - 0s 126ms/step - loss: 0.7787 - accuracy: 0.9067\n","Epoch 30/36\n","1/1 [==============================] - 0s 103ms/step - loss: 0.7707 - accuracy: 0.9067\n","Epoch 31/36\n","1/1 [==============================] - 0s 139ms/step - loss: 0.7642 - accuracy: 0.9067\n","Epoch 32/36\n","1/1 [==============================] - 0s 142ms/step - loss: 0.7600 - accuracy: 0.9067\n","Epoch 33/36\n","1/1 [==============================] - 0s 152ms/step - loss: 0.7600 - accuracy: 0.9067\n","Epoch 34/36\n","1/1 [==============================] - 0s 119ms/step - loss: 0.7784 - accuracy: 0.9067\n","Epoch 35/36\n","1/1 [==============================] - 0s 99ms/step - loss: 0.7766 - accuracy: 0.9067\n","Epoch 36/36\n","1/1 [==============================] - 0s 137ms/step - loss: 0.7755 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b4c17f4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1717 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:02:41,287]\u001b[0m Trial 28 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 16, 'num_epochs': 36}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/124\n","1/1 [==============================] - 6s 6s/step - loss: 14.7216 - accuracy: 0.0218\n","Epoch 2/124\n","1/1 [==============================] - 0s 140ms/step - loss: 3.0500 - accuracy: 0.1627\n","Epoch 3/124\n","1/1 [==============================] - 0s 175ms/step - loss: 1.4161 - accuracy: 0.9067\n","Epoch 4/124\n","1/1 [==============================] - 0s 140ms/step - loss: 0.9344 - accuracy: 0.9067\n","Epoch 5/124\n","1/1 [==============================] - 0s 129ms/step - loss: 0.7862 - accuracy: 0.9067\n","Epoch 6/124\n","1/1 [==============================] - 0s 132ms/step - loss: 0.7176 - accuracy: 0.9067\n","Epoch 7/124\n","1/1 [==============================] - 0s 133ms/step - loss: 0.7164 - accuracy: 0.9067\n","Epoch 8/124\n","1/1 [==============================] - 0s 137ms/step - loss: 0.6809 - accuracy: 0.9067\n","Epoch 9/124\n","1/1 [==============================] - 0s 168ms/step - loss: 0.6693 - accuracy: 0.9067\n","Epoch 10/124\n","1/1 [==============================] - 0s 160ms/step - loss: 0.6399 - accuracy: 0.9067\n","Epoch 11/124\n","1/1 [==============================] - 0s 140ms/step - loss: 0.6329 - accuracy: 0.9067\n","Epoch 12/124\n","1/1 [==============================] - 0s 120ms/step - loss: 0.6251 - accuracy: 0.9067\n","Epoch 13/124\n","1/1 [==============================] - 0s 179ms/step - loss: 0.6166 - accuracy: 0.9067\n","Epoch 14/124\n","1/1 [==============================] - 0s 121ms/step - loss: 0.6092 - accuracy: 0.9067\n","Epoch 15/124\n","1/1 [==============================] - 0s 86ms/step - loss: 0.6053 - accuracy: 0.9067\n","Epoch 16/124\n","1/1 [==============================] - 0s 84ms/step - loss: 0.6059 - accuracy: 0.9067\n","Epoch 17/124\n","1/1 [==============================] - 0s 139ms/step - loss: 0.6095 - accuracy: 0.9067\n","Epoch 18/124\n","1/1 [==============================] - 0s 134ms/step - loss: 0.6110 - accuracy: 0.9067\n","Epoch 19/124\n","1/1 [==============================] - 0s 123ms/step - loss: 0.6065 - accuracy: 0.9067\n","Epoch 20/124\n","1/1 [==============================] - 0s 120ms/step - loss: 0.6013 - accuracy: 0.9067\n","Epoch 21/124\n","1/1 [==============================] - 0s 114ms/step - loss: 0.5971 - accuracy: 0.9067\n","Epoch 22/124\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5945 - accuracy: 0.9067\n","Epoch 23/124\n","1/1 [==============================] - 0s 127ms/step - loss: 0.5932 - accuracy: 0.9067\n","Epoch 24/124\n","1/1 [==============================] - 0s 125ms/step - loss: 0.5929 - accuracy: 0.9067\n","Epoch 25/124\n","1/1 [==============================] - 0s 121ms/step - loss: 0.5927 - accuracy: 0.9067\n","Epoch 26/124\n","1/1 [==============================] - 0s 122ms/step - loss: 0.5920 - accuracy: 0.9067\n","Epoch 27/124\n","1/1 [==============================] - 0s 105ms/step - loss: 0.5909 - accuracy: 0.9067\n","Epoch 28/124\n","1/1 [==============================] - 0s 110ms/step - loss: 0.5894 - accuracy: 0.9067\n","Epoch 29/124\n","1/1 [==============================] - 0s 130ms/step - loss: 0.5877 - accuracy: 0.9067\n","Epoch 30/124\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5861 - accuracy: 0.9067\n","Epoch 31/124\n","1/1 [==============================] - 0s 198ms/step - loss: 0.5848 - accuracy: 0.9067\n","Epoch 32/124\n","1/1 [==============================] - 0s 207ms/step - loss: 0.5838 - accuracy: 0.9067\n","Epoch 33/124\n","1/1 [==============================] - 0s 169ms/step - loss: 0.5829 - accuracy: 0.9067\n","Epoch 34/124\n","1/1 [==============================] - 0s 171ms/step - loss: 0.5819 - accuracy: 0.9067\n","Epoch 35/124\n","1/1 [==============================] - 0s 158ms/step - loss: 0.5806 - accuracy: 0.9067\n","Epoch 36/124\n","1/1 [==============================] - 0s 146ms/step - loss: 0.5792 - accuracy: 0.9067\n","Epoch 37/124\n","1/1 [==============================] - 0s 175ms/step - loss: 0.5779 - accuracy: 0.9067\n","Epoch 38/124\n","1/1 [==============================] - 0s 142ms/step - loss: 0.5767 - accuracy: 0.9067\n","Epoch 39/124\n","1/1 [==============================] - 0s 169ms/step - loss: 0.5758 - accuracy: 0.9067\n","Epoch 40/124\n","1/1 [==============================] - 0s 167ms/step - loss: 0.5750 - accuracy: 0.9067\n","Epoch 41/124\n","1/1 [==============================] - 0s 173ms/step - loss: 0.5742 - accuracy: 0.9067\n","Epoch 42/124\n","1/1 [==============================] - 0s 176ms/step - loss: 0.5734 - accuracy: 0.9067\n","Epoch 43/124\n","1/1 [==============================] - 0s 147ms/step - loss: 0.5724 - accuracy: 0.9067\n","Epoch 44/124\n","1/1 [==============================] - 0s 159ms/step - loss: 0.5713 - accuracy: 0.9067\n","Epoch 45/124\n","1/1 [==============================] - 0s 172ms/step - loss: 0.5702 - accuracy: 0.9067\n","Epoch 46/124\n","1/1 [==============================] - 0s 178ms/step - loss: 0.5690 - accuracy: 0.9067\n","Epoch 47/124\n","1/1 [==============================] - 0s 167ms/step - loss: 0.5679 - accuracy: 0.9067\n","Epoch 48/124\n","1/1 [==============================] - 0s 162ms/step - loss: 0.5669 - accuracy: 0.9067\n","Epoch 49/124\n","1/1 [==============================] - 0s 168ms/step - loss: 0.5660 - accuracy: 0.9067\n","Epoch 50/124\n","1/1 [==============================] - 0s 161ms/step - loss: 0.5651 - accuracy: 0.9067\n","Epoch 51/124\n","1/1 [==============================] - 0s 148ms/step - loss: 0.5642 - accuracy: 0.9087\n","Epoch 52/124\n","1/1 [==============================] - 0s 155ms/step - loss: 0.5633 - accuracy: 0.9087\n","Epoch 53/124\n","1/1 [==============================] - 0s 141ms/step - loss: 0.5624 - accuracy: 0.9087\n","Epoch 54/124\n","1/1 [==============================] - 0s 251ms/step - loss: 0.5614 - accuracy: 0.9087\n","Epoch 55/124\n","1/1 [==============================] - 0s 148ms/step - loss: 0.5604 - accuracy: 0.9107\n","Epoch 56/124\n","1/1 [==============================] - 0s 130ms/step - loss: 0.5594 - accuracy: 0.9107\n","Epoch 57/124\n","1/1 [==============================] - 0s 148ms/step - loss: 0.5585 - accuracy: 0.9107\n","Epoch 58/124\n","1/1 [==============================] - 0s 122ms/step - loss: 0.5576 - accuracy: 0.9127\n","Epoch 59/124\n","1/1 [==============================] - 0s 122ms/step - loss: 0.5566 - accuracy: 0.9127\n","Epoch 60/124\n","1/1 [==============================] - 0s 163ms/step - loss: 0.5557 - accuracy: 0.9127\n","Epoch 61/124\n","1/1 [==============================] - 0s 171ms/step - loss: 0.5548 - accuracy: 0.9147\n","Epoch 62/124\n","1/1 [==============================] - 0s 148ms/step - loss: 0.5539 - accuracy: 0.9147\n","Epoch 63/124\n","1/1 [==============================] - 0s 145ms/step - loss: 0.5530 - accuracy: 0.9147\n","Epoch 64/124\n","1/1 [==============================] - 0s 160ms/step - loss: 0.5520 - accuracy: 0.9147\n","Epoch 65/124\n","1/1 [==============================] - 0s 165ms/step - loss: 0.5510 - accuracy: 0.9147\n","Epoch 66/124\n","1/1 [==============================] - 0s 178ms/step - loss: 0.5500 - accuracy: 0.9167\n","Epoch 67/124\n","1/1 [==============================] - 0s 172ms/step - loss: 0.5491 - accuracy: 0.9206\n","Epoch 68/124\n","1/1 [==============================] - 0s 177ms/step - loss: 0.5481 - accuracy: 0.9187\n","Epoch 69/124\n","1/1 [==============================] - 0s 149ms/step - loss: 0.5471 - accuracy: 0.9187\n","Epoch 70/124\n","1/1 [==============================] - 0s 145ms/step - loss: 0.5461 - accuracy: 0.9187\n","Epoch 71/124\n","1/1 [==============================] - 0s 128ms/step - loss: 0.5450 - accuracy: 0.9187\n","Epoch 72/124\n","1/1 [==============================] - 0s 126ms/step - loss: 0.5440 - accuracy: 0.9167\n","Epoch 73/124\n","1/1 [==============================] - 0s 149ms/step - loss: 0.5429 - accuracy: 0.9167\n","Epoch 74/124\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5418 - accuracy: 0.9167\n","Epoch 75/124\n","1/1 [==============================] - 0s 128ms/step - loss: 0.5407 - accuracy: 0.9187\n","Epoch 76/124\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5396 - accuracy: 0.9187\n","Epoch 77/124\n","1/1 [==============================] - 0s 123ms/step - loss: 0.5386 - accuracy: 0.9187\n","Epoch 78/124\n","1/1 [==============================] - 0s 125ms/step - loss: 0.5375 - accuracy: 0.9187\n","Epoch 79/124\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5363 - accuracy: 0.9167\n","Epoch 80/124\n","1/1 [==============================] - 0s 125ms/step - loss: 0.5351 - accuracy: 0.9167\n","Epoch 81/124\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5339 - accuracy: 0.9167\n","Epoch 82/124\n","1/1 [==============================] - 0s 131ms/step - loss: 0.5326 - accuracy: 0.9187\n","Epoch 83/124\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5313 - accuracy: 0.9187\n","Epoch 84/124\n","1/1 [==============================] - 0s 126ms/step - loss: 0.5298 - accuracy: 0.9187\n","Epoch 85/124\n","1/1 [==============================] - 0s 116ms/step - loss: 0.5285 - accuracy: 0.9187\n","Epoch 86/124\n","1/1 [==============================] - 0s 133ms/step - loss: 0.5270 - accuracy: 0.9187\n","Epoch 87/124\n","1/1 [==============================] - 0s 129ms/step - loss: 0.5057 - accuracy: 0.9226\n","Epoch 88/124\n","1/1 [==============================] - 0s 135ms/step - loss: 0.4185 - accuracy: 0.9246\n","Epoch 89/124\n","1/1 [==============================] - 0s 187ms/step - loss: 0.4572 - accuracy: 0.9266\n","Epoch 90/124\n","1/1 [==============================] - 0s 172ms/step - loss: 0.4847 - accuracy: 0.9226\n","Epoch 91/124\n","1/1 [==============================] - 0s 190ms/step - loss: 0.4646 - accuracy: 0.9246\n","Epoch 92/124\n","1/1 [==============================] - 0s 204ms/step - loss: 0.4244 - accuracy: 0.9246\n","Epoch 93/124\n","1/1 [==============================] - 0s 160ms/step - loss: 0.4110 - accuracy: 0.9246\n","Epoch 94/124\n","1/1 [==============================] - 0s 166ms/step - loss: 0.4133 - accuracy: 0.9246\n","Epoch 95/124\n","1/1 [==============================] - 0s 167ms/step - loss: 0.4638 - accuracy: 0.9246\n","Epoch 96/124\n","1/1 [==============================] - 0s 177ms/step - loss: 0.4629 - accuracy: 0.9246\n","Epoch 97/124\n","1/1 [==============================] - 0s 145ms/step - loss: 0.4618 - accuracy: 0.9246\n","Epoch 98/124\n","1/1 [==============================] - 0s 149ms/step - loss: 0.4607 - accuracy: 0.9226\n","Epoch 99/124\n","1/1 [==============================] - 0s 144ms/step - loss: 0.4593 - accuracy: 0.9226\n","Epoch 100/124\n","1/1 [==============================] - 0s 150ms/step - loss: 0.4577 - accuracy: 0.9246\n","Epoch 101/124\n","1/1 [==============================] - 0s 162ms/step - loss: 0.4557 - accuracy: 0.9246\n","Epoch 102/124\n","1/1 [==============================] - 0s 171ms/step - loss: 0.4532 - accuracy: 0.9246\n","Epoch 103/124\n","1/1 [==============================] - 0s 159ms/step - loss: 0.4507 - accuracy: 0.9246\n","Epoch 104/124\n","1/1 [==============================] - 0s 277ms/step - loss: 0.4491 - accuracy: 0.9286\n","Epoch 105/124\n","1/1 [==============================] - 0s 128ms/step - loss: 0.4474 - accuracy: 0.9266\n","Epoch 106/124\n","1/1 [==============================] - 0s 173ms/step - loss: 0.4427 - accuracy: 0.9286\n","Epoch 107/124\n","1/1 [==============================] - 0s 163ms/step - loss: 0.4389 - accuracy: 0.9345\n","Epoch 108/124\n","1/1 [==============================] - 0s 153ms/step - loss: 0.4367 - accuracy: 0.9325\n","Epoch 109/124\n","1/1 [==============================] - 0s 141ms/step - loss: 0.4364 - accuracy: 0.9325\n","Epoch 110/124\n","1/1 [==============================] - 0s 163ms/step - loss: nan - accuracy: 0.9325\n","Epoch 111/124\n","1/1 [==============================] - 0s 133ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/124\n","1/1 [==============================] - 0s 179ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/124\n","1/1 [==============================] - 0s 224ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/124\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/124\n","1/1 [==============================] - 0s 204ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/124\n","1/1 [==============================] - 0s 170ms/step - loss: nan - accuracy: 0.9067\n","Epoch 117/124\n","1/1 [==============================] - 0s 215ms/step - loss: nan - accuracy: 0.9067\n","Epoch 118/124\n","1/1 [==============================] - 0s 173ms/step - loss: nan - accuracy: 0.9067\n","Epoch 119/124\n","1/1 [==============================] - 0s 166ms/step - loss: nan - accuracy: 0.9067\n","Epoch 120/124\n","1/1 [==============================] - 0s 150ms/step - loss: nan - accuracy: 0.9067\n","Epoch 121/124\n","1/1 [==============================] - 0s 138ms/step - loss: nan - accuracy: 0.9067\n","Epoch 122/124\n","1/1 [==============================] - 0s 141ms/step - loss: nan - accuracy: 0.9067\n","Epoch 123/124\n","1/1 [==============================] - 0s 185ms/step - loss: nan - accuracy: 0.9067\n","Epoch 124/124\n","1/1 [==============================] - 0s 170ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b10402ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:03:08,445]\u001b[0m Trial 29 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 93, 'num_epochs': 124}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","1/1 [==============================] - 5s 5s/step - loss: 11.0410 - accuracy: 0.0099\n","Epoch 2/100\n","1/1 [==============================] - 0s 112ms/step - loss: 2.1803 - accuracy: 0.9067\n","Epoch 3/100\n","1/1 [==============================] - 0s 150ms/step - loss: 1.9715 - accuracy: 0.9067\n","Epoch 4/100\n","1/1 [==============================] - 0s 161ms/step - loss: 1.8735 - accuracy: 0.9067\n","Epoch 5/100\n","1/1 [==============================] - 0s 151ms/step - loss: 1.8104 - accuracy: 0.9067\n","Epoch 6/100\n","1/1 [==============================] - 0s 239ms/step - loss: 1.7340 - accuracy: 0.9067\n","Epoch 7/100\n","1/1 [==============================] - 0s 168ms/step - loss: 1.6771 - accuracy: 0.9067\n","Epoch 8/100\n","1/1 [==============================] - 0s 174ms/step - loss: 1.6226 - accuracy: 0.9067\n","Epoch 9/100\n","1/1 [==============================] - 0s 183ms/step - loss: 1.5665 - accuracy: 0.9067\n","Epoch 10/100\n","1/1 [==============================] - 0s 196ms/step - loss: 1.5084 - accuracy: 0.9067\n","Epoch 11/100\n","1/1 [==============================] - 0s 205ms/step - loss: 1.4493 - accuracy: 0.9067\n","Epoch 12/100\n","1/1 [==============================] - 0s 150ms/step - loss: 1.3899 - accuracy: 0.9067\n","Epoch 13/100\n","1/1 [==============================] - 0s 210ms/step - loss: 1.3367 - accuracy: 0.9067\n","Epoch 14/100\n","1/1 [==============================] - 0s 158ms/step - loss: 1.3075 - accuracy: 0.9067\n","Epoch 15/100\n","1/1 [==============================] - 0s 201ms/step - loss: 1.2737 - accuracy: 0.9067\n","Epoch 16/100\n","1/1 [==============================] - 0s 167ms/step - loss: 1.2465 - accuracy: 0.9067\n","Epoch 17/100\n","1/1 [==============================] - 0s 235ms/step - loss: 1.2246 - accuracy: 0.9067\n","Epoch 18/100\n","1/1 [==============================] - 0s 228ms/step - loss: 1.2075 - accuracy: 0.9067\n","Epoch 19/100\n","1/1 [==============================] - 0s 152ms/step - loss: 1.1940 - accuracy: 0.9067\n","Epoch 20/100\n","1/1 [==============================] - 0s 167ms/step - loss: 1.1845 - accuracy: 0.9067\n","Epoch 21/100\n","1/1 [==============================] - 0s 157ms/step - loss: 1.1804 - accuracy: 0.9067\n","Epoch 22/100\n","1/1 [==============================] - 0s 134ms/step - loss: 1.1776 - accuracy: 0.9067\n","Epoch 23/100\n","1/1 [==============================] - 0s 126ms/step - loss: 1.1749 - accuracy: 0.9067\n","Epoch 24/100\n","1/1 [==============================] - 0s 124ms/step - loss: 1.1728 - accuracy: 0.9067\n","Epoch 25/100\n","1/1 [==============================] - 0s 133ms/step - loss: 1.1709 - accuracy: 0.9067\n","Epoch 26/100\n","1/1 [==============================] - 0s 135ms/step - loss: 1.1691 - accuracy: 0.9067\n","Epoch 27/100\n","1/1 [==============================] - 0s 126ms/step - loss: 1.1675 - accuracy: 0.9067\n","Epoch 28/100\n","1/1 [==============================] - 0s 134ms/step - loss: 1.1662 - accuracy: 0.9067\n","Epoch 29/100\n","1/1 [==============================] - 0s 123ms/step - loss: 1.1653 - accuracy: 0.9067\n","Epoch 30/100\n","1/1 [==============================] - 0s 159ms/step - loss: 1.1647 - accuracy: 0.9067\n","Epoch 31/100\n","1/1 [==============================] - 0s 136ms/step - loss: 1.1641 - accuracy: 0.9067\n","Epoch 32/100\n","1/1 [==============================] - 0s 126ms/step - loss: 1.1635 - accuracy: 0.9067\n","Epoch 33/100\n","1/1 [==============================] - 0s 153ms/step - loss: 1.1629 - accuracy: 0.9067\n","Epoch 34/100\n","1/1 [==============================] - 0s 158ms/step - loss: 1.1624 - accuracy: 0.9067\n","Epoch 35/100\n","1/1 [==============================] - 0s 137ms/step - loss: 1.1620 - accuracy: 0.9067\n","Epoch 36/100\n","1/1 [==============================] - 0s 151ms/step - loss: 1.1615 - accuracy: 0.9067\n","Epoch 37/100\n","1/1 [==============================] - 0s 151ms/step - loss: 1.1609 - accuracy: 0.9067\n","Epoch 38/100\n","1/1 [==============================] - 0s 155ms/step - loss: 1.1603 - accuracy: 0.9067\n","Epoch 39/100\n","1/1 [==============================] - 0s 146ms/step - loss: 1.1597 - accuracy: 0.9067\n","Epoch 40/100\n","1/1 [==============================] - 0s 158ms/step - loss: 1.1591 - accuracy: 0.9067\n","Epoch 41/100\n","1/1 [==============================] - 0s 130ms/step - loss: 1.1586 - accuracy: 0.9067\n","Epoch 42/100\n","1/1 [==============================] - 0s 115ms/step - loss: 1.1580 - accuracy: 0.9067\n","Epoch 43/100\n","1/1 [==============================] - 0s 132ms/step - loss: 1.1576 - accuracy: 0.9067\n","Epoch 44/100\n","1/1 [==============================] - 0s 144ms/step - loss: 1.1572 - accuracy: 0.9067\n","Epoch 45/100\n","1/1 [==============================] - 0s 150ms/step - loss: 1.1568 - accuracy: 0.9067\n","Epoch 46/100\n","1/1 [==============================] - 0s 155ms/step - loss: 1.1564 - accuracy: 0.9067\n","Epoch 47/100\n","1/1 [==============================] - 0s 147ms/step - loss: 1.1561 - accuracy: 0.9067\n","Epoch 48/100\n","1/1 [==============================] - 0s 138ms/step - loss: 1.1558 - accuracy: 0.9067\n","Epoch 49/100\n","1/1 [==============================] - 0s 199ms/step - loss: 1.1554 - accuracy: 0.9067\n","Epoch 50/100\n","1/1 [==============================] - 0s 102ms/step - loss: 1.1551 - accuracy: 0.9067\n","Epoch 51/100\n","1/1 [==============================] - 0s 135ms/step - loss: 1.1548 - accuracy: 0.9067\n","Epoch 52/100\n","1/1 [==============================] - 0s 119ms/step - loss: 1.1545 - accuracy: 0.9067\n","Epoch 53/100\n","1/1 [==============================] - 0s 132ms/step - loss: 1.1542 - accuracy: 0.9067\n","Epoch 54/100\n","1/1 [==============================] - 0s 142ms/step - loss: 1.1539 - accuracy: 0.9067\n","Epoch 55/100\n","1/1 [==============================] - 0s 123ms/step - loss: 1.1536 - accuracy: 0.9067\n","Epoch 56/100\n","1/1 [==============================] - 0s 118ms/step - loss: 1.1533 - accuracy: 0.9067\n","Epoch 57/100\n","1/1 [==============================] - 0s 105ms/step - loss: 1.1531 - accuracy: 0.9067\n","Epoch 58/100\n","1/1 [==============================] - 0s 126ms/step - loss: 1.1528 - accuracy: 0.9067\n","Epoch 59/100\n","1/1 [==============================] - 0s 127ms/step - loss: 1.1525 - accuracy: 0.9067\n","Epoch 60/100\n","1/1 [==============================] - 0s 131ms/step - loss: 1.1522 - accuracy: 0.9067\n","Epoch 61/100\n","1/1 [==============================] - 0s 116ms/step - loss: 1.1520 - accuracy: 0.9067\n","Epoch 62/100\n","1/1 [==============================] - 0s 127ms/step - loss: 1.1517 - accuracy: 0.9067\n","Epoch 63/100\n","1/1 [==============================] - 0s 134ms/step - loss: 1.1515 - accuracy: 0.9067\n","Epoch 64/100\n","1/1 [==============================] - 0s 119ms/step - loss: 1.1512 - accuracy: 0.9067\n","Epoch 65/100\n","1/1 [==============================] - 0s 127ms/step - loss: 1.1510 - accuracy: 0.9067\n","Epoch 66/100\n","1/1 [==============================] - 0s 121ms/step - loss: 1.1508 - accuracy: 0.9067\n","Epoch 67/100\n","1/1 [==============================] - 0s 131ms/step - loss: 1.1506 - accuracy: 0.9067\n","Epoch 68/100\n","1/1 [==============================] - 0s 117ms/step - loss: 1.1504 - accuracy: 0.9067\n","Epoch 69/100\n","1/1 [==============================] - 0s 127ms/step - loss: 1.1501 - accuracy: 0.9067\n","Epoch 70/100\n","1/1 [==============================] - 0s 120ms/step - loss: 1.1499 - accuracy: 0.9067\n","Epoch 71/100\n","1/1 [==============================] - 0s 127ms/step - loss: 1.1497 - accuracy: 0.9067\n","Epoch 72/100\n","1/1 [==============================] - 0s 111ms/step - loss: 1.1495 - accuracy: 0.9067\n","Epoch 73/100\n","1/1 [==============================] - 0s 136ms/step - loss: 1.1493 - accuracy: 0.9067\n","Epoch 74/100\n","1/1 [==============================] - 0s 167ms/step - loss: 1.1491 - accuracy: 0.9067\n","Epoch 75/100\n","1/1 [==============================] - 0s 182ms/step - loss: 1.1490 - accuracy: 0.9067\n","Epoch 76/100\n","1/1 [==============================] - 0s 152ms/step - loss: 1.1488 - accuracy: 0.9067\n","Epoch 77/100\n","1/1 [==============================] - 0s 125ms/step - loss: 1.1486 - accuracy: 0.9067\n","Epoch 78/100\n","1/1 [==============================] - 0s 124ms/step - loss: 1.1484 - accuracy: 0.9067\n","Epoch 79/100\n","1/1 [==============================] - 0s 138ms/step - loss: 1.1482 - accuracy: 0.9067\n","Epoch 80/100\n","1/1 [==============================] - 0s 147ms/step - loss: 1.1481 - accuracy: 0.9067\n","Epoch 81/100\n","1/1 [==============================] - 0s 143ms/step - loss: 1.1479 - accuracy: 0.9067\n","Epoch 82/100\n","1/1 [==============================] - 0s 116ms/step - loss: 1.1477 - accuracy: 0.9067\n","Epoch 83/100\n","1/1 [==============================] - 0s 137ms/step - loss: 1.1475 - accuracy: 0.9067\n","Epoch 84/100\n","1/1 [==============================] - 0s 120ms/step - loss: 1.1474 - accuracy: 0.9067\n","Epoch 85/100\n","1/1 [==============================] - 0s 162ms/step - loss: 1.1472 - accuracy: 0.9067\n","Epoch 86/100\n","1/1 [==============================] - 0s 133ms/step - loss: 1.1470 - accuracy: 0.9067\n","Epoch 87/100\n","1/1 [==============================] - 0s 228ms/step - loss: 1.1469 - accuracy: 0.9067\n","Epoch 88/100\n","1/1 [==============================] - 0s 161ms/step - loss: 1.1467 - accuracy: 0.9067\n","Epoch 89/100\n","1/1 [==============================] - 0s 130ms/step - loss: 1.1465 - accuracy: 0.9067\n","Epoch 90/100\n","1/1 [==============================] - 0s 141ms/step - loss: 1.1464 - accuracy: 0.9067\n","Epoch 91/100\n","1/1 [==============================] - 0s 140ms/step - loss: 1.1462 - accuracy: 0.9067\n","Epoch 92/100\n","1/1 [==============================] - 0s 137ms/step - loss: 1.1461 - accuracy: 0.9067\n","Epoch 93/100\n","1/1 [==============================] - 0s 129ms/step - loss: 1.1459 - accuracy: 0.9067\n","Epoch 94/100\n","1/1 [==============================] - 0s 130ms/step - loss: 1.1458 - accuracy: 0.9067\n","Epoch 95/100\n","1/1 [==============================] - 0s 164ms/step - loss: 1.1456 - accuracy: 0.9067\n","Epoch 96/100\n","1/1 [==============================] - 0s 183ms/step - loss: 1.1455 - accuracy: 0.9067\n","Epoch 97/100\n","1/1 [==============================] - 0s 132ms/step - loss: 1.1453 - accuracy: 0.9067\n","Epoch 98/100\n","1/1 [==============================] - 0s 132ms/step - loss: 1.1452 - accuracy: 0.9067\n","Epoch 99/100\n","1/1 [==============================] - 0s 136ms/step - loss: 1.1450 - accuracy: 0.9067\n","Epoch 100/100\n","1/1 [==============================] - 0s 115ms/step - loss: 1.1449 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b1c5b6ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1692 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:03:30,438]\u001b[0m Trial 30 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 44, 'num_epochs': 100}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/123\n","1/1 [==============================] - 6s 6s/step - loss: 7.7598 - accuracy: 0.1032\n","Epoch 2/123\n","1/1 [==============================] - 0s 178ms/step - loss: 2.3318 - accuracy: 0.5437\n","Epoch 3/123\n","1/1 [==============================] - 0s 231ms/step - loss: 1.9330 - accuracy: 0.9067\n","Epoch 4/123\n","1/1 [==============================] - 0s 195ms/step - loss: 1.7103 - accuracy: 0.9067\n","Epoch 5/123\n","1/1 [==============================] - 0s 193ms/step - loss: 1.5601 - accuracy: 0.9067\n","Epoch 6/123\n","1/1 [==============================] - 0s 198ms/step - loss: 1.4642 - accuracy: 0.9067\n","Epoch 7/123\n","1/1 [==============================] - 0s 165ms/step - loss: 1.4128 - accuracy: 0.9067\n","Epoch 8/123\n","1/1 [==============================] - 0s 181ms/step - loss: 1.3518 - accuracy: 0.9067\n","Epoch 9/123\n","1/1 [==============================] - 0s 174ms/step - loss: 1.3119 - accuracy: 0.9067\n","Epoch 10/123\n","1/1 [==============================] - 0s 184ms/step - loss: 1.2570 - accuracy: 0.9067\n","Epoch 11/123\n","1/1 [==============================] - 0s 171ms/step - loss: 1.2003 - accuracy: 0.9067\n","Epoch 12/123\n","1/1 [==============================] - 0s 187ms/step - loss: 1.1245 - accuracy: 0.9067\n","Epoch 13/123\n","1/1 [==============================] - 0s 185ms/step - loss: 0.9761 - accuracy: 0.9067\n","Epoch 14/123\n","1/1 [==============================] - 0s 181ms/step - loss: 0.9379 - accuracy: 0.9067\n","Epoch 15/123\n","1/1 [==============================] - 0s 134ms/step - loss: 0.9245 - accuracy: 0.9067\n","Epoch 16/123\n","1/1 [==============================] - 0s 154ms/step - loss: 0.9214 - accuracy: 0.9067\n","Epoch 17/123\n","1/1 [==============================] - 0s 127ms/step - loss: 0.9186 - accuracy: 0.9067\n","Epoch 18/123\n","1/1 [==============================] - 0s 127ms/step - loss: 0.9126 - accuracy: 0.9067\n","Epoch 19/123\n","1/1 [==============================] - 0s 147ms/step - loss: 0.9042 - accuracy: 0.9067\n","Epoch 20/123\n","1/1 [==============================] - 0s 122ms/step - loss: 0.8958 - accuracy: 0.9067\n","Epoch 21/123\n","1/1 [==============================] - 0s 149ms/step - loss: 0.8884 - accuracy: 0.9067\n","Epoch 22/123\n","1/1 [==============================] - 0s 127ms/step - loss: 0.8809 - accuracy: 0.9067\n","Epoch 23/123\n","1/1 [==============================] - 0s 124ms/step - loss: 0.8720 - accuracy: 0.9067\n","Epoch 24/123\n","1/1 [==============================] - 0s 122ms/step - loss: 0.8618 - accuracy: 0.9067\n","Epoch 25/123\n","1/1 [==============================] - 0s 140ms/step - loss: 0.8525 - accuracy: 0.9067\n","Epoch 26/123\n","1/1 [==============================] - 0s 113ms/step - loss: 0.8462 - accuracy: 0.9067\n","Epoch 27/123\n","1/1 [==============================] - 0s 120ms/step - loss: 0.8430 - accuracy: 0.9067\n","Epoch 28/123\n","1/1 [==============================] - 0s 71ms/step - loss: 0.8416 - accuracy: 0.9067\n","Epoch 29/123\n","1/1 [==============================] - 0s 118ms/step - loss: 0.8411 - accuracy: 0.9067\n","Epoch 30/123\n","1/1 [==============================] - 0s 125ms/step - loss: 0.8412 - accuracy: 0.9067\n","Epoch 31/123\n","1/1 [==============================] - 0s 112ms/step - loss: 0.8416 - accuracy: 0.9067\n","Epoch 32/123\n","1/1 [==============================] - 0s 116ms/step - loss: 0.8419 - accuracy: 0.9067\n","Epoch 33/123\n","1/1 [==============================] - 0s 129ms/step - loss: 0.8420 - accuracy: 0.9067\n","Epoch 34/123\n","1/1 [==============================] - 0s 139ms/step - loss: 0.8417 - accuracy: 0.9067\n","Epoch 35/123\n","1/1 [==============================] - 0s 137ms/step - loss: 0.8409 - accuracy: 0.9067\n","Epoch 36/123\n","1/1 [==============================] - 0s 139ms/step - loss: 0.8397 - accuracy: 0.9067\n","Epoch 37/123\n","1/1 [==============================] - 0s 144ms/step - loss: 0.8383 - accuracy: 0.9067\n","Epoch 38/123\n","1/1 [==============================] - 0s 102ms/step - loss: 0.8368 - accuracy: 0.9067\n","Epoch 39/123\n","1/1 [==============================] - 0s 79ms/step - loss: 0.8352 - accuracy: 0.9067\n","Epoch 40/123\n","1/1 [==============================] - 0s 101ms/step - loss: 0.8337 - accuracy: 0.9067\n","Epoch 41/123\n","1/1 [==============================] - 0s 101ms/step - loss: 0.8323 - accuracy: 0.9067\n","Epoch 42/123\n","1/1 [==============================] - 0s 85ms/step - loss: 0.8311 - accuracy: 0.9067\n","Epoch 43/123\n","1/1 [==============================] - 0s 107ms/step - loss: 0.8300 - accuracy: 0.9067\n","Epoch 44/123\n","1/1 [==============================] - 0s 105ms/step - loss: 0.8291 - accuracy: 0.9067\n","Epoch 45/123\n","1/1 [==============================] - 0s 85ms/step - loss: 0.8283 - accuracy: 0.9067\n","Epoch 46/123\n","1/1 [==============================] - 0s 260ms/step - loss: 0.8276 - accuracy: 0.9067\n","Epoch 47/123\n","1/1 [==============================] - 0s 116ms/step - loss: 0.8270 - accuracy: 0.9067\n","Epoch 48/123\n","1/1 [==============================] - 0s 125ms/step - loss: 0.8265 - accuracy: 0.9067\n","Epoch 49/123\n","1/1 [==============================] - 0s 139ms/step - loss: 0.8260 - accuracy: 0.9067\n","Epoch 50/123\n","1/1 [==============================] - 0s 129ms/step - loss: 0.8256 - accuracy: 0.9067\n","Epoch 51/123\n","1/1 [==============================] - 0s 108ms/step - loss: 0.8251 - accuracy: 0.9067\n","Epoch 52/123\n","1/1 [==============================] - 0s 92ms/step - loss: 0.8247 - accuracy: 0.9067\n","Epoch 53/123\n","1/1 [==============================] - 0s 89ms/step - loss: 0.8242 - accuracy: 0.9067\n","Epoch 54/123\n","1/1 [==============================] - 0s 125ms/step - loss: 0.8237 - accuracy: 0.9067\n","Epoch 55/123\n","1/1 [==============================] - 0s 82ms/step - loss: 0.8232 - accuracy: 0.9067\n","Epoch 56/123\n","1/1 [==============================] - 0s 150ms/step - loss: 0.8227 - accuracy: 0.9067\n","Epoch 57/123\n","1/1 [==============================] - 0s 133ms/step - loss: 0.8222 - accuracy: 0.9067\n","Epoch 58/123\n","1/1 [==============================] - 0s 127ms/step - loss: 0.8218 - accuracy: 0.9067\n","Epoch 59/123\n","1/1 [==============================] - 0s 113ms/step - loss: 0.8213 - accuracy: 0.9067\n","Epoch 60/123\n","1/1 [==============================] - 0s 123ms/step - loss: 0.8208 - accuracy: 0.9067\n","Epoch 61/123\n","1/1 [==============================] - 0s 121ms/step - loss: 0.8203 - accuracy: 0.9067\n","Epoch 62/123\n","1/1 [==============================] - 0s 104ms/step - loss: 0.8199 - accuracy: 0.9067\n","Epoch 63/123\n","1/1 [==============================] - 0s 108ms/step - loss: 0.8195 - accuracy: 0.9067\n","Epoch 64/123\n","1/1 [==============================] - 0s 76ms/step - loss: 0.8190 - accuracy: 0.9067\n","Epoch 65/123\n","1/1 [==============================] - 0s 79ms/step - loss: 0.8186 - accuracy: 0.9067\n","Epoch 66/123\n","1/1 [==============================] - 0s 84ms/step - loss: 0.8181 - accuracy: 0.9067\n","Epoch 67/123\n","1/1 [==============================] - 0s 118ms/step - loss: 0.8177 - accuracy: 0.9067\n","Epoch 68/123\n","1/1 [==============================] - 0s 137ms/step - loss: 0.8173 - accuracy: 0.9067\n","Epoch 69/123\n","1/1 [==============================] - 0s 139ms/step - loss: 0.8169 - accuracy: 0.9067\n","Epoch 70/123\n","1/1 [==============================] - 0s 135ms/step - loss: 0.8166 - accuracy: 0.9067\n","Epoch 71/123\n","1/1 [==============================] - 0s 109ms/step - loss: 0.8162 - accuracy: 0.9067\n","Epoch 72/123\n","1/1 [==============================] - 0s 162ms/step - loss: 0.8159 - accuracy: 0.9067\n","Epoch 73/123\n","1/1 [==============================] - 0s 169ms/step - loss: 0.8155 - accuracy: 0.9067\n","Epoch 74/123\n","1/1 [==============================] - 0s 149ms/step - loss: 0.8152 - accuracy: 0.9067\n","Epoch 75/123\n","1/1 [==============================] - 0s 134ms/step - loss: 0.8148 - accuracy: 0.9067\n","Epoch 76/123\n","1/1 [==============================] - 0s 140ms/step - loss: 0.8145 - accuracy: 0.9067\n","Epoch 77/123\n","1/1 [==============================] - 0s 154ms/step - loss: 0.8141 - accuracy: 0.9067\n","Epoch 78/123\n","1/1 [==============================] - 0s 181ms/step - loss: 0.8138 - accuracy: 0.9067\n","Epoch 79/123\n","1/1 [==============================] - 0s 191ms/step - loss: 0.8134 - accuracy: 0.9067\n","Epoch 80/123\n","1/1 [==============================] - 0s 174ms/step - loss: 0.8131 - accuracy: 0.9067\n","Epoch 81/123\n","1/1 [==============================] - 0s 184ms/step - loss: 0.8128 - accuracy: 0.9067\n","Epoch 82/123\n","1/1 [==============================] - 0s 154ms/step - loss: 0.8125 - accuracy: 0.9067\n","Epoch 83/123\n","1/1 [==============================] - 0s 163ms/step - loss: 0.8121 - accuracy: 0.9067\n","Epoch 84/123\n","1/1 [==============================] - 0s 153ms/step - loss: 0.8118 - accuracy: 0.9067\n","Epoch 85/123\n","1/1 [==============================] - 0s 149ms/step - loss: 0.8115 - accuracy: 0.9067\n","Epoch 86/123\n","1/1 [==============================] - 0s 203ms/step - loss: 0.8112 - accuracy: 0.9067\n","Epoch 87/123\n","1/1 [==============================] - 0s 112ms/step - loss: 0.8109 - accuracy: 0.9067\n","Epoch 88/123\n","1/1 [==============================] - 0s 143ms/step - loss: 0.8106 - accuracy: 0.9067\n","Epoch 89/123\n","1/1 [==============================] - 0s 179ms/step - loss: 0.8104 - accuracy: 0.9067\n","Epoch 90/123\n","1/1 [==============================] - 0s 150ms/step - loss: 0.8101 - accuracy: 0.9067\n","Epoch 91/123\n","1/1 [==============================] - 0s 193ms/step - loss: 0.8098 - accuracy: 0.9067\n","Epoch 92/123\n","1/1 [==============================] - 0s 201ms/step - loss: 0.8095 - accuracy: 0.9067\n","Epoch 93/123\n","1/1 [==============================] - 0s 135ms/step - loss: 0.8093 - accuracy: 0.9067\n","Epoch 94/123\n","1/1 [==============================] - 0s 147ms/step - loss: 0.8090 - accuracy: 0.9067\n","Epoch 95/123\n","1/1 [==============================] - 0s 112ms/step - loss: 0.8087 - accuracy: 0.9067\n","Epoch 96/123\n","1/1 [==============================] - 0s 171ms/step - loss: 0.8085 - accuracy: 0.9067\n","Epoch 97/123\n","1/1 [==============================] - 0s 196ms/step - loss: 0.8082 - accuracy: 0.9067\n","Epoch 98/123\n","1/1 [==============================] - 0s 148ms/step - loss: 0.8080 - accuracy: 0.9067\n","Epoch 99/123\n","1/1 [==============================] - 0s 128ms/step - loss: 0.8077 - accuracy: 0.9067\n","Epoch 100/123\n","1/1 [==============================] - 0s 142ms/step - loss: 0.8075 - accuracy: 0.9067\n","Epoch 101/123\n","1/1 [==============================] - 0s 143ms/step - loss: 0.8072 - accuracy: 0.9067\n","Epoch 102/123\n","1/1 [==============================] - 0s 155ms/step - loss: 0.8069 - accuracy: 0.9067\n","Epoch 103/123\n","1/1 [==============================] - 0s 128ms/step - loss: 0.8067 - accuracy: 0.9067\n","Epoch 104/123\n","1/1 [==============================] - 0s 163ms/step - loss: 0.8064 - accuracy: 0.9067\n","Epoch 105/123\n","1/1 [==============================] - 0s 132ms/step - loss: 0.8062 - accuracy: 0.9067\n","Epoch 106/123\n","1/1 [==============================] - 0s 209ms/step - loss: 0.8060 - accuracy: 0.9067\n","Epoch 107/123\n","1/1 [==============================] - 0s 146ms/step - loss: 0.8057 - accuracy: 0.9067\n","Epoch 108/123\n","1/1 [==============================] - 0s 138ms/step - loss: 0.8055 - accuracy: 0.9067\n","Epoch 109/123\n","1/1 [==============================] - 0s 132ms/step - loss: 0.8052 - accuracy: 0.9067\n","Epoch 110/123\n","1/1 [==============================] - 0s 129ms/step - loss: 0.8050 - accuracy: 0.9067\n","Epoch 111/123\n","1/1 [==============================] - 0s 159ms/step - loss: 0.8048 - accuracy: 0.9067\n","Epoch 112/123\n","1/1 [==============================] - 0s 147ms/step - loss: 0.8045 - accuracy: 0.9067\n","Epoch 113/123\n","1/1 [==============================] - 0s 135ms/step - loss: 0.8043 - accuracy: 0.9067\n","Epoch 114/123\n","1/1 [==============================] - 0s 121ms/step - loss: 0.8041 - accuracy: 0.9067\n","Epoch 115/123\n","1/1 [==============================] - 0s 137ms/step - loss: 0.8038 - accuracy: 0.9067\n","Epoch 116/123\n","1/1 [==============================] - 0s 126ms/step - loss: 0.8036 - accuracy: 0.9067\n","Epoch 117/123\n","1/1 [==============================] - 0s 133ms/step - loss: 0.8033 - accuracy: 0.9067\n","Epoch 118/123\n","1/1 [==============================] - 0s 166ms/step - loss: 0.8031 - accuracy: 0.9067\n","Epoch 119/123\n","1/1 [==============================] - 0s 126ms/step - loss: 0.8029 - accuracy: 0.9067\n","Epoch 120/123\n","1/1 [==============================] - 0s 135ms/step - loss: 0.8027 - accuracy: 0.9067\n","Epoch 121/123\n","1/1 [==============================] - 0s 149ms/step - loss: 0.8024 - accuracy: 0.9067\n","Epoch 122/123\n","1/1 [==============================] - 0s 137ms/step - loss: 0.8022 - accuracy: 0.9067\n","Epoch 123/123\n","1/1 [==============================] - 0s 144ms/step - loss: 0.8020 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b244b5280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1070 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:03:56,011]\u001b[0m Trial 31 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 16, 'num_epochs': 123}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/32\n","1/1 [==============================] - 5s 5s/step - loss: 14.5588 - accuracy: 0.0000e+00\n","Epoch 2/32\n","1/1 [==============================] - 0s 143ms/step - loss: 10.1296 - accuracy: 0.0377\n","Epoch 3/32\n","1/1 [==============================] - 0s 146ms/step - loss: 2.0048 - accuracy: 0.8512\n","Epoch 4/32\n","1/1 [==============================] - 0s 157ms/step - loss: 1.6384 - accuracy: 0.9087\n","Epoch 5/32\n","1/1 [==============================] - 0s 149ms/step - loss: 1.3782 - accuracy: 0.9067\n","Epoch 6/32\n","1/1 [==============================] - 0s 148ms/step - loss: 1.1936 - accuracy: 0.9067\n","Epoch 7/32\n","1/1 [==============================] - 0s 134ms/step - loss: 1.0848 - accuracy: 0.9067\n","Epoch 8/32\n","1/1 [==============================] - 0s 138ms/step - loss: 0.9953 - accuracy: 0.9067\n","Epoch 9/32\n","1/1 [==============================] - 0s 141ms/step - loss: 0.9167 - accuracy: 0.9067\n","Epoch 10/32\n","1/1 [==============================] - 0s 128ms/step - loss: 0.8481 - accuracy: 0.9067\n","Epoch 11/32\n","1/1 [==============================] - 0s 94ms/step - loss: 0.7878 - accuracy: 0.9067\n","Epoch 12/32\n","1/1 [==============================] - 0s 94ms/step - loss: 0.7361 - accuracy: 0.9067\n","Epoch 13/32\n","1/1 [==============================] - 0s 108ms/step - loss: 0.6909 - accuracy: 0.9067\n","Epoch 14/32\n","1/1 [==============================] - 0s 81ms/step - loss: 0.6708 - accuracy: 0.9067\n","Epoch 15/32\n","1/1 [==============================] - 0s 115ms/step - loss: 0.6319 - accuracy: 0.9067\n","Epoch 16/32\n","1/1 [==============================] - 0s 121ms/step - loss: 0.6191 - accuracy: 0.9067\n","Epoch 17/32\n","1/1 [==============================] - 0s 113ms/step - loss: 0.5877 - accuracy: 0.9067\n","Epoch 18/32\n","1/1 [==============================] - 0s 114ms/step - loss: 0.5827 - accuracy: 0.9067\n","Epoch 19/32\n","1/1 [==============================] - 0s 229ms/step - loss: 0.5801 - accuracy: 0.9067\n","Epoch 20/32\n","1/1 [==============================] - 0s 135ms/step - loss: 0.5693 - accuracy: 0.9067\n","Epoch 21/32\n","1/1 [==============================] - 0s 138ms/step - loss: 0.5682 - accuracy: 0.9067\n","Epoch 22/32\n","1/1 [==============================] - 0s 135ms/step - loss: 0.5705 - accuracy: 0.9067\n","Epoch 23/32\n","1/1 [==============================] - 0s 120ms/step - loss: 0.5649 - accuracy: 0.9067\n","Epoch 24/32\n","1/1 [==============================] - 0s 122ms/step - loss: 0.5669 - accuracy: 0.9067\n","Epoch 25/32\n","1/1 [==============================] - 0s 114ms/step - loss: 0.5538 - accuracy: 0.9067\n","Epoch 26/32\n","1/1 [==============================] - 0s 88ms/step - loss: 0.5498 - accuracy: 0.9067\n","Epoch 27/32\n","1/1 [==============================] - 0s 112ms/step - loss: 0.5589 - accuracy: 0.9067\n","Epoch 28/32\n","1/1 [==============================] - 0s 115ms/step - loss: 0.5657 - accuracy: 0.9067\n","Epoch 29/32\n","1/1 [==============================] - 0s 113ms/step - loss: 0.5694 - accuracy: 0.9067\n","Epoch 30/32\n","1/1 [==============================] - 0s 104ms/step - loss: 0.5708 - accuracy: 0.9067\n","Epoch 31/32\n","1/1 [==============================] - 0s 99ms/step - loss: 0.5708 - accuracy: 0.9067\n","Epoch 32/32\n","1/1 [==============================] - 0s 120ms/step - loss: 0.5711 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b277cab80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1128 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:04:07,110]\u001b[0m Trial 32 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 60, 'num_epochs': 32}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/33\n","1/1 [==============================] - 6s 6s/step - loss: 9.0086 - accuracy: 0.1131\n","Epoch 2/33\n","1/1 [==============================] - 0s 114ms/step - loss: 1.8774 - accuracy: 0.7183\n","Epoch 3/33\n","1/1 [==============================] - 0s 135ms/step - loss: 1.4097 - accuracy: 0.9067\n","Epoch 4/33\n","1/1 [==============================] - 0s 135ms/step - loss: 1.2108 - accuracy: 0.9067\n","Epoch 5/33\n","1/1 [==============================] - 0s 148ms/step - loss: 1.0753 - accuracy: 0.9067\n","Epoch 6/33\n","1/1 [==============================] - 0s 145ms/step - loss: 0.9639 - accuracy: 0.9067\n","Epoch 7/33\n","1/1 [==============================] - 0s 146ms/step - loss: 0.8288 - accuracy: 0.9067\n","Epoch 8/33\n","1/1 [==============================] - 0s 173ms/step - loss: 0.7635 - accuracy: 0.9067\n","Epoch 9/33\n","1/1 [==============================] - 0s 157ms/step - loss: 0.8066 - accuracy: 0.9067\n","Epoch 10/33\n","1/1 [==============================] - 0s 158ms/step - loss: 0.8651 - accuracy: 0.9067\n","Epoch 11/33\n","1/1 [==============================] - 0s 147ms/step - loss: 0.8603 - accuracy: 0.9067\n","Epoch 12/33\n","1/1 [==============================] - 0s 119ms/step - loss: 0.8489 - accuracy: 0.9067\n","Epoch 13/33\n","1/1 [==============================] - 0s 114ms/step - loss: 0.8176 - accuracy: 0.9067\n","Epoch 14/33\n","1/1 [==============================] - 0s 151ms/step - loss: 0.7621 - accuracy: 0.9067\n","Epoch 15/33\n","1/1 [==============================] - 0s 152ms/step - loss: 0.7910 - accuracy: 0.9067\n","Epoch 16/33\n","1/1 [==============================] - 0s 160ms/step - loss: 0.7556 - accuracy: 0.9067\n","Epoch 17/33\n","1/1 [==============================] - 0s 118ms/step - loss: 0.7637 - accuracy: 0.9067\n","Epoch 18/33\n","1/1 [==============================] - 0s 117ms/step - loss: 0.7756 - accuracy: 0.9067\n","Epoch 19/33\n","1/1 [==============================] - 0s 146ms/step - loss: 0.7874 - accuracy: 0.9067\n","Epoch 20/33\n","1/1 [==============================] - 0s 154ms/step - loss: 0.7974 - accuracy: 0.9067\n","Epoch 21/33\n","1/1 [==============================] - 0s 141ms/step - loss: 0.8047 - accuracy: 0.9067\n","Epoch 22/33\n","1/1 [==============================] - 0s 135ms/step - loss: 0.8085 - accuracy: 0.9067\n","Epoch 23/33\n","1/1 [==============================] - 0s 139ms/step - loss: 0.8092 - accuracy: 0.9067\n","Epoch 24/33\n","1/1 [==============================] - 0s 150ms/step - loss: 0.8074 - accuracy: 0.9067\n","Epoch 25/33\n","1/1 [==============================] - 0s 106ms/step - loss: 0.8038 - accuracy: 0.9067\n","Epoch 26/33\n","1/1 [==============================] - 0s 80ms/step - loss: 0.7990 - accuracy: 0.9067\n","Epoch 27/33\n","1/1 [==============================] - 0s 123ms/step - loss: 0.7931 - accuracy: 0.9067\n","Epoch 28/33\n","1/1 [==============================] - 0s 102ms/step - loss: 0.7863 - accuracy: 0.9067\n","Epoch 29/33\n","1/1 [==============================] - 0s 99ms/step - loss: 0.7789 - accuracy: 0.9067\n","Epoch 30/33\n","1/1 [==============================] - 0s 97ms/step - loss: 0.7709 - accuracy: 0.9067\n","Epoch 31/33\n","1/1 [==============================] - 0s 115ms/step - loss: 0.7627 - accuracy: 0.9067\n","Epoch 32/33\n","1/1 [==============================] - 0s 107ms/step - loss: 0.7545 - accuracy: 0.9067\n","Epoch 33/33\n","1/1 [==============================] - 0s 114ms/step - loss: 0.7465 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b4c7deb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1408 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:04:19,201]\u001b[0m Trial 33 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 41, 'num_epochs': 33}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/123\n","1/1 [==============================] - 5s 5s/step - loss: 14.1255 - accuracy: 0.0238\n","Epoch 2/123\n","1/1 [==============================] - 0s 106ms/step - loss: 2.4707 - accuracy: 0.2341\n","Epoch 3/123\n","1/1 [==============================] - 0s 125ms/step - loss: 1.7284 - accuracy: 0.9067\n","Epoch 4/123\n","1/1 [==============================] - 0s 179ms/step - loss: 1.4614 - accuracy: 0.9067\n","Epoch 5/123\n","1/1 [==============================] - 0s 162ms/step - loss: 1.2790 - accuracy: 0.9067\n","Epoch 6/123\n","1/1 [==============================] - 0s 160ms/step - loss: 1.1930 - accuracy: 0.9067\n","Epoch 7/123\n","1/1 [==============================] - 0s 148ms/step - loss: 1.3169 - accuracy: 0.9067\n","Epoch 8/123\n","1/1 [==============================] - 0s 141ms/step - loss: 1.2950 - accuracy: 0.9067\n","Epoch 9/123\n","1/1 [==============================] - 0s 136ms/step - loss: 1.0572 - accuracy: 0.9067\n","Epoch 10/123\n","1/1 [==============================] - 0s 139ms/step - loss: 1.0894 - accuracy: 0.9067\n","Epoch 11/123\n","1/1 [==============================] - 0s 152ms/step - loss: 1.1162 - accuracy: 0.9067\n","Epoch 12/123\n","1/1 [==============================] - 0s 138ms/step - loss: 1.1155 - accuracy: 0.9067\n","Epoch 13/123\n","1/1 [==============================] - 0s 136ms/step - loss: 1.0948 - accuracy: 0.9067\n","Epoch 14/123\n","1/1 [==============================] - 0s 135ms/step - loss: 1.0751 - accuracy: 0.9067\n","Epoch 15/123\n","1/1 [==============================] - 0s 120ms/step - loss: 0.9512 - accuracy: 0.9067\n","Epoch 16/123\n","1/1 [==============================] - 0s 105ms/step - loss: 0.9388 - accuracy: 0.9067\n","Epoch 17/123\n","1/1 [==============================] - 0s 83ms/step - loss: 0.9378 - accuracy: 0.9067\n","Epoch 18/123\n","1/1 [==============================] - 0s 128ms/step - loss: 0.9309 - accuracy: 0.9067\n","Epoch 19/123\n","1/1 [==============================] - 0s 133ms/step - loss: 0.9123 - accuracy: 0.9067\n","Epoch 20/123\n","1/1 [==============================] - 0s 124ms/step - loss: 0.8902 - accuracy: 0.9067\n","Epoch 21/123\n","1/1 [==============================] - 0s 126ms/step - loss: 0.8690 - accuracy: 0.9067\n","Epoch 22/123\n","1/1 [==============================] - 0s 120ms/step - loss: 0.8497 - accuracy: 0.9067\n","Epoch 23/123\n","1/1 [==============================] - 0s 105ms/step - loss: 0.8334 - accuracy: 0.9067\n","Epoch 24/123\n","1/1 [==============================] - 0s 108ms/step - loss: 0.8216 - accuracy: 0.9067\n","Epoch 25/123\n","1/1 [==============================] - 0s 88ms/step - loss: 0.8148 - accuracy: 0.9067\n","Epoch 26/123\n","1/1 [==============================] - 0s 94ms/step - loss: 0.8116 - accuracy: 0.9067\n","Epoch 27/123\n","1/1 [==============================] - 0s 118ms/step - loss: 0.8110 - accuracy: 0.9067\n","Epoch 28/123\n","1/1 [==============================] - 0s 108ms/step - loss: 0.8131 - accuracy: 0.9067\n","Epoch 29/123\n","1/1 [==============================] - 0s 118ms/step - loss: 0.8388 - accuracy: 0.9067\n","Epoch 30/123\n","1/1 [==============================] - 0s 112ms/step - loss: 0.8603 - accuracy: 0.9067\n","Epoch 31/123\n","1/1 [==============================] - 0s 102ms/step - loss: 0.8603 - accuracy: 0.9067\n","Epoch 32/123\n","1/1 [==============================] - 0s 160ms/step - loss: 0.8598 - accuracy: 0.9067\n","Epoch 33/123\n","1/1 [==============================] - 0s 177ms/step - loss: 0.8587 - accuracy: 0.9067\n","Epoch 34/123\n","1/1 [==============================] - 0s 163ms/step - loss: 0.8569 - accuracy: 0.9067\n","Epoch 35/123\n","1/1 [==============================] - 0s 173ms/step - loss: 0.8548 - accuracy: 0.9067\n","Epoch 36/123\n","1/1 [==============================] - 0s 151ms/step - loss: 0.8527 - accuracy: 0.9067\n","Epoch 37/123\n","1/1 [==============================] - 0s 162ms/step - loss: 0.8509 - accuracy: 0.9067\n","Epoch 38/123\n","1/1 [==============================] - 0s 145ms/step - loss: 0.8091 - accuracy: 0.9067\n","Epoch 39/123\n","1/1 [==============================] - 0s 142ms/step - loss: 0.7983 - accuracy: 0.9067\n","Epoch 40/123\n","1/1 [==============================] - 0s 137ms/step - loss: 0.7968 - accuracy: 0.9067\n","Epoch 41/123\n","1/1 [==============================] - 0s 154ms/step - loss: 0.7973 - accuracy: 0.9067\n","Epoch 42/123\n","1/1 [==============================] - 0s 220ms/step - loss: 0.7983 - accuracy: 0.9067\n","Epoch 43/123\n","1/1 [==============================] - 0s 110ms/step - loss: 0.7996 - accuracy: 0.9067\n","Epoch 44/123\n","1/1 [==============================] - 0s 148ms/step - loss: 0.8007 - accuracy: 0.9067\n","Epoch 45/123\n","1/1 [==============================] - 0s 156ms/step - loss: 0.8016 - accuracy: 0.9067\n","Epoch 46/123\n","1/1 [==============================] - 0s 145ms/step - loss: 0.8020 - accuracy: 0.9067\n","Epoch 47/123\n","1/1 [==============================] - 0s 135ms/step - loss: 0.8020 - accuracy: 0.9067\n","Epoch 48/123\n","1/1 [==============================] - 0s 156ms/step - loss: 0.8015 - accuracy: 0.9067\n","Epoch 49/123\n","1/1 [==============================] - 0s 135ms/step - loss: 0.8005 - accuracy: 0.9067\n","Epoch 50/123\n","1/1 [==============================] - 0s 135ms/step - loss: 0.7992 - accuracy: 0.9067\n","Epoch 51/123\n","1/1 [==============================] - 0s 110ms/step - loss: 0.7976 - accuracy: 0.9067\n","Epoch 52/123\n","1/1 [==============================] - 0s 140ms/step - loss: 0.7959 - accuracy: 0.9067\n","Epoch 53/123\n","1/1 [==============================] - 0s 137ms/step - loss: 0.7941 - accuracy: 0.9067\n","Epoch 54/123\n","1/1 [==============================] - 0s 171ms/step - loss: 0.7923 - accuracy: 0.9067\n","Epoch 55/123\n","1/1 [==============================] - 0s 137ms/step - loss: 0.7906 - accuracy: 0.9067\n","Epoch 56/123\n","1/1 [==============================] - 0s 183ms/step - loss: 0.7889 - accuracy: 0.9067\n","Epoch 57/123\n","1/1 [==============================] - 0s 197ms/step - loss: 0.7876 - accuracy: 0.9067\n","Epoch 58/123\n","1/1 [==============================] - 0s 192ms/step - loss: 0.7864 - accuracy: 0.9067\n","Epoch 59/123\n","1/1 [==============================] - 0s 183ms/step - loss: 0.7854 - accuracy: 0.9067\n","Epoch 60/123\n","1/1 [==============================] - 0s 205ms/step - loss: 0.7846 - accuracy: 0.9067\n","Epoch 61/123\n","1/1 [==============================] - 0s 155ms/step - loss: 0.7841 - accuracy: 0.9067\n","Epoch 62/123\n","1/1 [==============================] - 0s 137ms/step - loss: 0.7838 - accuracy: 0.9067\n","Epoch 63/123\n","1/1 [==============================] - 0s 110ms/step - loss: 0.7836 - accuracy: 0.9067\n","Epoch 64/123\n","1/1 [==============================] - 0s 89ms/step - loss: 0.7834 - accuracy: 0.9067\n","Epoch 65/123\n","1/1 [==============================] - 0s 103ms/step - loss: 0.7831 - accuracy: 0.9067\n","Epoch 66/123\n","1/1 [==============================] - 0s 114ms/step - loss: 0.7826 - accuracy: 0.9087\n","Epoch 67/123\n","1/1 [==============================] - 0s 109ms/step - loss: 0.7820 - accuracy: 0.9087\n","Epoch 68/123\n","1/1 [==============================] - 0s 138ms/step - loss: 0.7813 - accuracy: 0.9087\n","Epoch 69/123\n","1/1 [==============================] - 0s 112ms/step - loss: 0.7805 - accuracy: 0.9087\n","Epoch 70/123\n","1/1 [==============================] - 0s 105ms/step - loss: 0.7796 - accuracy: 0.9087\n","Epoch 71/123\n","1/1 [==============================] - 0s 79ms/step - loss: 0.7787 - accuracy: 0.9087\n","Epoch 72/123\n","1/1 [==============================] - 0s 107ms/step - loss: 0.7779 - accuracy: 0.9087\n","Epoch 73/123\n","1/1 [==============================] - 0s 113ms/step - loss: 0.7772 - accuracy: 0.9107\n","Epoch 74/123\n","1/1 [==============================] - 0s 95ms/step - loss: 0.7766 - accuracy: 0.9107\n","Epoch 75/123\n","1/1 [==============================] - 0s 101ms/step - loss: 0.7761 - accuracy: 0.9107\n","Epoch 76/123\n","1/1 [==============================] - 0s 153ms/step - loss: 0.7755 - accuracy: 0.9107\n","Epoch 77/123\n","1/1 [==============================] - 0s 189ms/step - loss: 0.7750 - accuracy: 0.9107\n","Epoch 78/123\n","1/1 [==============================] - 0s 174ms/step - loss: 0.7745 - accuracy: 0.9107\n","Epoch 79/123\n","1/1 [==============================] - 0s 124ms/step - loss: 0.7740 - accuracy: 0.9107\n","Epoch 80/123\n","1/1 [==============================] - 0s 160ms/step - loss: 0.7734 - accuracy: 0.9127\n","Epoch 81/123\n","1/1 [==============================] - 0s 217ms/step - loss: 0.7729 - accuracy: 0.9127\n","Epoch 82/123\n","1/1 [==============================] - 0s 128ms/step - loss: 0.7723 - accuracy: 0.9127\n","Epoch 83/123\n","1/1 [==============================] - 0s 146ms/step - loss: 0.7717 - accuracy: 0.9127\n","Epoch 84/123\n","1/1 [==============================] - 0s 142ms/step - loss: 0.7711 - accuracy: 0.9147\n","Epoch 85/123\n","1/1 [==============================] - 0s 142ms/step - loss: 0.7705 - accuracy: 0.9127\n","Epoch 86/123\n","1/1 [==============================] - 0s 153ms/step - loss: 0.7700 - accuracy: 0.9127\n","Epoch 87/123\n","1/1 [==============================] - 0s 155ms/step - loss: 0.7694 - accuracy: 0.9127\n","Epoch 88/123\n","1/1 [==============================] - 0s 161ms/step - loss: 0.7689 - accuracy: 0.9167\n","Epoch 89/123\n","1/1 [==============================] - 0s 206ms/step - loss: 0.7683 - accuracy: 0.9167\n","Epoch 90/123\n","1/1 [==============================] - 0s 193ms/step - loss: 0.7678 - accuracy: 0.9167\n","Epoch 91/123\n","1/1 [==============================] - 0s 121ms/step - loss: 0.7672 - accuracy: 0.9167\n","Epoch 92/123\n","1/1 [==============================] - 0s 122ms/step - loss: 0.7666 - accuracy: 0.9167\n","Epoch 93/123\n","1/1 [==============================] - 0s 154ms/step - loss: 0.7661 - accuracy: 0.9167\n","Epoch 94/123\n","1/1 [==============================] - 0s 177ms/step - loss: 0.7655 - accuracy: 0.9167\n","Epoch 95/123\n","1/1 [==============================] - 0s 142ms/step - loss: 0.7649 - accuracy: 0.9187\n","Epoch 96/123\n","1/1 [==============================] - 0s 139ms/step - loss: 0.7642 - accuracy: 0.9187\n","Epoch 97/123\n","1/1 [==============================] - 0s 142ms/step - loss: 0.7636 - accuracy: 0.9187\n","Epoch 98/123\n","1/1 [==============================] - 0s 158ms/step - loss: 0.7629 - accuracy: 0.9187\n","Epoch 99/123\n","1/1 [==============================] - 0s 158ms/step - loss: 0.7622 - accuracy: 0.9206\n","Epoch 100/123\n","1/1 [==============================] - 0s 162ms/step - loss: 0.7616 - accuracy: 0.9206\n","Epoch 101/123\n","1/1 [==============================] - 0s 197ms/step - loss: 0.7609 - accuracy: 0.9206\n","Epoch 102/123\n","1/1 [==============================] - 0s 146ms/step - loss: 0.7602 - accuracy: 0.9206\n","Epoch 103/123\n","1/1 [==============================] - 0s 109ms/step - loss: 0.7595 - accuracy: 0.9206\n","Epoch 104/123\n","1/1 [==============================] - 0s 115ms/step - loss: 0.7587 - accuracy: 0.9226\n","Epoch 105/123\n","1/1 [==============================] - 0s 118ms/step - loss: 0.7580 - accuracy: 0.9226\n","Epoch 106/123\n","1/1 [==============================] - 0s 118ms/step - loss: 0.7572 - accuracy: 0.9226\n","Epoch 107/123\n","1/1 [==============================] - 0s 120ms/step - loss: 0.7563 - accuracy: 0.9226\n","Epoch 108/123\n","1/1 [==============================] - 0s 107ms/step - loss: 0.7554 - accuracy: 0.9226\n","Epoch 109/123\n","1/1 [==============================] - 0s 120ms/step - loss: 0.7544 - accuracy: 0.9226\n","Epoch 110/123\n","1/1 [==============================] - 0s 113ms/step - loss: 0.7534 - accuracy: 0.9206\n","Epoch 111/123\n","1/1 [==============================] - 0s 124ms/step - loss: 0.7523 - accuracy: 0.9206\n","Epoch 112/123\n","1/1 [==============================] - 0s 115ms/step - loss: 0.7512 - accuracy: 0.9206\n","Epoch 113/123\n","1/1 [==============================] - 0s 119ms/step - loss: 0.7499 - accuracy: 0.9206\n","Epoch 114/123\n","1/1 [==============================] - 0s 119ms/step - loss: 0.7486 - accuracy: 0.9206\n","Epoch 115/123\n","1/1 [==============================] - 0s 168ms/step - loss: 0.7475 - accuracy: 0.9206\n","Epoch 116/123\n","1/1 [==============================] - 0s 167ms/step - loss: 0.7464 - accuracy: 0.9206\n","Epoch 117/123\n","1/1 [==============================] - 0s 188ms/step - loss: 0.7453 - accuracy: 0.9206\n","Epoch 118/123\n","1/1 [==============================] - 0s 213ms/step - loss: 0.7442 - accuracy: 0.9206\n","Epoch 119/123\n","1/1 [==============================] - 0s 147ms/step - loss: 0.7431 - accuracy: 0.9206\n","Epoch 120/123\n","1/1 [==============================] - 0s 144ms/step - loss: 0.7417 - accuracy: 0.9206\n","Epoch 121/123\n","1/1 [==============================] - 0s 135ms/step - loss: 0.7401 - accuracy: 0.9206\n","Epoch 122/123\n","1/1 [==============================] - 0s 128ms/step - loss: 0.7385 - accuracy: 0.9206\n","Epoch 123/123\n","1/1 [==============================] - 0s 147ms/step - loss: 0.7373 - accuracy: 0.9206\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b24395f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.2335 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:04:43,872]\u001b[0m Trial 34 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 54, 'num_epochs': 123}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/65\n","1/1 [==============================] - 5s 5s/step - loss: 3.1934 - accuracy: 0.1468\n","Epoch 2/65\n","1/1 [==============================] - 0s 143ms/step - loss: 1.3155 - accuracy: 0.9067\n","Epoch 3/65\n","1/1 [==============================] - 0s 190ms/step - loss: 0.9570 - accuracy: 0.9067\n","Epoch 4/65\n","1/1 [==============================] - 0s 185ms/step - loss: 0.9104 - accuracy: 0.9067\n","Epoch 5/65\n","1/1 [==============================] - 0s 282ms/step - loss: 0.8360 - accuracy: 0.9067\n","Epoch 6/65\n","1/1 [==============================] - 0s 131ms/step - loss: 0.7773 - accuracy: 0.9067\n","Epoch 7/65\n","1/1 [==============================] - 0s 212ms/step - loss: 0.7414 - accuracy: 0.9067\n","Epoch 8/65\n","1/1 [==============================] - 0s 179ms/step - loss: 0.7571 - accuracy: 0.9067\n","Epoch 9/65\n","1/1 [==============================] - 0s 187ms/step - loss: 0.7485 - accuracy: 0.9067\n","Epoch 10/65\n","1/1 [==============================] - 0s 180ms/step - loss: 0.7268 - accuracy: 0.9067\n","Epoch 11/65\n","1/1 [==============================] - 0s 154ms/step - loss: 0.7054 - accuracy: 0.9067\n","Epoch 12/65\n","1/1 [==============================] - 0s 164ms/step - loss: 0.7430 - accuracy: 0.9067\n","Epoch 13/65\n","1/1 [==============================] - 0s 152ms/step - loss: 0.7840 - accuracy: 0.9067\n","Epoch 14/65\n","1/1 [==============================] - 0s 165ms/step - loss: 0.8134 - accuracy: 0.9067\n","Epoch 15/65\n","1/1 [==============================] - 0s 117ms/step - loss: 0.7581 - accuracy: 0.9067\n","Epoch 16/65\n","1/1 [==============================] - 0s 123ms/step - loss: 0.7551 - accuracy: 0.9067\n","Epoch 17/65\n","1/1 [==============================] - 0s 106ms/step - loss: 0.7568 - accuracy: 0.9067\n","Epoch 18/65\n","1/1 [==============================] - 0s 118ms/step - loss: 0.7576 - accuracy: 0.9067\n","Epoch 19/65\n","1/1 [==============================] - 0s 120ms/step - loss: 0.7370 - accuracy: 0.9067\n","Epoch 20/65\n","1/1 [==============================] - 0s 128ms/step - loss: 0.7385 - accuracy: 0.9067\n","Epoch 21/65\n","1/1 [==============================] - 0s 132ms/step - loss: 0.7224 - accuracy: 0.9067\n","Epoch 22/65\n","1/1 [==============================] - 0s 151ms/step - loss: 0.6948 - accuracy: 0.9067\n","Epoch 23/65\n","1/1 [==============================] - 0s 164ms/step - loss: 0.6943 - accuracy: 0.9067\n","Epoch 24/65\n","1/1 [==============================] - 0s 158ms/step - loss: 0.6909 - accuracy: 0.9067\n","Epoch 25/65\n","1/1 [==============================] - 0s 158ms/step - loss: 0.6830 - accuracy: 0.9067\n","Epoch 26/65\n","1/1 [==============================] - 0s 158ms/step - loss: 0.6735 - accuracy: 0.9067\n","Epoch 27/65\n","1/1 [==============================] - 0s 150ms/step - loss: 0.6678 - accuracy: 0.9067\n","Epoch 28/65\n","1/1 [==============================] - 0s 225ms/step - loss: 0.6648 - accuracy: 0.9067\n","Epoch 29/65\n","1/1 [==============================] - 0s 178ms/step - loss: 0.6579 - accuracy: 0.9067\n","Epoch 30/65\n","1/1 [==============================] - 0s 144ms/step - loss: 0.6611 - accuracy: 0.9067\n","Epoch 31/65\n","1/1 [==============================] - 0s 163ms/step - loss: 0.6649 - accuracy: 0.9067\n","Epoch 32/65\n","1/1 [==============================] - 0s 161ms/step - loss: 0.6660 - accuracy: 0.9067\n","Epoch 33/65\n","1/1 [==============================] - 0s 149ms/step - loss: 0.6621 - accuracy: 0.9067\n","Epoch 34/65\n","1/1 [==============================] - 0s 152ms/step - loss: 0.6545 - accuracy: 0.9067\n","Epoch 35/65\n","1/1 [==============================] - 0s 150ms/step - loss: 0.6470 - accuracy: 0.9067\n","Epoch 36/65\n","1/1 [==============================] - 0s 174ms/step - loss: 0.6427 - accuracy: 0.9067\n","Epoch 37/65\n","1/1 [==============================] - 0s 181ms/step - loss: 0.6432 - accuracy: 0.9067\n","Epoch 38/65\n","1/1 [==============================] - 0s 210ms/step - loss: 0.6639 - accuracy: 0.9067\n","Epoch 39/65\n","1/1 [==============================] - 0s 154ms/step - loss: 0.6621 - accuracy: 0.9067\n","Epoch 40/65\n","1/1 [==============================] - 0s 153ms/step - loss: 0.6604 - accuracy: 0.9067\n","Epoch 41/65\n","1/1 [==============================] - 0s 137ms/step - loss: 0.6590 - accuracy: 0.9067\n","Epoch 42/65\n","1/1 [==============================] - 0s 141ms/step - loss: 0.6576 - accuracy: 0.9067\n","Epoch 43/65\n","1/1 [==============================] - 0s 146ms/step - loss: 0.6561 - accuracy: 0.9067\n","Epoch 44/65\n","1/1 [==============================] - 0s 142ms/step - loss: 0.6542 - accuracy: 0.9067\n","Epoch 45/65\n","1/1 [==============================] - 0s 137ms/step - loss: 0.6521 - accuracy: 0.9067\n","Epoch 46/65\n","1/1 [==============================] - 0s 139ms/step - loss: 0.6500 - accuracy: 0.9067\n","Epoch 47/65\n","1/1 [==============================] - 0s 151ms/step - loss: 0.6257 - accuracy: 0.9067\n","Epoch 48/65\n","1/1 [==============================] - 0s 200ms/step - loss: 0.6212 - accuracy: 0.9067\n","Epoch 49/65\n","1/1 [==============================] - 0s 142ms/step - loss: 0.6198 - accuracy: 0.9067\n","Epoch 50/65\n","1/1 [==============================] - 0s 143ms/step - loss: 0.6192 - accuracy: 0.9067\n","Epoch 51/65\n","1/1 [==============================] - 0s 154ms/step - loss: 0.6182 - accuracy: 0.9087\n","Epoch 52/65\n","1/1 [==============================] - 0s 153ms/step - loss: 0.6159 - accuracy: 0.9087\n","Epoch 53/65\n","1/1 [==============================] - 0s 147ms/step - loss: 0.6126 - accuracy: 0.9107\n","Epoch 54/65\n","1/1 [==============================] - 0s 149ms/step - loss: 0.6092 - accuracy: 0.9107\n","Epoch 55/65\n","1/1 [==============================] - 0s 143ms/step - loss: 0.6061 - accuracy: 0.9107\n","Epoch 56/65\n","1/1 [==============================] - 0s 146ms/step - loss: 0.6035 - accuracy: 0.9147\n","Epoch 57/65\n","1/1 [==============================] - 0s 146ms/step - loss: 0.6014 - accuracy: 0.9147\n","Epoch 58/65\n","1/1 [==============================] - 0s 156ms/step - loss: 0.5994 - accuracy: 0.9107\n","Epoch 59/65\n","1/1 [==============================] - 0s 114ms/step - loss: 0.5969 - accuracy: 0.9107\n","Epoch 60/65\n","1/1 [==============================] - 0s 159ms/step - loss: 0.5945 - accuracy: 0.9127\n","Epoch 61/65\n","1/1 [==============================] - 0s 159ms/step - loss: 0.5920 - accuracy: 0.9127\n","Epoch 62/65\n","1/1 [==============================] - 0s 161ms/step - loss: 0.5889 - accuracy: 0.9127\n","Epoch 63/65\n","1/1 [==============================] - 0s 140ms/step - loss: 0.5857 - accuracy: 0.9167\n","Epoch 64/65\n","1/1 [==============================] - 0s 153ms/step - loss: 0.5821 - accuracy: 0.9147\n","Epoch 65/65\n","1/1 [==============================] - 0s 165ms/step - loss: 0.5802 - accuracy: 0.9167\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b454540d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1657 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:05:02,080]\u001b[0m Trial 35 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 126, 'num_epochs': 65}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/91\n","1/1 [==============================] - 6s 6s/step - loss: 13.3673 - accuracy: 0.0040\n","Epoch 2/91\n","1/1 [==============================] - 0s 262ms/step - loss: 1.0285 - accuracy: 0.9067\n","Epoch 3/91\n","1/1 [==============================] - 0s 194ms/step - loss: 0.8888 - accuracy: 0.9067\n","Epoch 4/91\n","1/1 [==============================] - 0s 268ms/step - loss: 0.8368 - accuracy: 0.9067\n","Epoch 5/91\n","1/1 [==============================] - 0s 257ms/step - loss: 0.7973 - accuracy: 0.9067\n","Epoch 6/91\n","1/1 [==============================] - 0s 195ms/step - loss: 0.7592 - accuracy: 0.9067\n","Epoch 7/91\n","1/1 [==============================] - 0s 214ms/step - loss: 0.7407 - accuracy: 0.9067\n","Epoch 8/91\n","1/1 [==============================] - 0s 169ms/step - loss: 0.8324 - accuracy: 0.9067\n","Epoch 9/91\n","1/1 [==============================] - 0s 261ms/step - loss: 0.7974 - accuracy: 0.9067\n","Epoch 10/91\n","1/1 [==============================] - 0s 287ms/step - loss: 0.7508 - accuracy: 0.9067\n","Epoch 11/91\n","1/1 [==============================] - 0s 293ms/step - loss: 0.7207 - accuracy: 0.9067\n","Epoch 12/91\n","1/1 [==============================] - 0s 360ms/step - loss: 0.7062 - accuracy: 0.9067\n","Epoch 13/91\n","1/1 [==============================] - 0s 368ms/step - loss: 0.6613 - accuracy: 0.9067\n","Epoch 14/91\n","1/1 [==============================] - 0s 386ms/step - loss: 0.6209 - accuracy: 0.9067\n","Epoch 15/91\n","1/1 [==============================] - 0s 265ms/step - loss: 0.5811 - accuracy: 0.9067\n","Epoch 16/91\n","1/1 [==============================] - 0s 341ms/step - loss: 0.5420 - accuracy: 0.9067\n","Epoch 17/91\n","1/1 [==============================] - 0s 265ms/step - loss: 0.5086 - accuracy: 0.9067\n","Epoch 18/91\n","1/1 [==============================] - 0s 273ms/step - loss: 0.4884 - accuracy: 0.9067\n","Epoch 19/91\n","1/1 [==============================] - 0s 224ms/step - loss: 0.5226 - accuracy: 0.9067\n","Epoch 20/91\n","1/1 [==============================] - 0s 242ms/step - loss: 0.5506 - accuracy: 0.9067\n","Epoch 21/91\n","1/1 [==============================] - 0s 262ms/step - loss: 0.5958 - accuracy: 0.9067\n","Epoch 22/91\n","1/1 [==============================] - 0s 273ms/step - loss: 0.5992 - accuracy: 0.9067\n","Epoch 23/91\n","1/1 [==============================] - 0s 277ms/step - loss: 0.5963 - accuracy: 0.9067\n","Epoch 24/91\n","1/1 [==============================] - 0s 229ms/step - loss: 0.5898 - accuracy: 0.9067\n","Epoch 25/91\n","1/1 [==============================] - 1s 536ms/step - loss: 0.5846 - accuracy: 0.9067\n","Epoch 26/91\n","1/1 [==============================] - 0s 243ms/step - loss: 0.5804 - accuracy: 0.9067\n","Epoch 27/91\n","1/1 [==============================] - 0s 263ms/step - loss: 0.5775 - accuracy: 0.9067\n","Epoch 28/91\n","1/1 [==============================] - 0s 475ms/step - loss: 0.5756 - accuracy: 0.9067\n","Epoch 29/91\n","1/1 [==============================] - 0s 322ms/step - loss: 0.5741 - accuracy: 0.9067\n","Epoch 30/91\n","1/1 [==============================] - 0s 368ms/step - loss: 0.5727 - accuracy: 0.9067\n","Epoch 31/91\n","1/1 [==============================] - 0s 267ms/step - loss: 0.5511 - accuracy: 0.9067\n","Epoch 32/91\n","1/1 [==============================] - 0s 340ms/step - loss: 0.5451 - accuracy: 0.9067\n","Epoch 33/91\n","1/1 [==============================] - 0s 324ms/step - loss: 0.5444 - accuracy: 0.9067\n","Epoch 34/91\n","1/1 [==============================] - 1s 535ms/step - loss: 0.5441 - accuracy: 0.9067\n","Epoch 35/91\n","1/1 [==============================] - 1s 530ms/step - loss: 0.5434 - accuracy: 0.9067\n","Epoch 36/91\n","1/1 [==============================] - 0s 262ms/step - loss: 0.5420 - accuracy: 0.9067\n","Epoch 37/91\n","1/1 [==============================] - 0s 350ms/step - loss: 0.5401 - accuracy: 0.9067\n","Epoch 38/91\n","1/1 [==============================] - 0s 286ms/step - loss: 0.5378 - accuracy: 0.9067\n","Epoch 39/91\n","1/1 [==============================] - 1s 522ms/step - loss: 0.5355 - accuracy: 0.9067\n","Epoch 40/91\n","1/1 [==============================] - 0s 211ms/step - loss: 0.5331 - accuracy: 0.9067\n","Epoch 41/91\n","1/1 [==============================] - 0s 375ms/step - loss: 0.5307 - accuracy: 0.9067\n","Epoch 42/91\n","1/1 [==============================] - 0s 463ms/step - loss: 0.5284 - accuracy: 0.9067\n","Epoch 43/91\n","1/1 [==============================] - 0s 331ms/step - loss: 0.5260 - accuracy: 0.9067\n","Epoch 44/91\n","1/1 [==============================] - 0s 244ms/step - loss: 0.5236 - accuracy: 0.9067\n","Epoch 45/91\n","1/1 [==============================] - 1s 530ms/step - loss: 0.5211 - accuracy: 0.9087\n","Epoch 46/91\n","1/1 [==============================] - 0s 239ms/step - loss: 0.5182 - accuracy: 0.9087\n","Epoch 47/91\n","1/1 [==============================] - 1s 721ms/step - loss: 0.5150 - accuracy: 0.9087\n","Epoch 48/91\n","1/1 [==============================] - 0s 244ms/step - loss: 0.5112 - accuracy: 0.9107\n","Epoch 49/91\n","1/1 [==============================] - 0s 403ms/step - loss: 0.5070 - accuracy: 0.9107\n","Epoch 50/91\n","1/1 [==============================] - 0s 278ms/step - loss: 0.5025 - accuracy: 0.9127\n","Epoch 51/91\n","1/1 [==============================] - 0s 280ms/step - loss: 0.4983 - accuracy: 0.9127\n","Epoch 52/91\n","1/1 [==============================] - 0s 369ms/step - loss: 0.4916 - accuracy: 0.9226\n","Epoch 53/91\n","1/1 [==============================] - 0s 291ms/step - loss: 0.5206 - accuracy: 0.9226\n","Epoch 54/91\n","1/1 [==============================] - 0s 239ms/step - loss: 0.5453 - accuracy: 0.9187\n","Epoch 55/91\n","1/1 [==============================] - 0s 289ms/step - loss: 0.5417 - accuracy: 0.9147\n","Epoch 56/91\n","1/1 [==============================] - 0s 369ms/step - loss: 0.5419 - accuracy: 0.9226\n","Epoch 57/91\n","1/1 [==============================] - 0s 418ms/step - loss: 0.5433 - accuracy: 0.9226\n","Epoch 58/91\n","1/1 [==============================] - 0s 380ms/step - loss: 0.5379 - accuracy: 0.9167\n","Epoch 59/91\n","1/1 [==============================] - 0s 298ms/step - loss: 0.5399 - accuracy: 0.9206\n","Epoch 60/91\n","1/1 [==============================] - 0s 428ms/step - loss: 0.5411 - accuracy: 0.9246\n","Epoch 61/91\n","1/1 [==============================] - 0s 320ms/step - loss: 0.5413 - accuracy: 0.9246\n","Epoch 62/91\n","1/1 [==============================] - 0s 322ms/step - loss: 0.5409 - accuracy: 0.9246\n","Epoch 63/91\n","1/1 [==============================] - 1s 560ms/step - loss: 0.5390 - accuracy: 0.9266\n","Epoch 64/91\n","1/1 [==============================] - 0s 473ms/step - loss: 0.5349 - accuracy: 0.9246\n","Epoch 65/91\n","1/1 [==============================] - 0s 288ms/step - loss: 0.5302 - accuracy: 0.9266\n","Epoch 66/91\n","1/1 [==============================] - 0s 339ms/step - loss: 0.5265 - accuracy: 0.9246\n","Epoch 67/91\n","1/1 [==============================] - 0s 312ms/step - loss: 0.5269 - accuracy: 0.9246\n","Epoch 68/91\n","1/1 [==============================] - 0s 227ms/step - loss: 0.5225 - accuracy: 0.9266\n","Epoch 69/91\n","1/1 [==============================] - 0s 202ms/step - loss: 0.5198 - accuracy: 0.9286\n","Epoch 70/91\n","1/1 [==============================] - 0s 420ms/step - loss: 0.5185 - accuracy: 0.9306\n","Epoch 71/91\n","1/1 [==============================] - 0s 375ms/step - loss: 0.5175 - accuracy: 0.9286\n","Epoch 72/91\n","1/1 [==============================] - 0s 303ms/step - loss: 0.5156 - accuracy: 0.9325\n","Epoch 73/91\n","1/1 [==============================] - 0s 312ms/step - loss: 0.5127 - accuracy: 0.9345\n","Epoch 74/91\n","1/1 [==============================] - 0s 346ms/step - loss: 0.5098 - accuracy: 0.9345\n","Epoch 75/91\n","1/1 [==============================] - 0s 285ms/step - loss: 0.5081 - accuracy: 0.9345\n","Epoch 76/91\n","1/1 [==============================] - 0s 360ms/step - loss: 0.5048 - accuracy: 0.9385\n","Epoch 77/91\n","1/1 [==============================] - 0s 377ms/step - loss: 0.5017 - accuracy: 0.9405\n","Epoch 78/91\n","1/1 [==============================] - 0s 404ms/step - loss: 0.4988 - accuracy: 0.9405\n","Epoch 79/91\n","1/1 [==============================] - 0s 325ms/step - loss: 0.4956 - accuracy: 0.9385\n","Epoch 80/91\n","1/1 [==============================] - 0s 407ms/step - loss: nan - accuracy: 0.9365\n","Epoch 81/91\n","1/1 [==============================] - 0s 266ms/step - loss: nan - accuracy: 0.9067\n","Epoch 82/91\n","1/1 [==============================] - 0s 313ms/step - loss: nan - accuracy: 0.9067\n","Epoch 83/91\n","1/1 [==============================] - 1s 576ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/91\n","1/1 [==============================] - 0s 401ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/91\n","1/1 [==============================] - 0s 274ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/91\n","1/1 [==============================] - 0s 301ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/91\n","1/1 [==============================] - 1s 589ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/91\n","1/1 [==============================] - 0s 375ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/91\n","1/1 [==============================] - 1s 571ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/91\n","1/1 [==============================] - 1s 793ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/91\n","1/1 [==============================] - 1s 617ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b4cd138b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:05:41,531]\u001b[0m Trial 36 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 206, 'num_epochs': 91}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/141\n","1/1 [==============================] - 5s 5s/step - loss: 15.2681 - accuracy: 0.0000e+00\n","Epoch 2/141\n","1/1 [==============================] - 0s 149ms/step - loss: 11.5515 - accuracy: 0.1567\n","Epoch 3/141\n","1/1 [==============================] - 0s 138ms/step - loss: 1.6301 - accuracy: 0.9067\n","Epoch 4/141\n","1/1 [==============================] - 0s 223ms/step - loss: 1.3037 - accuracy: 0.9067\n","Epoch 5/141\n","1/1 [==============================] - 0s 144ms/step - loss: 1.1649 - accuracy: 0.9067\n","Epoch 6/141\n","1/1 [==============================] - 0s 255ms/step - loss: 1.0553 - accuracy: 0.9067\n","Epoch 7/141\n","1/1 [==============================] - 0s 166ms/step - loss: 1.1294 - accuracy: 0.9067\n","Epoch 8/141\n","1/1 [==============================] - 0s 175ms/step - loss: 1.1139 - accuracy: 0.9067\n","Epoch 9/141\n","1/1 [==============================] - 0s 228ms/step - loss: 1.1109 - accuracy: 0.9067\n","Epoch 10/141\n","1/1 [==============================] - 0s 195ms/step - loss: 1.1117 - accuracy: 0.9067\n","Epoch 11/141\n","1/1 [==============================] - 0s 204ms/step - loss: 1.1104 - accuracy: 0.9067\n","Epoch 12/141\n","1/1 [==============================] - 0s 185ms/step - loss: 1.1065 - accuracy: 0.9067\n","Epoch 13/141\n","1/1 [==============================] - 0s 193ms/step - loss: 1.1026 - accuracy: 0.9067\n","Epoch 14/141\n","1/1 [==============================] - 0s 185ms/step - loss: 1.1003 - accuracy: 0.9067\n","Epoch 15/141\n","1/1 [==============================] - 0s 188ms/step - loss: 1.0996 - accuracy: 0.9067\n","Epoch 16/141\n","1/1 [==============================] - 0s 177ms/step - loss: 1.0984 - accuracy: 0.9067\n","Epoch 17/141\n","1/1 [==============================] - 0s 281ms/step - loss: 1.0964 - accuracy: 0.9067\n","Epoch 18/141\n","1/1 [==============================] - 0s 203ms/step - loss: 1.0941 - accuracy: 0.9067\n","Epoch 19/141\n","1/1 [==============================] - 0s 158ms/step - loss: 1.0922 - accuracy: 0.9067\n","Epoch 20/141\n","1/1 [==============================] - 0s 156ms/step - loss: 1.0906 - accuracy: 0.9067\n","Epoch 21/141\n","1/1 [==============================] - 0s 145ms/step - loss: 1.0892 - accuracy: 0.9067\n","Epoch 22/141\n","1/1 [==============================] - 0s 145ms/step - loss: 1.0879 - accuracy: 0.9067\n","Epoch 23/141\n","1/1 [==============================] - 0s 166ms/step - loss: 1.0861 - accuracy: 0.9067\n","Epoch 24/141\n","1/1 [==============================] - 0s 164ms/step - loss: 1.0838 - accuracy: 0.9067\n","Epoch 25/141\n","1/1 [==============================] - 0s 144ms/step - loss: 1.0815 - accuracy: 0.9067\n","Epoch 26/141\n","1/1 [==============================] - 0s 152ms/step - loss: 1.0795 - accuracy: 0.9067\n","Epoch 27/141\n","1/1 [==============================] - 0s 151ms/step - loss: 1.0774 - accuracy: 0.9087\n","Epoch 28/141\n","1/1 [==============================] - 0s 181ms/step - loss: 1.0751 - accuracy: 0.9107\n","Epoch 29/141\n","1/1 [==============================] - 0s 180ms/step - loss: 1.0723 - accuracy: 0.9127\n","Epoch 30/141\n","1/1 [==============================] - 0s 207ms/step - loss: 1.0691 - accuracy: 0.9127\n","Epoch 31/141\n","1/1 [==============================] - 0s 198ms/step - loss: 1.0665 - accuracy: 0.9147\n","Epoch 32/141\n","1/1 [==============================] - 0s 225ms/step - loss: 1.0916 - accuracy: 0.9167\n","Epoch 33/141\n","1/1 [==============================] - 0s 189ms/step - loss: 1.0905 - accuracy: 0.9147\n","Epoch 34/141\n","1/1 [==============================] - 0s 196ms/step - loss: 1.1256 - accuracy: 0.9127\n","Epoch 35/141\n","1/1 [==============================] - 0s 199ms/step - loss: 1.0663 - accuracy: 0.9147\n","Epoch 36/141\n","1/1 [==============================] - 0s 199ms/step - loss: 1.0685 - accuracy: 0.9147\n","Epoch 37/141\n","1/1 [==============================] - 0s 184ms/step - loss: 1.0706 - accuracy: 0.9147\n","Epoch 38/141\n","1/1 [==============================] - 0s 191ms/step - loss: 1.0715 - accuracy: 0.9147\n","Epoch 39/141\n","1/1 [==============================] - 0s 195ms/step - loss: 1.0729 - accuracy: 0.9127\n","Epoch 40/141\n","1/1 [==============================] - 0s 187ms/step - loss: 1.0734 - accuracy: 0.9107\n","Epoch 41/141\n","1/1 [==============================] - 0s 146ms/step - loss: 1.0730 - accuracy: 0.9107\n","Epoch 42/141\n","1/1 [==============================] - 0s 160ms/step - loss: 1.0724 - accuracy: 0.9087\n","Epoch 43/141\n","1/1 [==============================] - 0s 188ms/step - loss: 1.0727 - accuracy: 0.9087\n","Epoch 44/141\n","1/1 [==============================] - 0s 151ms/step - loss: 1.0730 - accuracy: 0.9067\n","Epoch 45/141\n","1/1 [==============================] - 0s 171ms/step - loss: 1.0720 - accuracy: 0.9067\n","Epoch 46/141\n","1/1 [==============================] - 0s 164ms/step - loss: 1.0709 - accuracy: 0.9087\n","Epoch 47/141\n","1/1 [==============================] - 0s 189ms/step - loss: 1.0697 - accuracy: 0.9087\n","Epoch 48/141\n","1/1 [==============================] - 0s 209ms/step - loss: 1.0687 - accuracy: 0.9087\n","Epoch 49/141\n","1/1 [==============================] - 0s 180ms/step - loss: 1.0680 - accuracy: 0.9147\n","Epoch 50/141\n","1/1 [==============================] - 0s 172ms/step - loss: 1.0674 - accuracy: 0.9167\n","Epoch 51/141\n","1/1 [==============================] - 0s 175ms/step - loss: 1.0666 - accuracy: 0.9206\n","Epoch 52/141\n","1/1 [==============================] - 0s 173ms/step - loss: 1.0656 - accuracy: 0.9187\n","Epoch 53/141\n","1/1 [==============================] - 0s 198ms/step - loss: 1.0643 - accuracy: 0.9187\n","Epoch 54/141\n","1/1 [==============================] - 0s 180ms/step - loss: 1.0630 - accuracy: 0.9206\n","Epoch 55/141\n","1/1 [==============================] - 0s 214ms/step - loss: 1.0617 - accuracy: 0.9206\n","Epoch 56/141\n","1/1 [==============================] - 0s 292ms/step - loss: 1.0605 - accuracy: 0.9187\n","Epoch 57/141\n","1/1 [==============================] - 0s 284ms/step - loss: 1.0596 - accuracy: 0.9187\n","Epoch 58/141\n","1/1 [==============================] - 0s 182ms/step - loss: 1.0588 - accuracy: 0.9187\n","Epoch 59/141\n","1/1 [==============================] - 0s 225ms/step - loss: 1.0577 - accuracy: 0.9187\n","Epoch 60/141\n","1/1 [==============================] - 0s 180ms/step - loss: 1.0563 - accuracy: 0.9206\n","Epoch 61/141\n","1/1 [==============================] - 0s 180ms/step - loss: 1.0564 - accuracy: 0.9206\n","Epoch 62/141\n","1/1 [==============================] - 0s 177ms/step - loss: 1.0541 - accuracy: 0.9226\n","Epoch 63/141\n","1/1 [==============================] - 0s 182ms/step - loss: 1.0530 - accuracy: 0.9226\n","Epoch 64/141\n","1/1 [==============================] - 0s 160ms/step - loss: 1.0516 - accuracy: 0.9206\n","Epoch 65/141\n","1/1 [==============================] - 0s 190ms/step - loss: 1.0509 - accuracy: 0.9226\n","Epoch 66/141\n","1/1 [==============================] - 0s 167ms/step - loss: 1.0497 - accuracy: 0.9226\n","Epoch 67/141\n","1/1 [==============================] - 0s 160ms/step - loss: 1.0485 - accuracy: 0.9226\n","Epoch 68/141\n","1/1 [==============================] - 0s 251ms/step - loss: 1.0474 - accuracy: 0.9246\n","Epoch 69/141\n","1/1 [==============================] - 0s 167ms/step - loss: nan - accuracy: 0.9246\n","Epoch 70/141\n","1/1 [==============================] - 0s 170ms/step - loss: nan - accuracy: 0.9067\n","Epoch 71/141\n","1/1 [==============================] - 0s 204ms/step - loss: nan - accuracy: 0.9067\n","Epoch 72/141\n","1/1 [==============================] - 0s 235ms/step - loss: nan - accuracy: 0.9067\n","Epoch 73/141\n","1/1 [==============================] - 0s 246ms/step - loss: nan - accuracy: 0.9067\n","Epoch 74/141\n","1/1 [==============================] - 0s 228ms/step - loss: nan - accuracy: 0.9067\n","Epoch 75/141\n","1/1 [==============================] - 0s 259ms/step - loss: nan - accuracy: 0.9067\n","Epoch 76/141\n","1/1 [==============================] - 0s 226ms/step - loss: nan - accuracy: 0.9067\n","Epoch 77/141\n","1/1 [==============================] - 0s 207ms/step - loss: nan - accuracy: 0.9067\n","Epoch 78/141\n","1/1 [==============================] - 0s 185ms/step - loss: nan - accuracy: 0.9067\n","Epoch 79/141\n","1/1 [==============================] - 0s 236ms/step - loss: nan - accuracy: 0.9067\n","Epoch 80/141\n","1/1 [==============================] - 0s 170ms/step - loss: nan - accuracy: 0.9067\n","Epoch 81/141\n","1/1 [==============================] - 0s 224ms/step - loss: nan - accuracy: 0.9067\n","Epoch 82/141\n","1/1 [==============================] - 0s 254ms/step - loss: nan - accuracy: 0.9067\n","Epoch 83/141\n","1/1 [==============================] - 0s 240ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/141\n","1/1 [==============================] - 0s 177ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/141\n","1/1 [==============================] - 0s 184ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/141\n","1/1 [==============================] - 0s 172ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/141\n","1/1 [==============================] - 0s 220ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/141\n","1/1 [==============================] - 0s 179ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/141\n","1/1 [==============================] - 0s 162ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/141\n","1/1 [==============================] - 0s 154ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/141\n","1/1 [==============================] - 0s 169ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/141\n","1/1 [==============================] - 0s 217ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/141\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/141\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/141\n","1/1 [==============================] - 0s 191ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/141\n","1/1 [==============================] - 0s 277ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/141\n","1/1 [==============================] - 0s 234ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/141\n","1/1 [==============================] - 0s 153ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/141\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/141\n","1/1 [==============================] - 0s 169ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/141\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/141\n","1/1 [==============================] - 0s 166ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/141\n","1/1 [==============================] - 0s 165ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/141\n","1/1 [==============================] - 0s 142ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/141\n","1/1 [==============================] - 0s 183ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/141\n","1/1 [==============================] - 0s 143ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/141\n","1/1 [==============================] - 0s 153ms/step - loss: nan - accuracy: 0.9067\n","Epoch 108/141\n","1/1 [==============================] - 0s 146ms/step - loss: nan - accuracy: 0.9067\n","Epoch 109/141\n","1/1 [==============================] - 0s 143ms/step - loss: nan - accuracy: 0.9067\n","Epoch 110/141\n","1/1 [==============================] - 0s 148ms/step - loss: nan - accuracy: 0.9067\n","Epoch 111/141\n","1/1 [==============================] - 0s 138ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/141\n","1/1 [==============================] - 0s 158ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/141\n","1/1 [==============================] - 0s 152ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/141\n","1/1 [==============================] - 0s 141ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/141\n","1/1 [==============================] - 0s 156ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/141\n","1/1 [==============================] - 0s 175ms/step - loss: nan - accuracy: 0.9067\n","Epoch 117/141\n","1/1 [==============================] - 0s 194ms/step - loss: nan - accuracy: 0.9067\n","Epoch 118/141\n","1/1 [==============================] - 0s 185ms/step - loss: nan - accuracy: 0.9067\n","Epoch 119/141\n","1/1 [==============================] - 0s 162ms/step - loss: nan - accuracy: 0.9067\n","Epoch 120/141\n","1/1 [==============================] - 0s 151ms/step - loss: nan - accuracy: 0.9067\n","Epoch 121/141\n","1/1 [==============================] - 0s 157ms/step - loss: nan - accuracy: 0.9067\n","Epoch 122/141\n","1/1 [==============================] - 0s 171ms/step - loss: nan - accuracy: 0.9067\n","Epoch 123/141\n","1/1 [==============================] - 0s 149ms/step - loss: nan - accuracy: 0.9067\n","Epoch 124/141\n","1/1 [==============================] - 0s 145ms/step - loss: nan - accuracy: 0.9067\n","Epoch 125/141\n","1/1 [==============================] - 0s 150ms/step - loss: nan - accuracy: 0.9067\n","Epoch 126/141\n","1/1 [==============================] - 0s 145ms/step - loss: nan - accuracy: 0.9067\n","Epoch 127/141\n","1/1 [==============================] - 0s 151ms/step - loss: nan - accuracy: 0.9067\n","Epoch 128/141\n","1/1 [==============================] - 0s 147ms/step - loss: nan - accuracy: 0.9067\n","Epoch 129/141\n","1/1 [==============================] - 0s 165ms/step - loss: nan - accuracy: 0.9067\n","Epoch 130/141\n","1/1 [==============================] - 0s 185ms/step - loss: nan - accuracy: 0.9067\n","Epoch 131/141\n","1/1 [==============================] - 0s 162ms/step - loss: nan - accuracy: 0.9067\n","Epoch 132/141\n","1/1 [==============================] - 0s 192ms/step - loss: nan - accuracy: 0.9067\n","Epoch 133/141\n","1/1 [==============================] - 0s 164ms/step - loss: nan - accuracy: 0.9067\n","Epoch 134/141\n","1/1 [==============================] - 0s 267ms/step - loss: nan - accuracy: 0.9067\n","Epoch 135/141\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 136/141\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 137/141\n","1/1 [==============================] - 0s 211ms/step - loss: nan - accuracy: 0.9067\n","Epoch 138/141\n","1/1 [==============================] - 0s 150ms/step - loss: nan - accuracy: 0.9067\n","Epoch 139/141\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 140/141\n","1/1 [==============================] - 0s 180ms/step - loss: nan - accuracy: 0.9067\n","Epoch 141/141\n","1/1 [==============================] - 0s 193ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b2754f700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:06:14,927]\u001b[0m Trial 37 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 157, 'num_epochs': 141}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/69\n","1/1 [==============================] - 6s 6s/step - loss: 15.1081 - accuracy: 0.0079\n","Epoch 2/69\n","1/1 [==============================] - 0s 134ms/step - loss: 1.9951 - accuracy: 0.6131\n","Epoch 3/69\n","1/1 [==============================] - 0s 176ms/step - loss: 1.2356 - accuracy: 0.9067\n","Epoch 4/69\n","1/1 [==============================] - 0s 178ms/step - loss: 0.9275 - accuracy: 0.9067\n","Epoch 5/69\n","1/1 [==============================] - 0s 144ms/step - loss: 0.8390 - accuracy: 0.9067\n","Epoch 6/69\n","1/1 [==============================] - 0s 173ms/step - loss: 0.8883 - accuracy: 0.9067\n","Epoch 7/69\n","1/1 [==============================] - 0s 169ms/step - loss: 0.8354 - accuracy: 0.9067\n","Epoch 8/69\n","1/1 [==============================] - 0s 168ms/step - loss: 0.8056 - accuracy: 0.9067\n","Epoch 9/69\n","1/1 [==============================] - 0s 134ms/step - loss: 0.8084 - accuracy: 0.9067\n","Epoch 10/69\n","1/1 [==============================] - 0s 140ms/step - loss: 0.7878 - accuracy: 0.9067\n","Epoch 11/69\n","1/1 [==============================] - 0s 147ms/step - loss: 0.7572 - accuracy: 0.9067\n","Epoch 12/69\n","1/1 [==============================] - 0s 135ms/step - loss: 0.7537 - accuracy: 0.9067\n","Epoch 13/69\n","1/1 [==============================] - 0s 127ms/step - loss: 0.7409 - accuracy: 0.9067\n","Epoch 14/69\n","1/1 [==============================] - 0s 85ms/step - loss: 0.7325 - accuracy: 0.9067\n","Epoch 15/69\n","1/1 [==============================] - 0s 102ms/step - loss: 0.7353 - accuracy: 0.9067\n","Epoch 16/69\n","1/1 [==============================] - 0s 118ms/step - loss: 0.7271 - accuracy: 0.9067\n","Epoch 17/69\n","1/1 [==============================] - 0s 116ms/step - loss: 0.7255 - accuracy: 0.9067\n","Epoch 18/69\n","1/1 [==============================] - 0s 118ms/step - loss: 0.7262 - accuracy: 0.9067\n","Epoch 19/69\n","1/1 [==============================] - 0s 121ms/step - loss: 0.7253 - accuracy: 0.9067\n","Epoch 20/69\n","1/1 [==============================] - 0s 117ms/step - loss: 0.7213 - accuracy: 0.9067\n","Epoch 21/69\n","1/1 [==============================] - 0s 120ms/step - loss: 0.7163 - accuracy: 0.9067\n","Epoch 22/69\n","1/1 [==============================] - 0s 125ms/step - loss: 0.7137 - accuracy: 0.9067\n","Epoch 23/69\n","1/1 [==============================] - 0s 112ms/step - loss: 0.7128 - accuracy: 0.9067\n","Epoch 24/69\n","1/1 [==============================] - 0s 117ms/step - loss: 0.7097 - accuracy: 0.9067\n","Epoch 25/69\n","1/1 [==============================] - 0s 116ms/step - loss: 0.7065 - accuracy: 0.9067\n","Epoch 26/69\n","1/1 [==============================] - 0s 119ms/step - loss: 0.7046 - accuracy: 0.9067\n","Epoch 27/69\n","1/1 [==============================] - 0s 133ms/step - loss: 0.7034 - accuracy: 0.9067\n","Epoch 28/69\n","1/1 [==============================] - 0s 120ms/step - loss: 0.7017 - accuracy: 0.9067\n","Epoch 29/69\n","1/1 [==============================] - 0s 121ms/step - loss: 0.6991 - accuracy: 0.9067\n","Epoch 30/69\n","1/1 [==============================] - 0s 123ms/step - loss: 0.6963 - accuracy: 0.9087\n","Epoch 31/69\n","1/1 [==============================] - 0s 122ms/step - loss: 0.6939 - accuracy: 0.9127\n","Epoch 32/69\n","1/1 [==============================] - 0s 120ms/step - loss: 0.6920 - accuracy: 0.9167\n","Epoch 33/69\n","1/1 [==============================] - 0s 121ms/step - loss: 0.6890 - accuracy: 0.9187\n","Epoch 34/69\n","1/1 [==============================] - 0s 120ms/step - loss: 0.6858 - accuracy: 0.9246\n","Epoch 35/69\n","1/1 [==============================] - 0s 116ms/step - loss: 0.6834 - accuracy: 0.9246\n","Epoch 36/69\n","1/1 [==============================] - 0s 140ms/step - loss: 0.6810 - accuracy: 0.9246\n","Epoch 37/69\n","1/1 [==============================] - 0s 189ms/step - loss: 0.6776 - accuracy: 0.9226\n","Epoch 38/69\n","1/1 [==============================] - 0s 134ms/step - loss: 0.6740 - accuracy: 0.9206\n","Epoch 39/69\n","1/1 [==============================] - 0s 261ms/step - loss: 0.6718 - accuracy: 0.9187\n","Epoch 40/69\n","1/1 [==============================] - 0s 141ms/step - loss: 0.6705 - accuracy: 0.9187\n","Epoch 41/69\n","1/1 [==============================] - 0s 165ms/step - loss: 0.6709 - accuracy: 0.9187\n","Epoch 42/69\n","1/1 [==============================] - 0s 173ms/step - loss: 0.6665 - accuracy: 0.9206\n","Epoch 43/69\n","1/1 [==============================] - 0s 155ms/step - loss: 0.6644 - accuracy: 0.9206\n","Epoch 44/69\n","1/1 [==============================] - 0s 127ms/step - loss: 0.6633 - accuracy: 0.9206\n","Epoch 45/69\n","1/1 [==============================] - 0s 145ms/step - loss: 0.6622 - accuracy: 0.9266\n","Epoch 46/69\n","1/1 [==============================] - 0s 143ms/step - loss: 0.6578 - accuracy: 0.9266\n","Epoch 47/69\n","1/1 [==============================] - 0s 140ms/step - loss: 0.6550 - accuracy: 0.9206\n","Epoch 48/69\n","1/1 [==============================] - 0s 132ms/step - loss: 0.6559 - accuracy: 0.9206\n","Epoch 49/69\n","1/1 [==============================] - 0s 130ms/step - loss: 0.6514 - accuracy: 0.9286\n","Epoch 50/69\n","1/1 [==============================] - 0s 204ms/step - loss: 0.6517 - accuracy: 0.9286\n","Epoch 51/69\n","1/1 [==============================] - 0s 183ms/step - loss: 0.6510 - accuracy: 0.9286\n","Epoch 52/69\n","1/1 [==============================] - 0s 186ms/step - loss: 0.6474 - accuracy: 0.9286\n","Epoch 53/69\n","1/1 [==============================] - 0s 172ms/step - loss: 0.6423 - accuracy: 0.9306\n","Epoch 54/69\n","1/1 [==============================] - 0s 178ms/step - loss: nan - accuracy: 0.9306\n","Epoch 55/69\n","1/1 [==============================] - 0s 195ms/step - loss: nan - accuracy: 0.9067\n","Epoch 56/69\n","1/1 [==============================] - 0s 166ms/step - loss: nan - accuracy: 0.9067\n","Epoch 57/69\n","1/1 [==============================] - 0s 134ms/step - loss: nan - accuracy: 0.9067\n","Epoch 58/69\n","1/1 [==============================] - 0s 222ms/step - loss: nan - accuracy: 0.9067\n","Epoch 59/69\n","1/1 [==============================] - 0s 187ms/step - loss: nan - accuracy: 0.9067\n","Epoch 60/69\n","1/1 [==============================] - 0s 204ms/step - loss: nan - accuracy: 0.9067\n","Epoch 61/69\n","1/1 [==============================] - 0s 225ms/step - loss: nan - accuracy: 0.9067\n","Epoch 62/69\n","1/1 [==============================] - 0s 198ms/step - loss: nan - accuracy: 0.9067\n","Epoch 63/69\n","1/1 [==============================] - 0s 164ms/step - loss: nan - accuracy: 0.9067\n","Epoch 64/69\n","1/1 [==============================] - 0s 166ms/step - loss: nan - accuracy: 0.9067\n","Epoch 65/69\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 66/69\n","1/1 [==============================] - 0s 241ms/step - loss: nan - accuracy: 0.9067\n","Epoch 67/69\n","1/1 [==============================] - 0s 184ms/step - loss: nan - accuracy: 0.9067\n","Epoch 68/69\n","1/1 [==============================] - 0s 201ms/step - loss: nan - accuracy: 0.9067\n","Epoch 69/69\n","1/1 [==============================] - 0s 213ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b4c84aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:06:33,650]\u001b[0m Trial 38 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 103, 'num_epochs': 69}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/73\n","1/1 [==============================] - 5s 5s/step - loss: 15.7458 - accuracy: 0.0040\n","Epoch 2/73\n","1/1 [==============================] - 0s 127ms/step - loss: 2.6039 - accuracy: 0.2044\n","Epoch 3/73\n","1/1 [==============================] - 0s 163ms/step - loss: 1.5309 - accuracy: 0.9067\n","Epoch 4/73\n","1/1 [==============================] - 0s 166ms/step - loss: 1.0703 - accuracy: 0.9067\n","Epoch 5/73\n","1/1 [==============================] - 0s 174ms/step - loss: 0.9141 - accuracy: 0.9067\n","Epoch 6/73\n","1/1 [==============================] - 0s 207ms/step - loss: 0.8186 - accuracy: 0.9067\n","Epoch 7/73\n","1/1 [==============================] - 0s 176ms/step - loss: 0.7615 - accuracy: 0.9067\n","Epoch 8/73\n","1/1 [==============================] - 0s 179ms/step - loss: 0.7375 - accuracy: 0.9067\n","Epoch 9/73\n","1/1 [==============================] - 0s 192ms/step - loss: 0.7366 - accuracy: 0.9067\n","Epoch 10/73\n","1/1 [==============================] - 0s 185ms/step - loss: 0.7680 - accuracy: 0.9067\n","Epoch 11/73\n","1/1 [==============================] - 0s 200ms/step - loss: 0.8072 - accuracy: 0.9067\n","Epoch 12/73\n","1/1 [==============================] - 0s 162ms/step - loss: 0.8023 - accuracy: 0.9067\n","Epoch 13/73\n","1/1 [==============================] - 0s 202ms/step - loss: 0.7270 - accuracy: 0.9067\n","Epoch 14/73\n","1/1 [==============================] - 0s 182ms/step - loss: 0.7217 - accuracy: 0.9067\n","Epoch 15/73\n","1/1 [==============================] - 0s 181ms/step - loss: 0.7385 - accuracy: 0.9067\n","Epoch 16/73\n","1/1 [==============================] - 0s 175ms/step - loss: 0.7419 - accuracy: 0.9067\n","Epoch 17/73\n","1/1 [==============================] - 0s 195ms/step - loss: 0.7328 - accuracy: 0.9067\n","Epoch 18/73\n","1/1 [==============================] - 0s 295ms/step - loss: 0.7150 - accuracy: 0.9067\n","Epoch 19/73\n","1/1 [==============================] - 0s 157ms/step - loss: 0.6923 - accuracy: 0.9067\n","Epoch 20/73\n","1/1 [==============================] - 0s 184ms/step - loss: 0.6677 - accuracy: 0.9067\n","Epoch 21/73\n","1/1 [==============================] - 0s 173ms/step - loss: 0.6469 - accuracy: 0.9067\n","Epoch 22/73\n","1/1 [==============================] - 0s 193ms/step - loss: 0.6351 - accuracy: 0.9067\n","Epoch 23/73\n","1/1 [==============================] - 0s 183ms/step - loss: 0.6738 - accuracy: 0.9067\n","Epoch 24/73\n","1/1 [==============================] - 0s 160ms/step - loss: 0.6771 - accuracy: 0.9067\n","Epoch 25/73\n","1/1 [==============================] - 0s 135ms/step - loss: 0.6740 - accuracy: 0.9067\n","Epoch 26/73\n","1/1 [==============================] - 0s 132ms/step - loss: 0.6732 - accuracy: 0.9067\n","Epoch 27/73\n","1/1 [==============================] - 0s 135ms/step - loss: 0.6724 - accuracy: 0.9067\n","Epoch 28/73\n","1/1 [==============================] - 0s 140ms/step - loss: 0.6704 - accuracy: 0.9067\n","Epoch 29/73\n","1/1 [==============================] - 0s 140ms/step - loss: 0.6685 - accuracy: 0.9067\n","Epoch 30/73\n","1/1 [==============================] - 0s 142ms/step - loss: 0.6666 - accuracy: 0.9067\n","Epoch 31/73\n","1/1 [==============================] - 0s 136ms/step - loss: 0.6647 - accuracy: 0.9067\n","Epoch 32/73\n","1/1 [==============================] - 0s 166ms/step - loss: 0.6627 - accuracy: 0.9067\n","Epoch 33/73\n","1/1 [==============================] - 0s 139ms/step - loss: 0.6608 - accuracy: 0.9067\n","Epoch 34/73\n","1/1 [==============================] - 0s 125ms/step - loss: 0.6591 - accuracy: 0.9067\n","Epoch 35/73\n","1/1 [==============================] - 0s 143ms/step - loss: 0.6577 - accuracy: 0.9067\n","Epoch 36/73\n","1/1 [==============================] - 0s 138ms/step - loss: 0.6567 - accuracy: 0.9067\n","Epoch 37/73\n","1/1 [==============================] - 0s 154ms/step - loss: 0.6558 - accuracy: 0.9067\n","Epoch 38/73\n","1/1 [==============================] - 0s 134ms/step - loss: 0.6548 - accuracy: 0.9067\n","Epoch 39/73\n","1/1 [==============================] - 0s 129ms/step - loss: 0.6535 - accuracy: 0.9067\n","Epoch 40/73\n","1/1 [==============================] - 0s 137ms/step - loss: 0.6519 - accuracy: 0.9067\n","Epoch 41/73\n","1/1 [==============================] - 0s 154ms/step - loss: 0.6503 - accuracy: 0.9067\n","Epoch 42/73\n","1/1 [==============================] - 0s 151ms/step - loss: 0.6489 - accuracy: 0.9067\n","Epoch 43/73\n","1/1 [==============================] - 0s 162ms/step - loss: 0.6477 - accuracy: 0.9067\n","Epoch 44/73\n","1/1 [==============================] - 0s 151ms/step - loss: 0.6466 - accuracy: 0.9067\n","Epoch 45/73\n","1/1 [==============================] - 0s 160ms/step - loss: 0.6455 - accuracy: 0.9087\n","Epoch 46/73\n","1/1 [==============================] - 0s 165ms/step - loss: 0.6443 - accuracy: 0.9087\n","Epoch 47/73\n","1/1 [==============================] - 0s 182ms/step - loss: 0.6431 - accuracy: 0.9087\n","Epoch 48/73\n","1/1 [==============================] - 0s 147ms/step - loss: 0.6417 - accuracy: 0.9087\n","Epoch 49/73\n","1/1 [==============================] - 0s 177ms/step - loss: 0.6402 - accuracy: 0.9087\n","Epoch 50/73\n","1/1 [==============================] - 0s 168ms/step - loss: 0.6388 - accuracy: 0.9107\n","Epoch 51/73\n","1/1 [==============================] - 0s 145ms/step - loss: 0.6375 - accuracy: 0.9127\n","Epoch 52/73\n","1/1 [==============================] - 0s 160ms/step - loss: 0.6362 - accuracy: 0.9147\n","Epoch 53/73\n","1/1 [==============================] - 0s 157ms/step - loss: 0.6350 - accuracy: 0.9147\n","Epoch 54/73\n","1/1 [==============================] - 0s 209ms/step - loss: 0.6335 - accuracy: 0.9147\n","Epoch 55/73\n","1/1 [==============================] - 0s 146ms/step - loss: 0.6319 - accuracy: 0.9187\n","Epoch 56/73\n","1/1 [==============================] - 0s 172ms/step - loss: 0.6303 - accuracy: 0.9206\n","Epoch 57/73\n","1/1 [==============================] - 0s 171ms/step - loss: 0.6288 - accuracy: 0.9167\n","Epoch 58/73\n","1/1 [==============================] - 0s 153ms/step - loss: 0.6273 - accuracy: 0.9167\n","Epoch 59/73\n","1/1 [==============================] - 0s 170ms/step - loss: 0.6258 - accuracy: 0.9206\n","Epoch 60/73\n","1/1 [==============================] - 0s 168ms/step - loss: 0.6242 - accuracy: 0.9206\n","Epoch 61/73\n","1/1 [==============================] - 0s 207ms/step - loss: 0.6227 - accuracy: 0.9187\n","Epoch 62/73\n","1/1 [==============================] - 0s 268ms/step - loss: 0.6214 - accuracy: 0.9187\n","Epoch 63/73\n","1/1 [==============================] - 0s 212ms/step - loss: 0.6203 - accuracy: 0.9226\n","Epoch 64/73\n","1/1 [==============================] - 0s 218ms/step - loss: 0.6188 - accuracy: 0.9226\n","Epoch 65/73\n","1/1 [==============================] - 0s 202ms/step - loss: 0.6172 - accuracy: 0.9226\n","Epoch 66/73\n","1/1 [==============================] - 0s 152ms/step - loss: 0.6151 - accuracy: 0.9187\n","Epoch 67/73\n","1/1 [==============================] - 0s 185ms/step - loss: 0.6129 - accuracy: 0.9206\n","Epoch 68/73\n","1/1 [==============================] - 0s 210ms/step - loss: 0.6110 - accuracy: 0.9206\n","Epoch 69/73\n","1/1 [==============================] - 0s 185ms/step - loss: 0.6093 - accuracy: 0.9246\n","Epoch 70/73\n","1/1 [==============================] - 0s 189ms/step - loss: 0.6074 - accuracy: 0.9246\n","Epoch 71/73\n","1/1 [==============================] - 0s 161ms/step - loss: 0.6048 - accuracy: 0.9266\n","Epoch 72/73\n","1/1 [==============================] - 0s 191ms/step - loss: 0.6023 - accuracy: 0.9246\n","Epoch 73/73\n","1/1 [==============================] - 0s 175ms/step - loss: 0.6010 - accuracy: 0.9246\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b101e0d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.2391 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:06:53,502]\u001b[0m Trial 39 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 139, 'num_epochs': 73}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","1/1 [==============================] - 6s 6s/step - loss: 4.6403 - accuracy: 0.0238\n","Epoch 2/100\n","1/1 [==============================] - 0s 147ms/step - loss: 1.5304 - accuracy: 0.9067\n","Epoch 3/100\n","1/1 [==============================] - 0s 201ms/step - loss: 1.4206 - accuracy: 0.9067\n","Epoch 4/100\n","1/1 [==============================] - 0s 160ms/step - loss: 1.3289 - accuracy: 0.9067\n","Epoch 5/100\n","1/1 [==============================] - 0s 209ms/step - loss: 1.2584 - accuracy: 0.9067\n","Epoch 6/100\n","1/1 [==============================] - 0s 167ms/step - loss: 1.3280 - accuracy: 0.9067\n","Epoch 7/100\n","1/1 [==============================] - 0s 204ms/step - loss: 1.3038 - accuracy: 0.9067\n","Epoch 8/100\n","1/1 [==============================] - 0s 158ms/step - loss: 1.2506 - accuracy: 0.9067\n","Epoch 9/100\n","1/1 [==============================] - 0s 168ms/step - loss: 1.2499 - accuracy: 0.9067\n","Epoch 10/100\n","1/1 [==============================] - 0s 177ms/step - loss: 1.3067 - accuracy: 0.9067\n","Epoch 11/100\n","1/1 [==============================] - 0s 182ms/step - loss: 1.3117 - accuracy: 0.9067\n","Epoch 12/100\n","1/1 [==============================] - 0s 188ms/step - loss: 1.2861 - accuracy: 0.9067\n","Epoch 13/100\n","1/1 [==============================] - 0s 167ms/step - loss: 1.2195 - accuracy: 0.9067\n","Epoch 14/100\n","1/1 [==============================] - 0s 189ms/step - loss: 1.1877 - accuracy: 0.9067\n","Epoch 15/100\n","1/1 [==============================] - 0s 205ms/step - loss: 1.2214 - accuracy: 0.9067\n","Epoch 16/100\n","1/1 [==============================] - 0s 179ms/step - loss: 1.2592 - accuracy: 0.9067\n","Epoch 17/100\n","1/1 [==============================] - 0s 210ms/step - loss: 1.2673 - accuracy: 0.9067\n","Epoch 18/100\n","1/1 [==============================] - 0s 199ms/step - loss: 1.2130 - accuracy: 0.9067\n","Epoch 19/100\n","1/1 [==============================] - 0s 205ms/step - loss: 1.1777 - accuracy: 0.9067\n","Epoch 20/100\n","1/1 [==============================] - 0s 182ms/step - loss: 1.1717 - accuracy: 0.9067\n","Epoch 21/100\n","1/1 [==============================] - 0s 177ms/step - loss: 1.1762 - accuracy: 0.9067\n","Epoch 22/100\n","1/1 [==============================] - 0s 179ms/step - loss: 1.2005 - accuracy: 0.9067\n","Epoch 23/100\n","1/1 [==============================] - 0s 181ms/step - loss: 1.1983 - accuracy: 0.9067\n","Epoch 24/100\n","1/1 [==============================] - 0s 211ms/step - loss: 1.1937 - accuracy: 0.9067\n","Epoch 25/100\n","1/1 [==============================] - 0s 175ms/step - loss: 1.1894 - accuracy: 0.9067\n","Epoch 26/100\n","1/1 [==============================] - 0s 197ms/step - loss: 1.2069 - accuracy: 0.9067\n","Epoch 27/100\n","1/1 [==============================] - 0s 189ms/step - loss: 1.2181 - accuracy: 0.9067\n","Epoch 28/100\n","1/1 [==============================] - 0s 200ms/step - loss: 1.2079 - accuracy: 0.9067\n","Epoch 29/100\n","1/1 [==============================] - 0s 180ms/step - loss: 1.2044 - accuracy: 0.9067\n","Epoch 30/100\n","1/1 [==============================] - 0s 205ms/step - loss: 1.2250 - accuracy: 0.9067\n","Epoch 31/100\n","1/1 [==============================] - 0s 184ms/step - loss: 1.2346 - accuracy: 0.9067\n","Epoch 32/100\n","1/1 [==============================] - 0s 198ms/step - loss: 1.2252 - accuracy: 0.9067\n","Epoch 33/100\n","1/1 [==============================] - 0s 215ms/step - loss: 1.2033 - accuracy: 0.9067\n","Epoch 34/100\n","1/1 [==============================] - 0s 188ms/step - loss: 1.2044 - accuracy: 0.9067\n","Epoch 35/100\n","1/1 [==============================] - 0s 191ms/step - loss: 1.2065 - accuracy: 0.9067\n","Epoch 36/100\n","1/1 [==============================] - 0s 237ms/step - loss: 1.2080 - accuracy: 0.9067\n","Epoch 37/100\n","1/1 [==============================] - 0s 168ms/step - loss: 1.2084 - accuracy: 0.9067\n","Epoch 38/100\n","1/1 [==============================] - 0s 161ms/step - loss: 1.2077 - accuracy: 0.9067\n","Epoch 39/100\n","1/1 [==============================] - 0s 253ms/step - loss: 1.2058 - accuracy: 0.9067\n","Epoch 40/100\n","1/1 [==============================] - 0s 233ms/step - loss: 1.2031 - accuracy: 0.9067\n","Epoch 41/100\n","1/1 [==============================] - 0s 253ms/step - loss: 1.1998 - accuracy: 0.9067\n","Epoch 42/100\n","1/1 [==============================] - 0s 241ms/step - loss: 1.1967 - accuracy: 0.9067\n","Epoch 43/100\n","1/1 [==============================] - 0s 241ms/step - loss: 1.1823 - accuracy: 0.9067\n","Epoch 44/100\n","1/1 [==============================] - 0s 176ms/step - loss: 1.1612 - accuracy: 0.9067\n","Epoch 45/100\n","1/1 [==============================] - 0s 207ms/step - loss: 1.2365 - accuracy: 0.8948\n","Epoch 46/100\n","1/1 [==============================] - 0s 200ms/step - loss: 1.3274 - accuracy: 0.8393\n","Epoch 47/100\n","1/1 [==============================] - 0s 286ms/step - loss: 1.3294 - accuracy: 0.7540\n","Epoch 48/100\n","1/1 [==============================] - 0s 199ms/step - loss: 1.3308 - accuracy: 0.6984\n","Epoch 49/100\n","1/1 [==============================] - 0s 231ms/step - loss: 1.4053 - accuracy: 0.6687\n","Epoch 50/100\n","1/1 [==============================] - 0s 199ms/step - loss: 1.4039 - accuracy: 0.6607\n","Epoch 51/100\n","1/1 [==============================] - 0s 234ms/step - loss: 1.3451 - accuracy: 0.6548\n","Epoch 52/100\n","1/1 [==============================] - 0s 175ms/step - loss: 1.2849 - accuracy: 0.6627\n","Epoch 53/100\n","1/1 [==============================] - 0s 276ms/step - loss: 1.2511 - accuracy: 0.6766\n","Epoch 54/100\n","1/1 [==============================] - 0s 205ms/step - loss: 1.2203 - accuracy: 0.7103\n","Epoch 55/100\n","1/1 [==============================] - 0s 169ms/step - loss: 1.1936 - accuracy: 0.7421\n","Epoch 56/100\n","1/1 [==============================] - 0s 203ms/step - loss: 1.1867 - accuracy: 0.7996\n","Epoch 57/100\n","1/1 [==============================] - 0s 232ms/step - loss: 1.1533 - accuracy: 0.8591\n","Epoch 58/100\n","1/1 [==============================] - 0s 233ms/step - loss: 1.1196 - accuracy: 0.8829\n","Epoch 59/100\n","1/1 [==============================] - 0s 240ms/step - loss: 1.0878 - accuracy: 0.8929\n","Epoch 60/100\n","1/1 [==============================] - 0s 167ms/step - loss: 1.0579 - accuracy: 0.8988\n","Epoch 61/100\n","1/1 [==============================] - 0s 194ms/step - loss: 1.0293 - accuracy: 0.9048\n","Epoch 62/100\n","1/1 [==============================] - 0s 192ms/step - loss: 1.0017 - accuracy: 0.9067\n","Epoch 63/100\n","1/1 [==============================] - 0s 164ms/step - loss: 0.9749 - accuracy: 0.9067\n","Epoch 64/100\n","1/1 [==============================] - 0s 208ms/step - loss: 0.9491 - accuracy: 0.9067\n","Epoch 65/100\n","1/1 [==============================] - 0s 276ms/step - loss: 0.8400 - accuracy: 0.9067\n","Epoch 66/100\n","1/1 [==============================] - 0s 238ms/step - loss: 0.8125 - accuracy: 0.9067\n","Epoch 67/100\n","1/1 [==============================] - 0s 208ms/step - loss: 0.8054 - accuracy: 0.9067\n","Epoch 68/100\n","1/1 [==============================] - 0s 274ms/step - loss: 0.8008 - accuracy: 0.9067\n","Epoch 69/100\n","1/1 [==============================] - 0s 190ms/step - loss: 0.7943 - accuracy: 0.9067\n","Epoch 70/100\n","1/1 [==============================] - 0s 260ms/step - loss: 0.7847 - accuracy: 0.9067\n","Epoch 71/100\n","1/1 [==============================] - 0s 199ms/step - loss: 0.7742 - accuracy: 0.9067\n","Epoch 72/100\n","1/1 [==============================] - 0s 238ms/step - loss: 0.7850 - accuracy: 0.9067\n","Epoch 73/100\n","1/1 [==============================] - 0s 213ms/step - loss: 0.7741 - accuracy: 0.9067\n","Epoch 74/100\n","1/1 [==============================] - 0s 205ms/step - loss: 0.7670 - accuracy: 0.9067\n","Epoch 75/100\n","1/1 [==============================] - 0s 250ms/step - loss: 0.7650 - accuracy: 0.9067\n","Epoch 76/100\n","1/1 [==============================] - 0s 214ms/step - loss: 0.7737 - accuracy: 0.9067\n","Epoch 77/100\n","1/1 [==============================] - 0s 207ms/step - loss: 0.7913 - accuracy: 0.9067\n","Epoch 78/100\n","1/1 [==============================] - 0s 258ms/step - loss: 0.7871 - accuracy: 0.9067\n","Epoch 79/100\n","1/1 [==============================] - 0s 244ms/step - loss: 0.7859 - accuracy: 0.9067\n","Epoch 80/100\n","1/1 [==============================] - 0s 192ms/step - loss: 0.7858 - accuracy: 0.9067\n","Epoch 81/100\n","1/1 [==============================] - 0s 258ms/step - loss: 0.7863 - accuracy: 0.9067\n","Epoch 82/100\n","1/1 [==============================] - 0s 164ms/step - loss: 0.7864 - accuracy: 0.9067\n","Epoch 83/100\n","1/1 [==============================] - 0s 178ms/step - loss: 0.7864 - accuracy: 0.9067\n","Epoch 84/100\n","1/1 [==============================] - 0s 192ms/step - loss: 0.7858 - accuracy: 0.9067\n","Epoch 85/100\n","1/1 [==============================] - 0s 256ms/step - loss: 0.7845 - accuracy: 0.9067\n","Epoch 86/100\n","1/1 [==============================] - 0s 183ms/step - loss: 0.7826 - accuracy: 0.9067\n","Epoch 87/100\n","1/1 [==============================] - 0s 219ms/step - loss: 0.7808 - accuracy: 0.9067\n","Epoch 88/100\n","1/1 [==============================] - 0s 216ms/step - loss: 0.7794 - accuracy: 0.9067\n","Epoch 89/100\n","1/1 [==============================] - 0s 236ms/step - loss: 0.7784 - accuracy: 0.9067\n","Epoch 90/100\n","1/1 [==============================] - 0s 173ms/step - loss: 0.7777 - accuracy: 0.9067\n","Epoch 91/100\n","1/1 [==============================] - 0s 163ms/step - loss: 0.7773 - accuracy: 0.9067\n","Epoch 92/100\n","1/1 [==============================] - 0s 182ms/step - loss: 0.7769 - accuracy: 0.9067\n","Epoch 93/100\n","1/1 [==============================] - 0s 198ms/step - loss: 0.7763 - accuracy: 0.9067\n","Epoch 94/100\n","1/1 [==============================] - 0s 229ms/step - loss: 0.7755 - accuracy: 0.9067\n","Epoch 95/100\n","1/1 [==============================] - 0s 173ms/step - loss: 0.7746 - accuracy: 0.9067\n","Epoch 96/100\n","1/1 [==============================] - 0s 203ms/step - loss: 0.7536 - accuracy: 0.9067\n","Epoch 97/100\n","1/1 [==============================] - 0s 154ms/step - loss: 0.7498 - accuracy: 0.9067\n","Epoch 98/100\n","1/1 [==============================] - 0s 231ms/step - loss: 0.7481 - accuracy: 0.9067\n","Epoch 99/100\n","1/1 [==============================] - 0s 197ms/step - loss: 0.7470 - accuracy: 0.9067\n","Epoch 100/100\n","1/1 [==============================] - 0s 198ms/step - loss: 0.7462 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b45638a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 0.9974 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:07:22,027]\u001b[0m Trial 40 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 174, 'num_epochs': 100}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/149\n","1/1 [==============================] - 5s 5s/step - loss: 14.2563 - accuracy: 0.0139\n","Epoch 2/149\n","1/1 [==============================] - 0s 152ms/step - loss: 1.7572 - accuracy: 0.8591\n","Epoch 3/149\n","1/1 [==============================] - 0s 176ms/step - loss: 1.0617 - accuracy: 0.9028\n","Epoch 4/149\n","1/1 [==============================] - 0s 212ms/step - loss: 0.9042 - accuracy: 0.9028\n","Epoch 5/149\n","1/1 [==============================] - 0s 164ms/step - loss: 0.8055 - accuracy: 0.9028\n","Epoch 6/149\n","1/1 [==============================] - 0s 214ms/step - loss: 0.7254 - accuracy: 0.9048\n","Epoch 7/149\n","1/1 [==============================] - 0s 154ms/step - loss: 0.6377 - accuracy: 0.9087\n","Epoch 8/149\n","1/1 [==============================] - 0s 210ms/step - loss: 0.5437 - accuracy: 0.9067\n","Epoch 9/149\n","1/1 [==============================] - 0s 178ms/step - loss: 0.4902 - accuracy: 0.9067\n","Epoch 10/149\n","1/1 [==============================] - 0s 189ms/step - loss: 0.6975 - accuracy: 0.9067\n","Epoch 11/149\n","1/1 [==============================] - 0s 168ms/step - loss: 0.5526 - accuracy: 0.9067\n","Epoch 12/149\n","1/1 [==============================] - 0s 183ms/step - loss: 0.4476 - accuracy: 0.9067\n","Epoch 13/149\n","1/1 [==============================] - 0s 160ms/step - loss: 0.4505 - accuracy: 0.9067\n","Epoch 14/149\n","1/1 [==============================] - 0s 151ms/step - loss: 0.4578 - accuracy: 0.9067\n","Epoch 15/149\n","1/1 [==============================] - 0s 151ms/step - loss: 0.4602 - accuracy: 0.9067\n","Epoch 16/149\n","1/1 [==============================] - 0s 146ms/step - loss: 0.4563 - accuracy: 0.9067\n","Epoch 17/149\n","1/1 [==============================] - 0s 149ms/step - loss: 0.4515 - accuracy: 0.9067\n","Epoch 18/149\n","1/1 [==============================] - 0s 136ms/step - loss: 0.4431 - accuracy: 0.9067\n","Epoch 19/149\n","1/1 [==============================] - 0s 147ms/step - loss: 0.4375 - accuracy: 0.9067\n","Epoch 20/149\n","1/1 [==============================] - 0s 141ms/step - loss: 0.4327 - accuracy: 0.9067\n","Epoch 21/149\n","1/1 [==============================] - 0s 142ms/step - loss: 0.4277 - accuracy: 0.9067\n","Epoch 22/149\n","1/1 [==============================] - 0s 221ms/step - loss: 0.4230 - accuracy: 0.9067\n","Epoch 23/149\n","1/1 [==============================] - 0s 250ms/step - loss: 0.4197 - accuracy: 0.9067\n","Epoch 24/149\n","1/1 [==============================] - 0s 178ms/step - loss: 0.4209 - accuracy: 0.9067\n","Epoch 25/149\n","1/1 [==============================] - 0s 198ms/step - loss: 0.4146 - accuracy: 0.9067\n","Epoch 26/149\n","1/1 [==============================] - 0s 198ms/step - loss: 0.4084 - accuracy: 0.9067\n","Epoch 27/149\n","1/1 [==============================] - 0s 292ms/step - loss: 0.4042 - accuracy: 0.9067\n","Epoch 28/149\n","1/1 [==============================] - 0s 173ms/step - loss: 0.4008 - accuracy: 0.9067\n","Epoch 29/149\n","1/1 [==============================] - 0s 180ms/step - loss: 0.3973 - accuracy: 0.9067\n","Epoch 30/149\n","1/1 [==============================] - 0s 178ms/step - loss: 0.3934 - accuracy: 0.9067\n","Epoch 31/149\n","1/1 [==============================] - 0s 174ms/step - loss: 0.3891 - accuracy: 0.9067\n","Epoch 32/149\n","1/1 [==============================] - 0s 169ms/step - loss: 0.3843 - accuracy: 0.9067\n","Epoch 33/149\n","1/1 [==============================] - 0s 172ms/step - loss: 0.3795 - accuracy: 0.9067\n","Epoch 34/149\n","1/1 [==============================] - 0s 178ms/step - loss: 0.3754 - accuracy: 0.9067\n","Epoch 35/149\n","1/1 [==============================] - 0s 211ms/step - loss: 0.3724 - accuracy: 0.9067\n","Epoch 36/149\n","1/1 [==============================] - 0s 196ms/step - loss: 0.3696 - accuracy: 0.9067\n","Epoch 37/149\n","1/1 [==============================] - 0s 219ms/step - loss: 0.3656 - accuracy: 0.9107\n","Epoch 38/149\n","1/1 [==============================] - 0s 185ms/step - loss: 0.3609 - accuracy: 0.9127\n","Epoch 39/149\n","1/1 [==============================] - 0s 193ms/step - loss: 0.3571 - accuracy: 0.9206\n","Epoch 40/149\n","1/1 [==============================] - 0s 178ms/step - loss: 0.3540 - accuracy: 0.9187\n","Epoch 41/149\n","1/1 [==============================] - 0s 193ms/step - loss: 0.3509 - accuracy: 0.9206\n","Epoch 42/149\n","1/1 [==============================] - 0s 226ms/step - loss: 0.3475 - accuracy: 0.9206\n","Epoch 43/149\n","1/1 [==============================] - 0s 231ms/step - loss: 0.3438 - accuracy: 0.9226\n","Epoch 44/149\n","1/1 [==============================] - 0s 221ms/step - loss: 0.3395 - accuracy: 0.9246\n","Epoch 45/149\n","1/1 [==============================] - 0s 223ms/step - loss: 0.3362 - accuracy: 0.9266\n","Epoch 46/149\n","1/1 [==============================] - 0s 191ms/step - loss: 0.3332 - accuracy: 0.9246\n","Epoch 47/149\n","1/1 [==============================] - 0s 170ms/step - loss: 0.3294 - accuracy: 0.9246\n","Epoch 48/149\n","1/1 [==============================] - 0s 253ms/step - loss: 0.3255 - accuracy: 0.9266\n","Epoch 49/149\n","1/1 [==============================] - 0s 169ms/step - loss: 0.3230 - accuracy: 0.9286\n","Epoch 50/149\n","1/1 [==============================] - 0s 291ms/step - loss: 0.3232 - accuracy: 0.9286\n","Epoch 51/149\n","1/1 [==============================] - 0s 183ms/step - loss: 0.3195 - accuracy: 0.9306\n","Epoch 52/149\n","1/1 [==============================] - 0s 218ms/step - loss: 0.3181 - accuracy: 0.9246\n","Epoch 53/149\n","1/1 [==============================] - 0s 200ms/step - loss: 0.3143 - accuracy: 0.9306\n","Epoch 54/149\n","1/1 [==============================] - 0s 251ms/step - loss: 0.3109 - accuracy: 0.9246\n","Epoch 55/149\n","1/1 [==============================] - 0s 242ms/step - loss: 0.3092 - accuracy: 0.9246\n","Epoch 56/149\n","1/1 [==============================] - 0s 209ms/step - loss: 0.3067 - accuracy: 0.9246\n","Epoch 57/149\n","1/1 [==============================] - 0s 198ms/step - loss: 0.3024 - accuracy: 0.9345\n","Epoch 58/149\n","1/1 [==============================] - 0s 188ms/step - loss: 0.2975 - accuracy: 0.9365\n","Epoch 59/149\n","1/1 [==============================] - 0s 212ms/step - loss: nan - accuracy: 0.9385\n","Epoch 60/149\n","1/1 [==============================] - 0s 293ms/step - loss: nan - accuracy: 0.9067\n","Epoch 61/149\n","1/1 [==============================] - 0s 189ms/step - loss: nan - accuracy: 0.9067\n","Epoch 62/149\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","Epoch 63/149\n","1/1 [==============================] - 0s 185ms/step - loss: nan - accuracy: 0.9067\n","Epoch 64/149\n","1/1 [==============================] - 0s 196ms/step - loss: nan - accuracy: 0.9067\n","Epoch 65/149\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","Epoch 66/149\n","1/1 [==============================] - 0s 206ms/step - loss: nan - accuracy: 0.9067\n","Epoch 67/149\n","1/1 [==============================] - 0s 160ms/step - loss: nan - accuracy: 0.9067\n","Epoch 68/149\n","1/1 [==============================] - 0s 196ms/step - loss: nan - accuracy: 0.9067\n","Epoch 69/149\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 70/149\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","Epoch 71/149\n","1/1 [==============================] - 0s 209ms/step - loss: nan - accuracy: 0.9067\n","Epoch 72/149\n","1/1 [==============================] - 0s 172ms/step - loss: nan - accuracy: 0.9067\n","Epoch 73/149\n","1/1 [==============================] - 0s 169ms/step - loss: nan - accuracy: 0.9067\n","Epoch 74/149\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 75/149\n","1/1 [==============================] - 0s 175ms/step - loss: nan - accuracy: 0.9067\n","Epoch 76/149\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 77/149\n","1/1 [==============================] - 0s 191ms/step - loss: nan - accuracy: 0.9067\n","Epoch 78/149\n","1/1 [==============================] - 0s 168ms/step - loss: nan - accuracy: 0.9067\n","Epoch 79/149\n","1/1 [==============================] - 0s 188ms/step - loss: nan - accuracy: 0.9067\n","Epoch 80/149\n","1/1 [==============================] - 0s 180ms/step - loss: nan - accuracy: 0.9067\n","Epoch 81/149\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","Epoch 82/149\n","1/1 [==============================] - 0s 195ms/step - loss: nan - accuracy: 0.9067\n","Epoch 83/149\n","1/1 [==============================] - 0s 218ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/149\n","1/1 [==============================] - 0s 235ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/149\n","1/1 [==============================] - 0s 213ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/149\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/149\n","1/1 [==============================] - 0s 276ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/149\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/149\n","1/1 [==============================] - 0s 183ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/149\n","1/1 [==============================] - 0s 192ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/149\n","1/1 [==============================] - 0s 180ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/149\n","1/1 [==============================] - 0s 205ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/149\n","1/1 [==============================] - 0s 197ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/149\n","1/1 [==============================] - 0s 198ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/149\n","1/1 [==============================] - 0s 183ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/149\n","1/1 [==============================] - 0s 211ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/149\n","1/1 [==============================] - 0s 242ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/149\n","1/1 [==============================] - 0s 230ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/149\n","1/1 [==============================] - 0s 204ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/149\n","1/1 [==============================] - 0s 227ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/149\n","1/1 [==============================] - 0s 234ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/149\n","1/1 [==============================] - 0s 205ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/149\n","1/1 [==============================] - 0s 203ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/149\n","1/1 [==============================] - 0s 239ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/149\n","1/1 [==============================] - 0s 212ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/149\n","1/1 [==============================] - 0s 368ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/149\n","1/1 [==============================] - 0s 178ms/step - loss: nan - accuracy: 0.9067\n","Epoch 108/149\n","1/1 [==============================] - 0s 196ms/step - loss: nan - accuracy: 0.9067\n","Epoch 109/149\n","1/1 [==============================] - 0s 195ms/step - loss: nan - accuracy: 0.9067\n","Epoch 110/149\n","1/1 [==============================] - 0s 190ms/step - loss: nan - accuracy: 0.9067\n","Epoch 111/149\n","1/1 [==============================] - 0s 199ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/149\n","1/1 [==============================] - 0s 202ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/149\n","1/1 [==============================] - 0s 188ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/149\n","1/1 [==============================] - 0s 201ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/149\n","1/1 [==============================] - 0s 187ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/149\n","1/1 [==============================] - 0s 218ms/step - loss: nan - accuracy: 0.9067\n","Epoch 117/149\n","1/1 [==============================] - 0s 179ms/step - loss: nan - accuracy: 0.9067\n","Epoch 118/149\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","Epoch 119/149\n","1/1 [==============================] - 0s 182ms/step - loss: nan - accuracy: 0.9067\n","Epoch 120/149\n","1/1 [==============================] - 0s 216ms/step - loss: nan - accuracy: 0.9067\n","Epoch 121/149\n","1/1 [==============================] - 0s 200ms/step - loss: nan - accuracy: 0.9067\n","Epoch 122/149\n","1/1 [==============================] - 0s 193ms/step - loss: nan - accuracy: 0.9067\n","Epoch 123/149\n","1/1 [==============================] - 0s 195ms/step - loss: nan - accuracy: 0.9067\n","Epoch 124/149\n","1/1 [==============================] - 0s 194ms/step - loss: nan - accuracy: 0.9067\n","Epoch 125/149\n","1/1 [==============================] - 0s 227ms/step - loss: nan - accuracy: 0.9067\n","Epoch 126/149\n","1/1 [==============================] - 0s 203ms/step - loss: nan - accuracy: 0.9067\n","Epoch 127/149\n","1/1 [==============================] - 0s 200ms/step - loss: nan - accuracy: 0.9067\n","Epoch 128/149\n","1/1 [==============================] - 0s 183ms/step - loss: nan - accuracy: 0.9067\n","Epoch 129/149\n","1/1 [==============================] - 0s 213ms/step - loss: nan - accuracy: 0.9067\n","Epoch 130/149\n","1/1 [==============================] - 0s 200ms/step - loss: nan - accuracy: 0.9067\n","Epoch 131/149\n","1/1 [==============================] - 0s 234ms/step - loss: nan - accuracy: 0.9067\n","Epoch 132/149\n","1/1 [==============================] - 0s 198ms/step - loss: nan - accuracy: 0.9067\n","Epoch 133/149\n","1/1 [==============================] - 0s 257ms/step - loss: nan - accuracy: 0.9067\n","Epoch 134/149\n","1/1 [==============================] - 0s 206ms/step - loss: nan - accuracy: 0.9067\n","Epoch 135/149\n","1/1 [==============================] - 0s 203ms/step - loss: nan - accuracy: 0.9067\n","Epoch 136/149\n","1/1 [==============================] - 0s 191ms/step - loss: nan - accuracy: 0.9067\n","Epoch 137/149\n","1/1 [==============================] - 0s 167ms/step - loss: nan - accuracy: 0.9067\n","Epoch 138/149\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 139/149\n","1/1 [==============================] - 0s 160ms/step - loss: nan - accuracy: 0.9067\n","Epoch 140/149\n","1/1 [==============================] - 0s 155ms/step - loss: nan - accuracy: 0.9067\n","Epoch 141/149\n","1/1 [==============================] - 0s 200ms/step - loss: nan - accuracy: 0.9067\n","Epoch 142/149\n","1/1 [==============================] - 0s 247ms/step - loss: nan - accuracy: 0.9067\n","Epoch 143/149\n","1/1 [==============================] - 0s 201ms/step - loss: nan - accuracy: 0.9067\n","Epoch 144/149\n","1/1 [==============================] - 0s 191ms/step - loss: nan - accuracy: 0.9067\n","Epoch 145/149\n","1/1 [==============================] - 0s 219ms/step - loss: nan - accuracy: 0.9067\n","Epoch 146/149\n","1/1 [==============================] - 0s 313ms/step - loss: nan - accuracy: 0.9067\n","Epoch 147/149\n","1/1 [==============================] - 0s 278ms/step - loss: nan - accuracy: 0.9067\n","Epoch 148/149\n","1/1 [==============================] - 0s 205ms/step - loss: nan - accuracy: 0.9067\n","Epoch 149/149\n","1/1 [==============================] - 0s 234ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b247cbf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 2s 2s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:07:59,877]\u001b[0m Trial 41 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 182, 'num_epochs': 149}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/107\n","1/1 [==============================] - 5s 5s/step - loss: 8.8247 - accuracy: 0.0198\n","Epoch 2/107\n","1/1 [==============================] - 0s 121ms/step - loss: 2.1389 - accuracy: 0.8433\n","Epoch 3/107\n","1/1 [==============================] - 0s 196ms/step - loss: 1.8621 - accuracy: 0.9067\n","Epoch 4/107\n","1/1 [==============================] - 0s 201ms/step - loss: 1.7066 - accuracy: 0.9067\n","Epoch 5/107\n","1/1 [==============================] - 0s 164ms/step - loss: 1.6587 - accuracy: 0.9067\n","Epoch 6/107\n","1/1 [==============================] - 0s 201ms/step - loss: 1.6402 - accuracy: 0.9067\n","Epoch 7/107\n","1/1 [==============================] - 0s 175ms/step - loss: 1.6179 - accuracy: 0.9067\n","Epoch 8/107\n","1/1 [==============================] - 0s 182ms/step - loss: 1.5941 - accuracy: 0.9067\n","Epoch 9/107\n","1/1 [==============================] - 0s 197ms/step - loss: 1.5690 - accuracy: 0.9067\n","Epoch 10/107\n","1/1 [==============================] - 0s 161ms/step - loss: 1.5439 - accuracy: 0.9067\n","Epoch 11/107\n","1/1 [==============================] - 0s 193ms/step - loss: 1.5388 - accuracy: 0.9067\n","Epoch 12/107\n","1/1 [==============================] - 0s 165ms/step - loss: 1.5134 - accuracy: 0.9067\n","Epoch 13/107\n","1/1 [==============================] - 0s 202ms/step - loss: 1.4932 - accuracy: 0.9067\n","Epoch 14/107\n","1/1 [==============================] - 0s 131ms/step - loss: 1.4763 - accuracy: 0.9067\n","Epoch 15/107\n","1/1 [==============================] - 0s 176ms/step - loss: 1.4625 - accuracy: 0.9067\n","Epoch 16/107\n","1/1 [==============================] - 0s 167ms/step - loss: 1.4519 - accuracy: 0.9067\n","Epoch 17/107\n","1/1 [==============================] - 0s 133ms/step - loss: 1.4419 - accuracy: 0.9067\n","Epoch 18/107\n","1/1 [==============================] - 0s 138ms/step - loss: 1.4330 - accuracy: 0.9067\n","Epoch 19/107\n","1/1 [==============================] - 0s 137ms/step - loss: 1.4221 - accuracy: 0.9067\n","Epoch 20/107\n","1/1 [==============================] - 0s 137ms/step - loss: 1.4060 - accuracy: 0.9067\n","Epoch 21/107\n","1/1 [==============================] - 0s 133ms/step - loss: 1.3914 - accuracy: 0.9067\n","Epoch 22/107\n","1/1 [==============================] - 0s 139ms/step - loss: 1.3774 - accuracy: 0.9067\n","Epoch 23/107\n","1/1 [==============================] - 0s 149ms/step - loss: 1.3637 - accuracy: 0.9067\n","Epoch 24/107\n","1/1 [==============================] - 0s 132ms/step - loss: 1.3502 - accuracy: 0.9067\n","Epoch 25/107\n","1/1 [==============================] - 0s 141ms/step - loss: 1.3379 - accuracy: 0.9067\n","Epoch 26/107\n","1/1 [==============================] - 0s 129ms/step - loss: 1.3264 - accuracy: 0.9067\n","Epoch 27/107\n","1/1 [==============================] - 0s 145ms/step - loss: 1.3138 - accuracy: 0.9067\n","Epoch 28/107\n","1/1 [==============================] - 0s 162ms/step - loss: 1.3024 - accuracy: 0.9067\n","Epoch 29/107\n","1/1 [==============================] - 0s 158ms/step - loss: 1.2930 - accuracy: 0.9067\n","Epoch 30/107\n","1/1 [==============================] - 0s 131ms/step - loss: 1.2825 - accuracy: 0.9067\n","Epoch 31/107\n","1/1 [==============================] - 0s 142ms/step - loss: 1.2709 - accuracy: 0.9067\n","Epoch 32/107\n","1/1 [==============================] - 0s 144ms/step - loss: 1.2601 - accuracy: 0.9067\n","Epoch 33/107\n","1/1 [==============================] - 0s 144ms/step - loss: 1.2499 - accuracy: 0.9067\n","Epoch 34/107\n","1/1 [==============================] - 0s 128ms/step - loss: 1.2398 - accuracy: 0.9067\n","Epoch 35/107\n","1/1 [==============================] - 0s 179ms/step - loss: 1.2295 - accuracy: 0.9067\n","Epoch 36/107\n","1/1 [==============================] - 0s 157ms/step - loss: 1.2187 - accuracy: 0.9067\n","Epoch 37/107\n","1/1 [==============================] - 0s 144ms/step - loss: 1.2074 - accuracy: 0.9067\n","Epoch 38/107\n","1/1 [==============================] - 0s 159ms/step - loss: 1.1958 - accuracy: 0.9067\n","Epoch 39/107\n","1/1 [==============================] - 0s 272ms/step - loss: 1.1842 - accuracy: 0.9067\n","Epoch 40/107\n","1/1 [==============================] - 0s 152ms/step - loss: 1.1723 - accuracy: 0.9067\n","Epoch 41/107\n","1/1 [==============================] - 0s 161ms/step - loss: 1.1607 - accuracy: 0.9067\n","Epoch 42/107\n","1/1 [==============================] - 0s 166ms/step - loss: 1.1493 - accuracy: 0.9067\n","Epoch 43/107\n","1/1 [==============================] - 0s 153ms/step - loss: 1.1384 - accuracy: 0.9067\n","Epoch 44/107\n","1/1 [==============================] - 0s 160ms/step - loss: 1.1278 - accuracy: 0.9067\n","Epoch 45/107\n","1/1 [==============================] - 0s 196ms/step - loss: 1.1176 - accuracy: 0.9067\n","Epoch 46/107\n","1/1 [==============================] - 0s 160ms/step - loss: 1.1082 - accuracy: 0.9067\n","Epoch 47/107\n","1/1 [==============================] - 0s 209ms/step - loss: 1.1011 - accuracy: 0.9067\n","Epoch 48/107\n","1/1 [==============================] - 0s 137ms/step - loss: 1.1209 - accuracy: 0.9067\n","Epoch 49/107\n","1/1 [==============================] - 0s 117ms/step - loss: 1.1263 - accuracy: 0.9067\n","Epoch 50/107\n","1/1 [==============================] - 0s 153ms/step - loss: 1.1207 - accuracy: 0.9067\n","Epoch 51/107\n","1/1 [==============================] - 0s 157ms/step - loss: 1.1188 - accuracy: 0.9067\n","Epoch 52/107\n","1/1 [==============================] - 0s 125ms/step - loss: 1.1175 - accuracy: 0.9067\n","Epoch 53/107\n","1/1 [==============================] - 0s 139ms/step - loss: 1.1167 - accuracy: 0.9067\n","Epoch 54/107\n","1/1 [==============================] - 0s 140ms/step - loss: 1.1160 - accuracy: 0.9067\n","Epoch 55/107\n","1/1 [==============================] - 0s 130ms/step - loss: 1.0964 - accuracy: 0.9067\n","Epoch 56/107\n","1/1 [==============================] - 0s 135ms/step - loss: 1.0932 - accuracy: 0.9067\n","Epoch 57/107\n","1/1 [==============================] - 0s 151ms/step - loss: 1.0927 - accuracy: 0.9067\n","Epoch 58/107\n","1/1 [==============================] - 0s 133ms/step - loss: 1.0927 - accuracy: 0.9067\n","Epoch 59/107\n","1/1 [==============================] - 0s 132ms/step - loss: 1.0929 - accuracy: 0.9067\n","Epoch 60/107\n","1/1 [==============================] - 0s 145ms/step - loss: 1.0929 - accuracy: 0.9067\n","Epoch 61/107\n","1/1 [==============================] - 0s 163ms/step - loss: 1.0928 - accuracy: 0.9067\n","Epoch 62/107\n","1/1 [==============================] - 0s 140ms/step - loss: 1.0924 - accuracy: 0.9067\n","Epoch 63/107\n","1/1 [==============================] - 0s 145ms/step - loss: 1.0919 - accuracy: 0.9067\n","Epoch 64/107\n","1/1 [==============================] - 0s 149ms/step - loss: 1.0912 - accuracy: 0.9067\n","Epoch 65/107\n","1/1 [==============================] - 0s 153ms/step - loss: 1.0904 - accuracy: 0.9067\n","Epoch 66/107\n","1/1 [==============================] - 0s 249ms/step - loss: 1.0896 - accuracy: 0.9067\n","Epoch 67/107\n","1/1 [==============================] - 0s 358ms/step - loss: 1.0887 - accuracy: 0.9067\n","Epoch 68/107\n","1/1 [==============================] - 0s 313ms/step - loss: 1.0878 - accuracy: 0.9067\n","Epoch 69/107\n","1/1 [==============================] - 0s 250ms/step - loss: 1.0871 - accuracy: 0.9067\n","Epoch 70/107\n","1/1 [==============================] - 0s 273ms/step - loss: 1.0865 - accuracy: 0.9067\n","Epoch 71/107\n","1/1 [==============================] - 0s 266ms/step - loss: 1.0861 - accuracy: 0.9067\n","Epoch 72/107\n","1/1 [==============================] - 0s 291ms/step - loss: 1.0858 - accuracy: 0.9067\n","Epoch 73/107\n","1/1 [==============================] - 0s 173ms/step - loss: 1.0857 - accuracy: 0.9067\n","Epoch 74/107\n","1/1 [==============================] - 0s 238ms/step - loss: 1.0856 - accuracy: 0.9067\n","Epoch 75/107\n","1/1 [==============================] - 0s 313ms/step - loss: 1.0854 - accuracy: 0.9067\n","Epoch 76/107\n","1/1 [==============================] - 0s 262ms/step - loss: 1.0852 - accuracy: 0.9067\n","Epoch 77/107\n","1/1 [==============================] - 0s 279ms/step - loss: 1.0849 - accuracy: 0.9067\n","Epoch 78/107\n","1/1 [==============================] - 0s 198ms/step - loss: 1.0846 - accuracy: 0.9067\n","Epoch 79/107\n","1/1 [==============================] - 0s 176ms/step - loss: 1.0842 - accuracy: 0.9067\n","Epoch 80/107\n","1/1 [==============================] - 0s 154ms/step - loss: 1.0838 - accuracy: 0.9067\n","Epoch 81/107\n","1/1 [==============================] - 0s 162ms/step - loss: 1.0835 - accuracy: 0.9067\n","Epoch 82/107\n","1/1 [==============================] - 0s 124ms/step - loss: 1.0831 - accuracy: 0.9067\n","Epoch 83/107\n","1/1 [==============================] - 0s 335ms/step - loss: 1.0829 - accuracy: 0.9067\n","Epoch 84/107\n","1/1 [==============================] - 0s 287ms/step - loss: 1.0826 - accuracy: 0.9067\n","Epoch 85/107\n","1/1 [==============================] - 0s 276ms/step - loss: 1.0824 - accuracy: 0.9067\n","Epoch 86/107\n","1/1 [==============================] - 0s 254ms/step - loss: 1.0822 - accuracy: 0.9067\n","Epoch 87/107\n","1/1 [==============================] - 0s 148ms/step - loss: 1.0820 - accuracy: 0.9067\n","Epoch 88/107\n","1/1 [==============================] - 0s 215ms/step - loss: 1.0818 - accuracy: 0.9067\n","Epoch 89/107\n","1/1 [==============================] - 0s 113ms/step - loss: 1.0816 - accuracy: 0.9067\n","Epoch 90/107\n","1/1 [==============================] - 0s 209ms/step - loss: 1.0814 - accuracy: 0.9067\n","Epoch 91/107\n","1/1 [==============================] - 0s 136ms/step - loss: 1.0812 - accuracy: 0.9067\n","Epoch 92/107\n","1/1 [==============================] - 0s 245ms/step - loss: 1.0809 - accuracy: 0.9067\n","Epoch 93/107\n","1/1 [==============================] - 0s 142ms/step - loss: 1.0807 - accuracy: 0.9067\n","Epoch 94/107\n","1/1 [==============================] - 0s 195ms/step - loss: 1.0805 - accuracy: 0.9067\n","Epoch 95/107\n","1/1 [==============================] - 0s 123ms/step - loss: 1.0802 - accuracy: 0.9067\n","Epoch 96/107\n","1/1 [==============================] - 0s 203ms/step - loss: 1.0800 - accuracy: 0.9067\n","Epoch 97/107\n","1/1 [==============================] - 0s 173ms/step - loss: 1.0798 - accuracy: 0.9067\n","Epoch 98/107\n","1/1 [==============================] - 0s 232ms/step - loss: 1.0796 - accuracy: 0.9067\n","Epoch 99/107\n","1/1 [==============================] - 0s 280ms/step - loss: 1.0794 - accuracy: 0.9067\n","Epoch 100/107\n","1/1 [==============================] - 0s 283ms/step - loss: 1.0792 - accuracy: 0.9067\n","Epoch 101/107\n","1/1 [==============================] - 0s 133ms/step - loss: 1.0791 - accuracy: 0.9067\n","Epoch 102/107\n","1/1 [==============================] - 0s 180ms/step - loss: 1.0789 - accuracy: 0.9067\n","Epoch 103/107\n","1/1 [==============================] - 0s 153ms/step - loss: 1.0787 - accuracy: 0.9067\n","Epoch 104/107\n","1/1 [==============================] - 0s 194ms/step - loss: 1.0785 - accuracy: 0.9067\n","Epoch 105/107\n","1/1 [==============================] - 0s 201ms/step - loss: 1.0783 - accuracy: 0.9067\n","Epoch 106/107\n","1/1 [==============================] - 0s 121ms/step - loss: 1.0781 - accuracy: 0.9067\n","Epoch 107/107\n","1/1 [==============================] - 0s 175ms/step - loss: 1.0779 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b11e22ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1137 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:08:26,648]\u001b[0m Trial 42 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 73, 'num_epochs': 107}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/115\n","1/1 [==============================] - 6s 6s/step - loss: 2.8315 - accuracy: 0.1131\n","Epoch 2/115\n","1/1 [==============================] - 0s 123ms/step - loss: 1.1289 - accuracy: 0.9067\n","Epoch 3/115\n","1/1 [==============================] - 0s 175ms/step - loss: 1.0132 - accuracy: 0.9067\n","Epoch 4/115\n","1/1 [==============================] - 0s 180ms/step - loss: 0.9609 - accuracy: 0.9067\n","Epoch 5/115\n","1/1 [==============================] - 0s 179ms/step - loss: 0.8719 - accuracy: 0.9067\n","Epoch 6/115\n","1/1 [==============================] - 0s 173ms/step - loss: 0.7229 - accuracy: 0.9067\n","Epoch 7/115\n","1/1 [==============================] - 0s 193ms/step - loss: 0.7469 - accuracy: 0.9067\n","Epoch 8/115\n","1/1 [==============================] - 0s 173ms/step - loss: 0.7350 - accuracy: 0.9067\n","Epoch 9/115\n","1/1 [==============================] - 0s 219ms/step - loss: 0.7692 - accuracy: 0.9067\n","Epoch 10/115\n","1/1 [==============================] - 0s 194ms/step - loss: 0.7969 - accuracy: 0.9067\n","Epoch 11/115\n","1/1 [==============================] - 0s 241ms/step - loss: 0.8130 - accuracy: 0.9067\n","Epoch 12/115\n","1/1 [==============================] - 0s 219ms/step - loss: 0.8182 - accuracy: 0.9067\n","Epoch 13/115\n","1/1 [==============================] - 0s 195ms/step - loss: 0.8148 - accuracy: 0.9067\n","Epoch 14/115\n","1/1 [==============================] - 0s 201ms/step - loss: 0.8030 - accuracy: 0.9067\n","Epoch 15/115\n","1/1 [==============================] - 0s 161ms/step - loss: 0.7843 - accuracy: 0.9067\n","Epoch 16/115\n","1/1 [==============================] - 0s 419ms/step - loss: 0.7609 - accuracy: 0.9067\n","Epoch 17/115\n","1/1 [==============================] - 0s 306ms/step - loss: 0.7363 - accuracy: 0.9067\n","Epoch 18/115\n","1/1 [==============================] - 0s 156ms/step - loss: 0.7136 - accuracy: 0.9067\n","Epoch 19/115\n","1/1 [==============================] - 0s 256ms/step - loss: 0.6952 - accuracy: 0.9067\n","Epoch 20/115\n","1/1 [==============================] - 0s 89ms/step - loss: 0.6818 - accuracy: 0.9067\n","Epoch 21/115\n","1/1 [==============================] - 0s 76ms/step - loss: 0.6744 - accuracy: 0.9067\n","Epoch 22/115\n","1/1 [==============================] - 0s 81ms/step - loss: 0.6975 - accuracy: 0.9067\n","Epoch 23/115\n","1/1 [==============================] - 0s 75ms/step - loss: 0.7460 - accuracy: 0.9067\n","Epoch 24/115\n","1/1 [==============================] - 0s 77ms/step - loss: 0.7493 - accuracy: 0.9067\n","Epoch 25/115\n","1/1 [==============================] - 0s 79ms/step - loss: 0.7579 - accuracy: 0.9067\n","Epoch 26/115\n","1/1 [==============================] - 0s 93ms/step - loss: 0.7563 - accuracy: 0.9067\n","Epoch 27/115\n","1/1 [==============================] - 0s 119ms/step - loss: 0.7481 - accuracy: 0.9067\n","Epoch 28/115\n","1/1 [==============================] - 0s 97ms/step - loss: 0.7434 - accuracy: 0.9067\n","Epoch 29/115\n","1/1 [==============================] - 0s 90ms/step - loss: 0.7407 - accuracy: 0.9067\n","Epoch 30/115\n","1/1 [==============================] - 0s 88ms/step - loss: 0.7391 - accuracy: 0.9067\n","Epoch 31/115\n","1/1 [==============================] - 0s 96ms/step - loss: 0.7379 - accuracy: 0.9067\n","Epoch 32/115\n","1/1 [==============================] - 0s 112ms/step - loss: 0.7150 - accuracy: 0.9067\n","Epoch 33/115\n","1/1 [==============================] - 0s 135ms/step - loss: 0.6891 - accuracy: 0.9067\n","Epoch 34/115\n","1/1 [==============================] - 0s 126ms/step - loss: 0.6866 - accuracy: 0.9067\n","Epoch 35/115\n","1/1 [==============================] - 0s 134ms/step - loss: 0.6671 - accuracy: 0.9067\n","Epoch 36/115\n","1/1 [==============================] - 0s 103ms/step - loss: 0.6651 - accuracy: 0.9067\n","Epoch 37/115\n","1/1 [==============================] - 0s 99ms/step - loss: 0.6707 - accuracy: 0.9067\n","Epoch 38/115\n","1/1 [==============================] - 0s 97ms/step - loss: 0.6762 - accuracy: 0.9067\n","Epoch 39/115\n","1/1 [==============================] - 0s 96ms/step - loss: 0.6803 - accuracy: 0.9067\n","Epoch 40/115\n","1/1 [==============================] - 0s 128ms/step - loss: 0.6824 - accuracy: 0.9067\n","Epoch 41/115\n","1/1 [==============================] - 0s 107ms/step - loss: 0.6824 - accuracy: 0.9067\n","Epoch 42/115\n","1/1 [==============================] - 0s 110ms/step - loss: 0.6624 - accuracy: 0.9067\n","Epoch 43/115\n","1/1 [==============================] - 0s 103ms/step - loss: 0.6561 - accuracy: 0.9067\n","Epoch 44/115\n","1/1 [==============================] - 0s 234ms/step - loss: 0.6620 - accuracy: 0.9067\n","Epoch 45/115\n","1/1 [==============================] - 0s 175ms/step - loss: 0.6635 - accuracy: 0.9067\n","Epoch 46/115\n","1/1 [==============================] - 0s 114ms/step - loss: 0.6578 - accuracy: 0.9067\n","Epoch 47/115\n","1/1 [==============================] - 0s 100ms/step - loss: 0.6481 - accuracy: 0.9067\n","Epoch 48/115\n","1/1 [==============================] - 0s 105ms/step - loss: 0.6376 - accuracy: 0.9067\n","Epoch 49/115\n","1/1 [==============================] - 0s 116ms/step - loss: 0.6282 - accuracy: 0.9067\n","Epoch 50/115\n","1/1 [==============================] - 0s 102ms/step - loss: 0.6215 - accuracy: 0.9067\n","Epoch 51/115\n","1/1 [==============================] - 0s 89ms/step - loss: 0.6182 - accuracy: 0.9067\n","Epoch 52/115\n","1/1 [==============================] - 0s 104ms/step - loss: 0.6182 - accuracy: 0.9067\n","Epoch 53/115\n","1/1 [==============================] - 0s 95ms/step - loss: 0.6198 - accuracy: 0.9067\n","Epoch 54/115\n","1/1 [==============================] - 0s 90ms/step - loss: 0.6139 - accuracy: 0.9067\n","Epoch 55/115\n","1/1 [==============================] - 0s 98ms/step - loss: 0.6100 - accuracy: 0.9067\n","Epoch 56/115\n","1/1 [==============================] - 0s 99ms/step - loss: 0.6078 - accuracy: 0.9087\n","Epoch 57/115\n","1/1 [==============================] - 0s 103ms/step - loss: 0.6069 - accuracy: 0.9087\n","Epoch 58/115\n","1/1 [==============================] - 0s 129ms/step - loss: 0.6067 - accuracy: 0.9107\n","Epoch 59/115\n","1/1 [==============================] - 0s 117ms/step - loss: 0.6066 - accuracy: 0.9127\n","Epoch 60/115\n","1/1 [==============================] - 0s 112ms/step - loss: 0.6062 - accuracy: 0.9127\n","Epoch 61/115\n","1/1 [==============================] - 0s 148ms/step - loss: 0.6055 - accuracy: 0.9147\n","Epoch 62/115\n","1/1 [==============================] - 0s 141ms/step - loss: 0.6044 - accuracy: 0.9167\n","Epoch 63/115\n","1/1 [==============================] - 0s 92ms/step - loss: 0.6029 - accuracy: 0.9187\n","Epoch 64/115\n","1/1 [==============================] - 0s 98ms/step - loss: 0.6011 - accuracy: 0.9187\n","Epoch 65/115\n","1/1 [==============================] - 0s 111ms/step - loss: 0.5991 - accuracy: 0.9187\n","Epoch 66/115\n","1/1 [==============================] - 0s 151ms/step - loss: 0.5974 - accuracy: 0.9187\n","Epoch 67/115\n","1/1 [==============================] - 0s 149ms/step - loss: 0.5960 - accuracy: 0.9206\n","Epoch 68/115\n","1/1 [==============================] - 0s 153ms/step - loss: 0.5949 - accuracy: 0.9206\n","Epoch 69/115\n","1/1 [==============================] - 0s 186ms/step - loss: 0.5939 - accuracy: 0.9206\n","Epoch 70/115\n","1/1 [==============================] - 0s 162ms/step - loss: 0.5929 - accuracy: 0.9206\n","Epoch 71/115\n","1/1 [==============================] - 0s 157ms/step - loss: 0.5918 - accuracy: 0.9226\n","Epoch 72/115\n","1/1 [==============================] - 0s 141ms/step - loss: 0.5905 - accuracy: 0.9226\n","Epoch 73/115\n","1/1 [==============================] - 0s 116ms/step - loss: 0.5890 - accuracy: 0.9226\n","Epoch 74/115\n","1/1 [==============================] - 0s 136ms/step - loss: 0.5875 - accuracy: 0.9226\n","Epoch 75/115\n","1/1 [==============================] - 0s 142ms/step - loss: 0.5860 - accuracy: 0.9226\n","Epoch 76/115\n","1/1 [==============================] - 0s 151ms/step - loss: 0.5846 - accuracy: 0.9246\n","Epoch 77/115\n","1/1 [==============================] - 0s 150ms/step - loss: 0.5835 - accuracy: 0.9226\n","Epoch 78/115\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5827 - accuracy: 0.9246\n","Epoch 79/115\n","1/1 [==============================] - 0s 110ms/step - loss: 0.5816 - accuracy: 0.9246\n","Epoch 80/115\n","1/1 [==============================] - 0s 106ms/step - loss: 0.5798 - accuracy: 0.9246\n","Epoch 81/115\n","1/1 [==============================] - 0s 102ms/step - loss: 0.5779 - accuracy: 0.9246\n","Epoch 82/115\n","1/1 [==============================] - 0s 128ms/step - loss: 0.5761 - accuracy: 0.9266\n","Epoch 83/115\n","1/1 [==============================] - 0s 136ms/step - loss: 0.5746 - accuracy: 0.9266\n","Epoch 84/115\n","1/1 [==============================] - 0s 109ms/step - loss: 0.5730 - accuracy: 0.9266\n","Epoch 85/115\n","1/1 [==============================] - 0s 115ms/step - loss: 0.5710 - accuracy: 0.9266\n","Epoch 86/115\n","1/1 [==============================] - 0s 130ms/step - loss: 0.5687 - accuracy: 0.9266\n","Epoch 87/115\n","1/1 [==============================] - 0s 139ms/step - loss: 0.5664 - accuracy: 0.9286\n","Epoch 88/115\n","1/1 [==============================] - 0s 139ms/step - loss: 0.5647 - accuracy: 0.9286\n","Epoch 89/115\n","1/1 [==============================] - 0s 110ms/step - loss: 0.5367 - accuracy: 0.9286\n","Epoch 90/115\n","1/1 [==============================] - 0s 108ms/step - loss: 0.5314 - accuracy: 0.9286\n","Epoch 91/115\n","1/1 [==============================] - 0s 120ms/step - loss: nan - accuracy: 0.9286\n","Epoch 92/115\n","1/1 [==============================] - 0s 113ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/115\n","1/1 [==============================] - 0s 145ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/115\n","1/1 [==============================] - 0s 144ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/115\n","1/1 [==============================] - 0s 130ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/115\n","1/1 [==============================] - 0s 108ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/115\n","1/1 [==============================] - 0s 134ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/115\n","1/1 [==============================] - 0s 143ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/115\n","1/1 [==============================] - 0s 120ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/115\n","1/1 [==============================] - 0s 137ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/115\n","1/1 [==============================] - 0s 121ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/115\n","1/1 [==============================] - 0s 118ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/115\n","1/1 [==============================] - 0s 116ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/115\n","1/1 [==============================] - 0s 133ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/115\n","1/1 [==============================] - 0s 128ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/115\n","1/1 [==============================] - 0s 127ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/115\n","1/1 [==============================] - 0s 148ms/step - loss: nan - accuracy: 0.9067\n","Epoch 108/115\n","1/1 [==============================] - 0s 147ms/step - loss: nan - accuracy: 0.9067\n","Epoch 109/115\n","1/1 [==============================] - 0s 142ms/step - loss: nan - accuracy: 0.9067\n","Epoch 110/115\n","1/1 [==============================] - 0s 152ms/step - loss: nan - accuracy: 0.9067\n","Epoch 111/115\n","1/1 [==============================] - 0s 146ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/115\n","1/1 [==============================] - 0s 139ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/115\n","1/1 [==============================] - 0s 139ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/115\n","1/1 [==============================] - 0s 154ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/115\n","1/1 [==============================] - 0s 146ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b02660670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:08:50,519]\u001b[0m Trial 43 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 112, 'num_epochs': 115}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/139\n","1/1 [==============================] - 6s 6s/step - loss: 3.7092 - accuracy: 0.0218\n","Epoch 2/139\n","1/1 [==============================] - 0s 127ms/step - loss: 2.1172 - accuracy: 0.3294\n","Epoch 3/139\n","1/1 [==============================] - 0s 167ms/step - loss: 1.8073 - accuracy: 0.8909\n","Epoch 4/139\n","1/1 [==============================] - 0s 206ms/step - loss: 1.6113 - accuracy: 0.9067\n","Epoch 5/139\n","1/1 [==============================] - 0s 88ms/step - loss: 1.4577 - accuracy: 0.9067\n","Epoch 6/139\n","1/1 [==============================] - 0s 103ms/step - loss: 1.3275 - accuracy: 0.9067\n","Epoch 7/139\n","1/1 [==============================] - 0s 141ms/step - loss: 1.2378 - accuracy: 0.9067\n","Epoch 8/139\n","1/1 [==============================] - 0s 185ms/step - loss: 1.1599 - accuracy: 0.9067\n","Epoch 9/139\n","1/1 [==============================] - 0s 174ms/step - loss: 1.0880 - accuracy: 0.9067\n","Epoch 10/139\n","1/1 [==============================] - 0s 194ms/step - loss: 1.0022 - accuracy: 0.9067\n","Epoch 11/139\n","1/1 [==============================] - 0s 213ms/step - loss: 0.9458 - accuracy: 0.9067\n","Epoch 12/139\n","1/1 [==============================] - 0s 159ms/step - loss: 0.9171 - accuracy: 0.9067\n","Epoch 13/139\n","1/1 [==============================] - 0s 153ms/step - loss: 0.8856 - accuracy: 0.9067\n","Epoch 14/139\n","1/1 [==============================] - 0s 206ms/step - loss: 0.8969 - accuracy: 0.9067\n","Epoch 15/139\n","1/1 [==============================] - 0s 113ms/step - loss: 0.9262 - accuracy: 0.9067\n","Epoch 16/139\n","1/1 [==============================] - 0s 144ms/step - loss: 0.9304 - accuracy: 0.9067\n","Epoch 17/139\n","1/1 [==============================] - 0s 148ms/step - loss: 0.9697 - accuracy: 0.9067\n","Epoch 18/139\n","1/1 [==============================] - 0s 177ms/step - loss: 0.9653 - accuracy: 0.9067\n","Epoch 19/139\n","1/1 [==============================] - 0s 170ms/step - loss: 0.9898 - accuracy: 0.9067\n","Epoch 20/139\n","1/1 [==============================] - 0s 135ms/step - loss: 0.9925 - accuracy: 0.9067\n","Epoch 21/139\n","1/1 [==============================] - 0s 129ms/step - loss: 0.9652 - accuracy: 0.9067\n","Epoch 22/139\n","1/1 [==============================] - 0s 138ms/step - loss: 0.9577 - accuracy: 0.9067\n","Epoch 23/139\n","1/1 [==============================] - 0s 147ms/step - loss: 0.9337 - accuracy: 0.9067\n","Epoch 24/139\n","1/1 [==============================] - 0s 146ms/step - loss: 0.8852 - accuracy: 0.9067\n","Epoch 25/139\n","1/1 [==============================] - 0s 159ms/step - loss: 0.8572 - accuracy: 0.9067\n","Epoch 26/139\n","1/1 [==============================] - 0s 129ms/step - loss: 0.8317 - accuracy: 0.9067\n","Epoch 27/139\n","1/1 [==============================] - 0s 128ms/step - loss: 0.8300 - accuracy: 0.9067\n","Epoch 28/139\n","1/1 [==============================] - 0s 237ms/step - loss: 0.8284 - accuracy: 0.9067\n","Epoch 29/139\n","1/1 [==============================] - 0s 167ms/step - loss: 0.8264 - accuracy: 0.9067\n","Epoch 30/139\n","1/1 [==============================] - 0s 222ms/step - loss: 0.8237 - accuracy: 0.9067\n","Epoch 31/139\n","1/1 [==============================] - 0s 181ms/step - loss: 0.8206 - accuracy: 0.9067\n","Epoch 32/139\n","1/1 [==============================] - 0s 182ms/step - loss: 0.8178 - accuracy: 0.9067\n","Epoch 33/139\n","1/1 [==============================] - 0s 142ms/step - loss: 0.8155 - accuracy: 0.9067\n","Epoch 34/139\n","1/1 [==============================] - 0s 186ms/step - loss: 0.8140 - accuracy: 0.9067\n","Epoch 35/139\n","1/1 [==============================] - 0s 191ms/step - loss: 0.8136 - accuracy: 0.9067\n","Epoch 36/139\n","1/1 [==============================] - 0s 191ms/step - loss: 0.8131 - accuracy: 0.9067\n","Epoch 37/139\n","1/1 [==============================] - 0s 188ms/step - loss: 0.8103 - accuracy: 0.9067\n","Epoch 38/139\n","1/1 [==============================] - 0s 149ms/step - loss: 0.8088 - accuracy: 0.9067\n","Epoch 39/139\n","1/1 [==============================] - 0s 152ms/step - loss: 0.8078 - accuracy: 0.9067\n","Epoch 40/139\n","1/1 [==============================] - 0s 148ms/step - loss: 0.8069 - accuracy: 0.9067\n","Epoch 41/139\n","1/1 [==============================] - 0s 169ms/step - loss: 0.8060 - accuracy: 0.9067\n","Epoch 42/139\n","1/1 [==============================] - 0s 132ms/step - loss: 0.8051 - accuracy: 0.9067\n","Epoch 43/139\n","1/1 [==============================] - 0s 153ms/step - loss: 0.8041 - accuracy: 0.9067\n","Epoch 44/139\n","1/1 [==============================] - 0s 126ms/step - loss: 0.8031 - accuracy: 0.9067\n","Epoch 45/139\n","1/1 [==============================] - 0s 176ms/step - loss: 0.8021 - accuracy: 0.9067\n","Epoch 46/139\n","1/1 [==============================] - 0s 143ms/step - loss: 0.8013 - accuracy: 0.9067\n","Epoch 47/139\n","1/1 [==============================] - 0s 133ms/step - loss: 0.8006 - accuracy: 0.9067\n","Epoch 48/139\n","1/1 [==============================] - 0s 142ms/step - loss: 0.8000 - accuracy: 0.9067\n","Epoch 49/139\n","1/1 [==============================] - 0s 249ms/step - loss: 0.7995 - accuracy: 0.9067\n","Epoch 50/139\n","1/1 [==============================] - 0s 128ms/step - loss: 0.7991 - accuracy: 0.9067\n","Epoch 51/139\n","1/1 [==============================] - 0s 140ms/step - loss: 0.7986 - accuracy: 0.9067\n","Epoch 52/139\n","1/1 [==============================] - 0s 138ms/step - loss: 0.7980 - accuracy: 0.9067\n","Epoch 53/139\n","1/1 [==============================] - 0s 125ms/step - loss: 0.7974 - accuracy: 0.9067\n","Epoch 54/139\n","1/1 [==============================] - 0s 137ms/step - loss: 0.7968 - accuracy: 0.9067\n","Epoch 55/139\n","1/1 [==============================] - 0s 123ms/step - loss: 0.7962 - accuracy: 0.9067\n","Epoch 56/139\n","1/1 [==============================] - 0s 129ms/step - loss: 0.7957 - accuracy: 0.9067\n","Epoch 57/139\n","1/1 [==============================] - 0s 136ms/step - loss: 0.7952 - accuracy: 0.9067\n","Epoch 58/139\n","1/1 [==============================] - 0s 131ms/step - loss: 0.7948 - accuracy: 0.9067\n","Epoch 59/139\n","1/1 [==============================] - 0s 157ms/step - loss: 0.7943 - accuracy: 0.9067\n","Epoch 60/139\n","1/1 [==============================] - 0s 121ms/step - loss: 0.7939 - accuracy: 0.9067\n","Epoch 61/139\n","1/1 [==============================] - 0s 134ms/step - loss: 0.7935 - accuracy: 0.9067\n","Epoch 62/139\n","1/1 [==============================] - 0s 177ms/step - loss: 0.7931 - accuracy: 0.9067\n","Epoch 63/139\n","1/1 [==============================] - 0s 154ms/step - loss: 0.7927 - accuracy: 0.9067\n","Epoch 64/139\n","1/1 [==============================] - 0s 154ms/step - loss: 0.7923 - accuracy: 0.9067\n","Epoch 65/139\n","1/1 [==============================] - 0s 176ms/step - loss: 0.7919 - accuracy: 0.9067\n","Epoch 66/139\n","1/1 [==============================] - 0s 223ms/step - loss: 0.7915 - accuracy: 0.9067\n","Epoch 67/139\n","1/1 [==============================] - 0s 152ms/step - loss: 0.7912 - accuracy: 0.9067\n","Epoch 68/139\n","1/1 [==============================] - 0s 171ms/step - loss: 0.7909 - accuracy: 0.9067\n","Epoch 69/139\n","1/1 [==============================] - 0s 139ms/step - loss: 0.7905 - accuracy: 0.9067\n","Epoch 70/139\n","1/1 [==============================] - 0s 133ms/step - loss: 0.7902 - accuracy: 0.9067\n","Epoch 71/139\n","1/1 [==============================] - 0s 124ms/step - loss: 0.7899 - accuracy: 0.9067\n","Epoch 72/139\n","1/1 [==============================] - 0s 149ms/step - loss: 0.7896 - accuracy: 0.9067\n","Epoch 73/139\n","1/1 [==============================] - 0s 139ms/step - loss: 0.7892 - accuracy: 0.9067\n","Epoch 74/139\n","1/1 [==============================] - 0s 133ms/step - loss: 0.7889 - accuracy: 0.9067\n","Epoch 75/139\n","1/1 [==============================] - 0s 143ms/step - loss: 0.7886 - accuracy: 0.9067\n","Epoch 76/139\n","1/1 [==============================] - 0s 168ms/step - loss: 0.7883 - accuracy: 0.9067\n","Epoch 77/139\n","1/1 [==============================] - 0s 181ms/step - loss: 0.7880 - accuracy: 0.9067\n","Epoch 78/139\n","1/1 [==============================] - 0s 328ms/step - loss: 0.7877 - accuracy: 0.9067\n","Epoch 79/139\n","1/1 [==============================] - 0s 248ms/step - loss: 0.7874 - accuracy: 0.9067\n","Epoch 80/139\n","1/1 [==============================] - 0s 163ms/step - loss: 0.7871 - accuracy: 0.9067\n","Epoch 81/139\n","1/1 [==============================] - 0s 164ms/step - loss: 0.7868 - accuracy: 0.9067\n","Epoch 82/139\n","1/1 [==============================] - 0s 175ms/step - loss: 0.7865 - accuracy: 0.9067\n","Epoch 83/139\n","1/1 [==============================] - 0s 165ms/step - loss: 0.7862 - accuracy: 0.9067\n","Epoch 84/139\n","1/1 [==============================] - 0s 172ms/step - loss: 0.7859 - accuracy: 0.9067\n","Epoch 85/139\n","1/1 [==============================] - 0s 155ms/step - loss: 0.7857 - accuracy: 0.9067\n","Epoch 86/139\n","1/1 [==============================] - 0s 165ms/step - loss: 0.7854 - accuracy: 0.9067\n","Epoch 87/139\n","1/1 [==============================] - 0s 125ms/step - loss: 0.7851 - accuracy: 0.9067\n","Epoch 88/139\n","1/1 [==============================] - 0s 130ms/step - loss: 0.7848 - accuracy: 0.9067\n","Epoch 89/139\n","1/1 [==============================] - 0s 122ms/step - loss: 0.7846 - accuracy: 0.9067\n","Epoch 90/139\n","1/1 [==============================] - 0s 89ms/step - loss: 0.7843 - accuracy: 0.9067\n","Epoch 91/139\n","1/1 [==============================] - 0s 268ms/step - loss: 0.7840 - accuracy: 0.9067\n","Epoch 92/139\n","1/1 [==============================] - 0s 192ms/step - loss: 0.7838 - accuracy: 0.9067\n","Epoch 93/139\n","1/1 [==============================] - 0s 195ms/step - loss: 0.7835 - accuracy: 0.9067\n","Epoch 94/139\n","1/1 [==============================] - 0s 178ms/step - loss: 0.7833 - accuracy: 0.9067\n","Epoch 95/139\n","1/1 [==============================] - 0s 152ms/step - loss: 0.7830 - accuracy: 0.9067\n","Epoch 96/139\n","1/1 [==============================] - 0s 124ms/step - loss: 0.7828 - accuracy: 0.9067\n","Epoch 97/139\n","1/1 [==============================] - 0s 135ms/step - loss: 0.7825 - accuracy: 0.9067\n","Epoch 98/139\n","1/1 [==============================] - 0s 129ms/step - loss: 0.7823 - accuracy: 0.9067\n","Epoch 99/139\n","1/1 [==============================] - 0s 166ms/step - loss: 0.7820 - accuracy: 0.9067\n","Epoch 100/139\n","1/1 [==============================] - 0s 173ms/step - loss: 0.7818 - accuracy: 0.9067\n","Epoch 101/139\n","1/1 [==============================] - 0s 162ms/step - loss: 0.7816 - accuracy: 0.9067\n","Epoch 102/139\n","1/1 [==============================] - 0s 168ms/step - loss: 0.7813 - accuracy: 0.9067\n","Epoch 103/139\n","1/1 [==============================] - 0s 175ms/step - loss: 0.7811 - accuracy: 0.9067\n","Epoch 104/139\n","1/1 [==============================] - 0s 157ms/step - loss: 0.7809 - accuracy: 0.9067\n","Epoch 105/139\n","1/1 [==============================] - 0s 141ms/step - loss: 0.7806 - accuracy: 0.9067\n","Epoch 106/139\n","1/1 [==============================] - 0s 137ms/step - loss: 0.7804 - accuracy: 0.9067\n","Epoch 107/139\n","1/1 [==============================] - 0s 174ms/step - loss: 0.7802 - accuracy: 0.9067\n","Epoch 108/139\n","1/1 [==============================] - 0s 148ms/step - loss: 0.7799 - accuracy: 0.9067\n","Epoch 109/139\n","1/1 [==============================] - 0s 173ms/step - loss: 0.7797 - accuracy: 0.9067\n","Epoch 110/139\n","1/1 [==============================] - 0s 160ms/step - loss: 0.7795 - accuracy: 0.9067\n","Epoch 111/139\n","1/1 [==============================] - 0s 136ms/step - loss: 0.7793 - accuracy: 0.9067\n","Epoch 112/139\n","1/1 [==============================] - 0s 144ms/step - loss: 0.7790 - accuracy: 0.9067\n","Epoch 113/139\n","1/1 [==============================] - 0s 126ms/step - loss: 0.7788 - accuracy: 0.9067\n","Epoch 114/139\n","1/1 [==============================] - 0s 167ms/step - loss: 0.7786 - accuracy: 0.9067\n","Epoch 115/139\n","1/1 [==============================] - 0s 179ms/step - loss: 0.7784 - accuracy: 0.9067\n","Epoch 116/139\n","1/1 [==============================] - 0s 163ms/step - loss: 0.7781 - accuracy: 0.9067\n","Epoch 117/139\n","1/1 [==============================] - 0s 211ms/step - loss: 0.7779 - accuracy: 0.9067\n","Epoch 118/139\n","1/1 [==============================] - 0s 182ms/step - loss: 0.7777 - accuracy: 0.9067\n","Epoch 119/139\n","1/1 [==============================] - 0s 156ms/step - loss: 0.7775 - accuracy: 0.9067\n","Epoch 120/139\n","1/1 [==============================] - 0s 144ms/step - loss: 0.7773 - accuracy: 0.9067\n","Epoch 121/139\n","1/1 [==============================] - 0s 131ms/step - loss: 0.7770 - accuracy: 0.9067\n","Epoch 122/139\n","1/1 [==============================] - 0s 138ms/step - loss: 0.7768 - accuracy: 0.9067\n","Epoch 123/139\n","1/1 [==============================] - 0s 136ms/step - loss: 0.7766 - accuracy: 0.9067\n","Epoch 124/139\n","1/1 [==============================] - 0s 166ms/step - loss: 0.7764 - accuracy: 0.9067\n","Epoch 125/139\n","1/1 [==============================] - 0s 157ms/step - loss: 0.7762 - accuracy: 0.9067\n","Epoch 126/139\n","1/1 [==============================] - 0s 136ms/step - loss: 0.7760 - accuracy: 0.9067\n","Epoch 127/139\n","1/1 [==============================] - 0s 139ms/step - loss: 0.7758 - accuracy: 0.9067\n","Epoch 128/139\n","1/1 [==============================] - 0s 162ms/step - loss: 0.7756 - accuracy: 0.9067\n","Epoch 129/139\n","1/1 [==============================] - 0s 237ms/step - loss: 0.7754 - accuracy: 0.9067\n","Epoch 130/139\n","1/1 [==============================] - 0s 176ms/step - loss: 0.7751 - accuracy: 0.9067\n","Epoch 131/139\n","1/1 [==============================] - 0s 171ms/step - loss: 0.7749 - accuracy: 0.9067\n","Epoch 132/139\n","1/1 [==============================] - 0s 136ms/step - loss: 0.7747 - accuracy: 0.9067\n","Epoch 133/139\n","1/1 [==============================] - 0s 148ms/step - loss: 0.7745 - accuracy: 0.9067\n","Epoch 134/139\n","1/1 [==============================] - 0s 123ms/step - loss: 0.7743 - accuracy: 0.9067\n","Epoch 135/139\n","1/1 [==============================] - 0s 136ms/step - loss: 0.7741 - accuracy: 0.9067\n","Epoch 136/139\n","1/1 [==============================] - 0s 113ms/step - loss: 0.7739 - accuracy: 0.9067\n","Epoch 137/139\n","1/1 [==============================] - 0s 122ms/step - loss: 0.7737 - accuracy: 0.9067\n","Epoch 138/139\n","1/1 [==============================] - 0s 77ms/step - loss: 0.7735 - accuracy: 0.9067\n","Epoch 139/139\n","1/1 [==============================] - 0s 114ms/step - loss: 0.7733 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b27ae2dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1003 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:09:20,005]\u001b[0m Trial 44 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 22, 'num_epochs': 139}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/129\n","1/1 [==============================] - 6s 6s/step - loss: 11.4821 - accuracy: 0.0218\n","Epoch 2/129\n","1/1 [==============================] - 0s 112ms/step - loss: 10.7735 - accuracy: 0.0437\n","Epoch 3/129\n","1/1 [==============================] - 0s 148ms/step - loss: 7.7117 - accuracy: 0.0734\n","Epoch 4/129\n","1/1 [==============================] - 0s 175ms/step - loss: 4.1360 - accuracy: 0.1071\n","Epoch 5/129\n","1/1 [==============================] - 0s 175ms/step - loss: 2.6150 - accuracy: 0.1409\n","Epoch 6/129\n","1/1 [==============================] - 0s 205ms/step - loss: 2.3035 - accuracy: 0.2024\n","Epoch 7/129\n","1/1 [==============================] - 0s 126ms/step - loss: 2.1232 - accuracy: 0.2659\n","Epoch 8/129\n","1/1 [==============================] - 0s 168ms/step - loss: 1.9931 - accuracy: 0.3433\n","Epoch 9/129\n","1/1 [==============================] - 0s 155ms/step - loss: 1.9124 - accuracy: 0.4325\n","Epoch 10/129\n","1/1 [==============================] - 0s 174ms/step - loss: 1.8302 - accuracy: 0.5278\n","Epoch 11/129\n","1/1 [==============================] - 0s 142ms/step - loss: 1.7615 - accuracy: 0.6032\n","Epoch 12/129\n","1/1 [==============================] - 0s 148ms/step - loss: 1.7029 - accuracy: 0.6766\n","Epoch 13/129\n","1/1 [==============================] - 0s 119ms/step - loss: 1.6518 - accuracy: 0.7361\n","Epoch 14/129\n","1/1 [==============================] - 0s 117ms/step - loss: 1.6070 - accuracy: 0.7817\n","Epoch 15/129\n","1/1 [==============================] - 0s 144ms/step - loss: 1.5677 - accuracy: 0.8155\n","Epoch 16/129\n","1/1 [==============================] - 0s 117ms/step - loss: 1.5355 - accuracy: 0.8571\n","Epoch 17/129\n","1/1 [==============================] - 0s 115ms/step - loss: 1.5270 - accuracy: 0.8869\n","Epoch 18/129\n","1/1 [==============================] - 0s 130ms/step - loss: 1.5008 - accuracy: 0.8948\n","Epoch 19/129\n","1/1 [==============================] - 0s 127ms/step - loss: 1.4780 - accuracy: 0.8968\n","Epoch 20/129\n","1/1 [==============================] - 0s 152ms/step - loss: 1.4578 - accuracy: 0.8988\n","Epoch 21/129\n","1/1 [==============================] - 0s 113ms/step - loss: 1.4373 - accuracy: 0.8988\n","Epoch 22/129\n","1/1 [==============================] - 0s 145ms/step - loss: 1.4172 - accuracy: 0.9008\n","Epoch 23/129\n","1/1 [==============================] - 0s 157ms/step - loss: 1.3988 - accuracy: 0.9028\n","Epoch 24/129\n","1/1 [==============================] - 0s 156ms/step - loss: 1.3822 - accuracy: 0.9028\n","Epoch 25/129\n","1/1 [==============================] - 0s 144ms/step - loss: 1.3668 - accuracy: 0.9028\n","Epoch 26/129\n","1/1 [==============================] - 0s 164ms/step - loss: 1.3518 - accuracy: 0.9028\n","Epoch 27/129\n","1/1 [==============================] - 0s 126ms/step - loss: 1.3372 - accuracy: 0.9028\n","Epoch 28/129\n","1/1 [==============================] - 0s 142ms/step - loss: 1.3051 - accuracy: 0.9048\n","Epoch 29/129\n","1/1 [==============================] - 0s 166ms/step - loss: 1.2868 - accuracy: 0.9048\n","Epoch 30/129\n","1/1 [==============================] - 0s 202ms/step - loss: 1.2731 - accuracy: 0.9048\n","Epoch 31/129\n","1/1 [==============================] - 0s 148ms/step - loss: 1.2602 - accuracy: 0.9048\n","Epoch 32/129\n","1/1 [==============================] - 0s 169ms/step - loss: 1.2480 - accuracy: 0.9048\n","Epoch 33/129\n","1/1 [==============================] - 0s 185ms/step - loss: 1.2361 - accuracy: 0.9048\n","Epoch 34/129\n","1/1 [==============================] - 0s 167ms/step - loss: 1.2244 - accuracy: 0.9067\n","Epoch 35/129\n","1/1 [==============================] - 0s 136ms/step - loss: 1.2129 - accuracy: 0.9067\n","Epoch 36/129\n","1/1 [==============================] - 0s 128ms/step - loss: 1.2018 - accuracy: 0.9067\n","Epoch 37/129\n","1/1 [==============================] - 0s 117ms/step - loss: 1.1910 - accuracy: 0.9067\n","Epoch 38/129\n","1/1 [==============================] - 0s 139ms/step - loss: 1.1803 - accuracy: 0.9067\n","Epoch 39/129\n","1/1 [==============================] - 0s 165ms/step - loss: 1.1695 - accuracy: 0.9067\n","Epoch 40/129\n","1/1 [==============================] - 0s 134ms/step - loss: 1.1586 - accuracy: 0.9067\n","Epoch 41/129\n","1/1 [==============================] - 0s 137ms/step - loss: 1.1475 - accuracy: 0.9067\n","Epoch 42/129\n","1/1 [==============================] - 0s 141ms/step - loss: 1.1362 - accuracy: 0.9087\n","Epoch 43/129\n","1/1 [==============================] - 0s 144ms/step - loss: 1.1245 - accuracy: 0.9087\n","Epoch 44/129\n","1/1 [==============================] - 0s 152ms/step - loss: 1.1124 - accuracy: 0.9087\n","Epoch 45/129\n","1/1 [==============================] - 0s 191ms/step - loss: 1.1000 - accuracy: 0.9087\n","Epoch 46/129\n","1/1 [==============================] - 0s 185ms/step - loss: 1.0873 - accuracy: 0.9067\n","Epoch 47/129\n","1/1 [==============================] - 0s 151ms/step - loss: 1.0745 - accuracy: 0.9067\n","Epoch 48/129\n","1/1 [==============================] - 0s 130ms/step - loss: 1.0615 - accuracy: 0.9067\n","Epoch 49/129\n","1/1 [==============================] - 0s 164ms/step - loss: 1.0481 - accuracy: 0.9067\n","Epoch 50/129\n","1/1 [==============================] - 0s 126ms/step - loss: 1.0341 - accuracy: 0.9067\n","Epoch 51/129\n","1/1 [==============================] - 0s 137ms/step - loss: 1.0197 - accuracy: 0.9067\n","Epoch 52/129\n","1/1 [==============================] - 0s 163ms/step - loss: 1.0047 - accuracy: 0.9067\n","Epoch 53/129\n","1/1 [==============================] - 0s 160ms/step - loss: 0.9894 - accuracy: 0.9067\n","Epoch 54/129\n","1/1 [==============================] - 0s 160ms/step - loss: 0.9740 - accuracy: 0.9067\n","Epoch 55/129\n","1/1 [==============================] - 0s 161ms/step - loss: 0.9610 - accuracy: 0.9067\n","Epoch 56/129\n","1/1 [==============================] - 0s 136ms/step - loss: 0.9521 - accuracy: 0.9067\n","Epoch 57/129\n","1/1 [==============================] - 0s 142ms/step - loss: 0.9450 - accuracy: 0.9067\n","Epoch 58/129\n","1/1 [==============================] - 0s 118ms/step - loss: 0.9390 - accuracy: 0.9067\n","Epoch 59/129\n","1/1 [==============================] - 0s 162ms/step - loss: 0.9341 - accuracy: 0.9067\n","Epoch 60/129\n","1/1 [==============================] - 0s 149ms/step - loss: 0.9304 - accuracy: 0.9067\n","Epoch 61/129\n","1/1 [==============================] - 0s 124ms/step - loss: 0.9486 - accuracy: 0.9067\n","Epoch 62/129\n","1/1 [==============================] - 0s 152ms/step - loss: 0.9447 - accuracy: 0.9067\n","Epoch 63/129\n","1/1 [==============================] - 0s 136ms/step - loss: 0.9419 - accuracy: 0.9067\n","Epoch 64/129\n","1/1 [==============================] - 0s 124ms/step - loss: 0.9413 - accuracy: 0.9067\n","Epoch 65/129\n","1/1 [==============================] - 0s 129ms/step - loss: 0.9382 - accuracy: 0.9067\n","Epoch 66/129\n","1/1 [==============================] - 0s 115ms/step - loss: 0.9348 - accuracy: 0.9067\n","Epoch 67/129\n","1/1 [==============================] - 0s 91ms/step - loss: 0.9325 - accuracy: 0.9067\n","Epoch 68/129\n","1/1 [==============================] - 0s 98ms/step - loss: 0.9307 - accuracy: 0.9067\n","Epoch 69/129\n","1/1 [==============================] - 0s 115ms/step - loss: 0.9291 - accuracy: 0.9067\n","Epoch 70/129\n","1/1 [==============================] - 0s 158ms/step - loss: 0.9277 - accuracy: 0.9067\n","Epoch 71/129\n","1/1 [==============================] - 0s 125ms/step - loss: 0.9263 - accuracy: 0.9067\n","Epoch 72/129\n","1/1 [==============================] - 0s 135ms/step - loss: 0.9250 - accuracy: 0.9067\n","Epoch 73/129\n","1/1 [==============================] - 0s 132ms/step - loss: 0.9238 - accuracy: 0.9067\n","Epoch 74/129\n","1/1 [==============================] - 0s 140ms/step - loss: 0.9226 - accuracy: 0.9067\n","Epoch 75/129\n","1/1 [==============================] - 0s 130ms/step - loss: 0.9215 - accuracy: 0.9067\n","Epoch 76/129\n","1/1 [==============================] - 0s 135ms/step - loss: 0.9204 - accuracy: 0.9067\n","Epoch 77/129\n","1/1 [==============================] - 0s 137ms/step - loss: 0.9194 - accuracy: 0.9067\n","Epoch 78/129\n","1/1 [==============================] - 0s 165ms/step - loss: 0.9184 - accuracy: 0.9067\n","Epoch 79/129\n","1/1 [==============================] - 0s 234ms/step - loss: 0.9175 - accuracy: 0.9067\n","Epoch 80/129\n","1/1 [==============================] - 0s 103ms/step - loss: 0.9166 - accuracy: 0.9067\n","Epoch 81/129\n","1/1 [==============================] - 0s 123ms/step - loss: 0.9158 - accuracy: 0.9067\n","Epoch 82/129\n","1/1 [==============================] - 0s 146ms/step - loss: 0.9150 - accuracy: 0.9067\n","Epoch 83/129\n","1/1 [==============================] - 0s 192ms/step - loss: 0.9142 - accuracy: 0.9067\n","Epoch 84/129\n","1/1 [==============================] - 0s 272ms/step - loss: 0.9135 - accuracy: 0.9067\n","Epoch 85/129\n","1/1 [==============================] - 0s 227ms/step - loss: 0.9128 - accuracy: 0.9067\n","Epoch 86/129\n","1/1 [==============================] - 0s 144ms/step - loss: 0.9122 - accuracy: 0.9067\n","Epoch 87/129\n","1/1 [==============================] - 0s 163ms/step - loss: 0.9116 - accuracy: 0.9067\n","Epoch 88/129\n","1/1 [==============================] - 0s 130ms/step - loss: 0.9110 - accuracy: 0.9067\n","Epoch 89/129\n","1/1 [==============================] - 0s 130ms/step - loss: 0.9105 - accuracy: 0.9067\n","Epoch 90/129\n","1/1 [==============================] - 0s 132ms/step - loss: 0.9100 - accuracy: 0.9067\n","Epoch 91/129\n","1/1 [==============================] - 0s 170ms/step - loss: 0.9095 - accuracy: 0.9067\n","Epoch 92/129\n","1/1 [==============================] - 0s 250ms/step - loss: 0.9090 - accuracy: 0.9067\n","Epoch 93/129\n","1/1 [==============================] - 0s 187ms/step - loss: 0.9086 - accuracy: 0.9067\n","Epoch 94/129\n","1/1 [==============================] - 0s 110ms/step - loss: 0.9081 - accuracy: 0.9067\n","Epoch 95/129\n","1/1 [==============================] - 0s 127ms/step - loss: 0.9077 - accuracy: 0.9067\n","Epoch 96/129\n","1/1 [==============================] - 0s 134ms/step - loss: 0.9072 - accuracy: 0.9067\n","Epoch 97/129\n","1/1 [==============================] - 0s 127ms/step - loss: 0.9068 - accuracy: 0.9067\n","Epoch 98/129\n","1/1 [==============================] - 0s 121ms/step - loss: 0.9064 - accuracy: 0.9067\n","Epoch 99/129\n","1/1 [==============================] - 0s 135ms/step - loss: 0.9060 - accuracy: 0.9067\n","Epoch 100/129\n","1/1 [==============================] - 0s 115ms/step - loss: 0.9056 - accuracy: 0.9067\n","Epoch 101/129\n","1/1 [==============================] - 0s 119ms/step - loss: 0.9052 - accuracy: 0.9067\n","Epoch 102/129\n","1/1 [==============================] - 0s 119ms/step - loss: 0.9048 - accuracy: 0.9067\n","Epoch 103/129\n","1/1 [==============================] - 0s 128ms/step - loss: 0.9044 - accuracy: 0.9067\n","Epoch 104/129\n","1/1 [==============================] - 0s 91ms/step - loss: 0.9040 - accuracy: 0.9067\n","Epoch 105/129\n","1/1 [==============================] - 0s 88ms/step - loss: 0.9036 - accuracy: 0.9067\n","Epoch 106/129\n","1/1 [==============================] - 0s 156ms/step - loss: 0.9033 - accuracy: 0.9067\n","Epoch 107/129\n","1/1 [==============================] - 0s 141ms/step - loss: 0.9029 - accuracy: 0.9067\n","Epoch 108/129\n","1/1 [==============================] - 0s 123ms/step - loss: 0.9025 - accuracy: 0.9067\n","Epoch 109/129\n","1/1 [==============================] - 0s 121ms/step - loss: 0.9021 - accuracy: 0.9067\n","Epoch 110/129\n","1/1 [==============================] - 0s 123ms/step - loss: 0.9018 - accuracy: 0.9067\n","Epoch 111/129\n","1/1 [==============================] - 0s 142ms/step - loss: 0.9014 - accuracy: 0.9067\n","Epoch 112/129\n","1/1 [==============================] - 0s 164ms/step - loss: 0.9010 - accuracy: 0.9067\n","Epoch 113/129\n","1/1 [==============================] - 0s 147ms/step - loss: 0.9007 - accuracy: 0.9067\n","Epoch 114/129\n","1/1 [==============================] - 0s 144ms/step - loss: 0.9003 - accuracy: 0.9067\n","Epoch 115/129\n","1/1 [==============================] - 0s 157ms/step - loss: 0.9000 - accuracy: 0.9067\n","Epoch 116/129\n","1/1 [==============================] - 0s 150ms/step - loss: 0.8997 - accuracy: 0.9067\n","Epoch 117/129\n","1/1 [==============================] - 0s 133ms/step - loss: 0.8994 - accuracy: 0.9067\n","Epoch 118/129\n","1/1 [==============================] - 0s 145ms/step - loss: 0.8991 - accuracy: 0.9067\n","Epoch 119/129\n","1/1 [==============================] - 0s 154ms/step - loss: 0.8988 - accuracy: 0.9067\n","Epoch 120/129\n","1/1 [==============================] - 0s 151ms/step - loss: 0.8986 - accuracy: 0.9067\n","Epoch 121/129\n","1/1 [==============================] - 0s 138ms/step - loss: 0.8983 - accuracy: 0.9067\n","Epoch 122/129\n","1/1 [==============================] - 0s 147ms/step - loss: 0.8981 - accuracy: 0.9067\n","Epoch 123/129\n","1/1 [==============================] - 0s 140ms/step - loss: 0.8978 - accuracy: 0.9067\n","Epoch 124/129\n","1/1 [==============================] - 0s 148ms/step - loss: 0.8976 - accuracy: 0.9067\n","Epoch 125/129\n","1/1 [==============================] - 0s 176ms/step - loss: 0.8974 - accuracy: 0.9067\n","Epoch 126/129\n","1/1 [==============================] - 0s 163ms/step - loss: 0.8971 - accuracy: 0.9067\n","Epoch 127/129\n","1/1 [==============================] - 0s 152ms/step - loss: 0.8969 - accuracy: 0.9067\n","Epoch 128/129\n","1/1 [==============================] - 0s 147ms/step - loss: 0.8967 - accuracy: 0.9067\n","Epoch 129/129\n","1/1 [==============================] - 0s 160ms/step - loss: 0.8964 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b4544af70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1135 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:09:46,939]\u001b[0m Trial 45 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 30, 'num_epochs': 129}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/107\n","1/1 [==============================] - 6s 6s/step - loss: 14.3619 - accuracy: 0.0119\n","Epoch 2/107\n","1/1 [==============================] - 0s 247ms/step - loss: 1.4676 - accuracy: 0.9067\n","Epoch 3/107\n","1/1 [==============================] - 0s 238ms/step - loss: 0.9316 - accuracy: 0.9067\n","Epoch 4/107\n","1/1 [==============================] - 0s 311ms/step - loss: 0.6598 - accuracy: 0.9067\n","Epoch 5/107\n","1/1 [==============================] - 0s 267ms/step - loss: 0.6364 - accuracy: 0.9067\n","Epoch 6/107\n","1/1 [==============================] - 0s 347ms/step - loss: 0.6887 - accuracy: 0.9067\n","Epoch 7/107\n","1/1 [==============================] - 1s 616ms/step - loss: 0.6335 - accuracy: 0.9067\n","Epoch 8/107\n","1/1 [==============================] - 1s 536ms/step - loss: 0.6558 - accuracy: 0.9067\n","Epoch 9/107\n","1/1 [==============================] - 0s 457ms/step - loss: 0.6553 - accuracy: 0.9067\n","Epoch 10/107\n","1/1 [==============================] - 0s 443ms/step - loss: 0.6634 - accuracy: 0.9067\n","Epoch 11/107\n","1/1 [==============================] - 0s 462ms/step - loss: 0.6842 - accuracy: 0.9067\n","Epoch 12/107\n","1/1 [==============================] - 0s 375ms/step - loss: 0.6651 - accuracy: 0.9067\n","Epoch 13/107\n","1/1 [==============================] - 1s 507ms/step - loss: 0.6321 - accuracy: 0.9067\n","Epoch 14/107\n","1/1 [==============================] - 0s 375ms/step - loss: 0.5995 - accuracy: 0.9067\n","Epoch 15/107\n","1/1 [==============================] - 0s 418ms/step - loss: 0.6034 - accuracy: 0.9067\n","Epoch 16/107\n","1/1 [==============================] - 0s 346ms/step - loss: 0.5855 - accuracy: 0.9067\n","Epoch 17/107\n","1/1 [==============================] - 0s 351ms/step - loss: 0.5744 - accuracy: 0.9067\n","Epoch 18/107\n","1/1 [==============================] - 0s 385ms/step - loss: 0.5611 - accuracy: 0.9067\n","Epoch 19/107\n","1/1 [==============================] - 1s 1s/step - loss: 0.5450 - accuracy: 0.9067\n","Epoch 20/107\n","1/1 [==============================] - 0s 487ms/step - loss: 0.5278 - accuracy: 0.9067\n","Epoch 21/107\n","1/1 [==============================] - 0s 333ms/step - loss: 0.5147 - accuracy: 0.9067\n","Epoch 22/107\n","1/1 [==============================] - 0s 411ms/step - loss: 0.5334 - accuracy: 0.9067\n","Epoch 23/107\n","1/1 [==============================] - 1s 727ms/step - loss: 0.5023 - accuracy: 0.9067\n","Epoch 24/107\n","1/1 [==============================] - 0s 364ms/step - loss: 0.5004 - accuracy: 0.9067\n","Epoch 25/107\n","1/1 [==============================] - 1s 856ms/step - loss: 0.5043 - accuracy: 0.9067\n","Epoch 26/107\n","1/1 [==============================] - 0s 451ms/step - loss: 0.5020 - accuracy: 0.9067\n","Epoch 27/107\n","1/1 [==============================] - 0s 301ms/step - loss: 0.5000 - accuracy: 0.9067\n","Epoch 28/107\n","1/1 [==============================] - 0s 312ms/step - loss: nan - accuracy: 0.9067\n","Epoch 29/107\n","1/1 [==============================] - 0s 364ms/step - loss: nan - accuracy: 0.9067\n","Epoch 30/107\n","1/1 [==============================] - 1s 568ms/step - loss: nan - accuracy: 0.9067\n","Epoch 31/107\n","1/1 [==============================] - 1s 534ms/step - loss: nan - accuracy: 0.9067\n","Epoch 32/107\n","1/1 [==============================] - 1s 539ms/step - loss: nan - accuracy: 0.9067\n","Epoch 33/107\n","1/1 [==============================] - 0s 281ms/step - loss: nan - accuracy: 0.9067\n","Epoch 34/107\n","1/1 [==============================] - 1s 787ms/step - loss: nan - accuracy: 0.9067\n","Epoch 35/107\n","1/1 [==============================] - 1s 583ms/step - loss: nan - accuracy: 0.9067\n","Epoch 36/107\n","1/1 [==============================] - 1s 557ms/step - loss: nan - accuracy: 0.9067\n","Epoch 37/107\n","1/1 [==============================] - 0s 318ms/step - loss: nan - accuracy: 0.9067\n","Epoch 38/107\n","1/1 [==============================] - 0s 267ms/step - loss: nan - accuracy: 0.9067\n","Epoch 39/107\n","1/1 [==============================] - 0s 379ms/step - loss: nan - accuracy: 0.9067\n","Epoch 40/107\n","1/1 [==============================] - 0s 413ms/step - loss: nan - accuracy: 0.9067\n","Epoch 41/107\n","1/1 [==============================] - 0s 311ms/step - loss: nan - accuracy: 0.9067\n","Epoch 42/107\n","1/1 [==============================] - 0s 332ms/step - loss: nan - accuracy: 0.9067\n","Epoch 43/107\n","1/1 [==============================] - 0s 339ms/step - loss: nan - accuracy: 0.9067\n","Epoch 44/107\n","1/1 [==============================] - 0s 496ms/step - loss: nan - accuracy: 0.9067\n","Epoch 45/107\n","1/1 [==============================] - 0s 436ms/step - loss: nan - accuracy: 0.9067\n","Epoch 46/107\n","1/1 [==============================] - 0s 328ms/step - loss: nan - accuracy: 0.9067\n","Epoch 47/107\n","1/1 [==============================] - 0s 326ms/step - loss: nan - accuracy: 0.9067\n","Epoch 48/107\n","1/1 [==============================] - 0s 287ms/step - loss: nan - accuracy: 0.9067\n","Epoch 49/107\n","1/1 [==============================] - 0s 438ms/step - loss: nan - accuracy: 0.9067\n","Epoch 50/107\n","1/1 [==============================] - 0s 468ms/step - loss: nan - accuracy: 0.9067\n","Epoch 51/107\n","1/1 [==============================] - 0s 381ms/step - loss: nan - accuracy: 0.9067\n","Epoch 52/107\n","1/1 [==============================] - 1s 508ms/step - loss: nan - accuracy: 0.9067\n","Epoch 53/107\n","1/1 [==============================] - 0s 444ms/step - loss: nan - accuracy: 0.9067\n","Epoch 54/107\n","1/1 [==============================] - 1s 967ms/step - loss: nan - accuracy: 0.9067\n","Epoch 55/107\n","1/1 [==============================] - 0s 477ms/step - loss: nan - accuracy: 0.9067\n","Epoch 56/107\n","1/1 [==============================] - 0s 476ms/step - loss: nan - accuracy: 0.9067\n","Epoch 57/107\n","1/1 [==============================] - 0s 336ms/step - loss: nan - accuracy: 0.9067\n","Epoch 58/107\n","1/1 [==============================] - 0s 296ms/step - loss: nan - accuracy: 0.9067\n","Epoch 59/107\n","1/1 [==============================] - 0s 480ms/step - loss: nan - accuracy: 0.9067\n","Epoch 60/107\n","1/1 [==============================] - 0s 355ms/step - loss: nan - accuracy: 0.9067\n","Epoch 61/107\n","1/1 [==============================] - 0s 261ms/step - loss: nan - accuracy: 0.9067\n","Epoch 62/107\n","1/1 [==============================] - 0s 232ms/step - loss: nan - accuracy: 0.9067\n","Epoch 63/107\n","1/1 [==============================] - 0s 269ms/step - loss: nan - accuracy: 0.9067\n","Epoch 64/107\n","1/1 [==============================] - 0s 290ms/step - loss: nan - accuracy: 0.9067\n","Epoch 65/107\n","1/1 [==============================] - 0s 243ms/step - loss: nan - accuracy: 0.9067\n","Epoch 66/107\n","1/1 [==============================] - 0s 446ms/step - loss: nan - accuracy: 0.9067\n","Epoch 67/107\n","1/1 [==============================] - 0s 416ms/step - loss: nan - accuracy: 0.9067\n","Epoch 68/107\n","1/1 [==============================] - 0s 418ms/step - loss: nan - accuracy: 0.9067\n","Epoch 69/107\n","1/1 [==============================] - 0s 337ms/step - loss: nan - accuracy: 0.9067\n","Epoch 70/107\n","1/1 [==============================] - 0s 367ms/step - loss: nan - accuracy: 0.9067\n","Epoch 71/107\n","1/1 [==============================] - 0s 354ms/step - loss: nan - accuracy: 0.9067\n","Epoch 72/107\n","1/1 [==============================] - 0s 319ms/step - loss: nan - accuracy: 0.9067\n","Epoch 73/107\n","1/1 [==============================] - 0s 331ms/step - loss: nan - accuracy: 0.9067\n","Epoch 74/107\n","1/1 [==============================] - 0s 470ms/step - loss: nan - accuracy: 0.9067\n","Epoch 75/107\n","1/1 [==============================] - 0s 339ms/step - loss: nan - accuracy: 0.9067\n","Epoch 76/107\n","1/1 [==============================] - 0s 323ms/step - loss: nan - accuracy: 0.9067\n","Epoch 77/107\n","1/1 [==============================] - 0s 276ms/step - loss: nan - accuracy: 0.9067\n","Epoch 78/107\n","1/1 [==============================] - 0s 333ms/step - loss: nan - accuracy: 0.9067\n","Epoch 79/107\n","1/1 [==============================] - 0s 359ms/step - loss: nan - accuracy: 0.9067\n","Epoch 80/107\n","1/1 [==============================] - 0s 345ms/step - loss: nan - accuracy: 0.9067\n","Epoch 81/107\n","1/1 [==============================] - 0s 321ms/step - loss: nan - accuracy: 0.9067\n","Epoch 82/107\n","1/1 [==============================] - 0s 331ms/step - loss: nan - accuracy: 0.9067\n","Epoch 83/107\n","1/1 [==============================] - 0s 438ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/107\n","1/1 [==============================] - 0s 464ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/107\n","1/1 [==============================] - 0s 497ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/107\n","1/1 [==============================] - 0s 461ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/107\n","1/1 [==============================] - 0s 337ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/107\n","1/1 [==============================] - 0s 355ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/107\n","1/1 [==============================] - 0s 401ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/107\n","1/1 [==============================] - 1s 645ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/107\n","1/1 [==============================] - 1s 632ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/107\n","1/1 [==============================] - 0s 432ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/107\n","1/1 [==============================] - 0s 469ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/107\n","1/1 [==============================] - 0s 343ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/107\n","1/1 [==============================] - 0s 380ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/107\n","1/1 [==============================] - 0s 357ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/107\n","1/1 [==============================] - 0s 317ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/107\n","1/1 [==============================] - 0s 382ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/107\n","1/1 [==============================] - 0s 289ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/107\n","1/1 [==============================] - 0s 365ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/107\n","1/1 [==============================] - 1s 577ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/107\n","1/1 [==============================] - 0s 458ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/107\n","1/1 [==============================] - 0s 370ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/107\n","1/1 [==============================] - 0s 426ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/107\n","1/1 [==============================] - 0s 439ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/107\n","1/1 [==============================] - 0s 411ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/107\n","1/1 [==============================] - 0s 478ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b44f58700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:10:39,231]\u001b[0m Trial 46 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 238, 'num_epochs': 107}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/117\n","1/1 [==============================] - 6s 6s/step - loss: 14.5935 - accuracy: 0.0060\n","Epoch 2/117\n","1/1 [==============================] - 0s 368ms/step - loss: 1.2864 - accuracy: 0.8313\n","Epoch 3/117\n","1/1 [==============================] - 0s 287ms/step - loss: 0.8638 - accuracy: 0.9067\n","Epoch 4/117\n","1/1 [==============================] - 0s 270ms/step - loss: 0.7407 - accuracy: 0.9067\n","Epoch 5/117\n","1/1 [==============================] - 0s 337ms/step - loss: 0.6570 - accuracy: 0.9067\n","Epoch 6/117\n","1/1 [==============================] - 0s 321ms/step - loss: 0.6304 - accuracy: 0.9067\n","Epoch 7/117\n","1/1 [==============================] - 0s 362ms/step - loss: 0.6327 - accuracy: 0.9067\n","Epoch 8/117\n","1/1 [==============================] - 1s 553ms/step - loss: 0.6276 - accuracy: 0.9067\n","Epoch 9/117\n","1/1 [==============================] - 0s 226ms/step - loss: 0.6167 - accuracy: 0.9067\n","Epoch 10/117\n","1/1 [==============================] - 0s 435ms/step - loss: 0.6288 - accuracy: 0.9067\n","Epoch 11/117\n","1/1 [==============================] - 0s 202ms/step - loss: 0.6275 - accuracy: 0.9067\n","Epoch 12/117\n","1/1 [==============================] - 0s 294ms/step - loss: 0.6227 - accuracy: 0.9067\n","Epoch 13/117\n","1/1 [==============================] - 0s 229ms/step - loss: 0.6194 - accuracy: 0.9067\n","Epoch 14/117\n","1/1 [==============================] - 0s 185ms/step - loss: 0.6155 - accuracy: 0.9067\n","Epoch 15/117\n","1/1 [==============================] - 0s 219ms/step - loss: 0.6116 - accuracy: 0.9067\n","Epoch 16/117\n","1/1 [==============================] - 0s 243ms/step - loss: 0.6082 - accuracy: 0.9067\n","Epoch 17/117\n","1/1 [==============================] - 0s 175ms/step - loss: 0.6044 - accuracy: 0.9067\n","Epoch 18/117\n","1/1 [==============================] - 0s 207ms/step - loss: 0.6005 - accuracy: 0.9067\n","Epoch 19/117\n","1/1 [==============================] - 0s 274ms/step - loss: 0.5969 - accuracy: 0.9067\n","Epoch 20/117\n","1/1 [==============================] - 0s 226ms/step - loss: 0.5932 - accuracy: 0.9067\n","Epoch 21/117\n","1/1 [==============================] - 0s 245ms/step - loss: 0.5886 - accuracy: 0.9067\n","Epoch 22/117\n","1/1 [==============================] - 0s 230ms/step - loss: 0.5834 - accuracy: 0.9067\n","Epoch 23/117\n","1/1 [==============================] - 0s 312ms/step - loss: 0.5786 - accuracy: 0.9107\n","Epoch 24/117\n","1/1 [==============================] - 0s 451ms/step - loss: 0.5743 - accuracy: 0.9167\n","Epoch 25/117\n","1/1 [==============================] - 0s 422ms/step - loss: 0.5698 - accuracy: 0.9187\n","Epoch 26/117\n","1/1 [==============================] - 0s 268ms/step - loss: 0.5676 - accuracy: 0.9167\n","Epoch 27/117\n","1/1 [==============================] - 0s 420ms/step - loss: 0.5682 - accuracy: 0.9187\n","Epoch 28/117\n","1/1 [==============================] - 0s 267ms/step - loss: 0.5699 - accuracy: 0.9107\n","Epoch 29/117\n","1/1 [==============================] - 0s 195ms/step - loss: 0.5813 - accuracy: 0.9067\n","Epoch 30/117\n","1/1 [==============================] - 0s 221ms/step - loss: 0.5657 - accuracy: 0.9067\n","Epoch 31/117\n","1/1 [==============================] - 0s 252ms/step - loss: 0.5823 - accuracy: 0.9067\n","Epoch 32/117\n","1/1 [==============================] - 0s 221ms/step - loss: 0.6136 - accuracy: 0.9067\n","Epoch 33/117\n","1/1 [==============================] - 0s 234ms/step - loss: 0.6267 - accuracy: 0.9067\n","Epoch 34/117\n","1/1 [==============================] - 0s 175ms/step - loss: 0.6218 - accuracy: 0.9067\n","Epoch 35/117\n","1/1 [==============================] - 0s 200ms/step - loss: 0.6037 - accuracy: 0.9067\n","Epoch 36/117\n","1/1 [==============================] - 0s 175ms/step - loss: 0.5790 - accuracy: 0.9067\n","Epoch 37/117\n","1/1 [==============================] - 0s 289ms/step - loss: 0.5646 - accuracy: 0.9067\n","Epoch 38/117\n","1/1 [==============================] - 0s 294ms/step - loss: 0.5680 - accuracy: 0.9067\n","Epoch 39/117\n","1/1 [==============================] - 0s 385ms/step - loss: 0.6074 - accuracy: 0.9067\n","Epoch 40/117\n","1/1 [==============================] - 0s 250ms/step - loss: 0.5597 - accuracy: 0.9067\n","Epoch 41/117\n","1/1 [==============================] - 0s 286ms/step - loss: 0.5553 - accuracy: 0.9067\n","Epoch 42/117\n","1/1 [==============================] - 0s 269ms/step - loss: 0.5606 - accuracy: 0.9067\n","Epoch 43/117\n","1/1 [==============================] - 0s 265ms/step - loss: 0.5671 - accuracy: 0.9067\n","Epoch 44/117\n","1/1 [==============================] - 0s 204ms/step - loss: 0.5701 - accuracy: 0.9067\n","Epoch 45/117\n","1/1 [==============================] - 0s 207ms/step - loss: 0.5683 - accuracy: 0.9067\n","Epoch 46/117\n","1/1 [==============================] - 0s 284ms/step - loss: 0.5625 - accuracy: 0.9067\n","Epoch 47/117\n","1/1 [==============================] - 0s 297ms/step - loss: 0.5545 - accuracy: 0.9067\n","Epoch 48/117\n","1/1 [==============================] - 0s 354ms/step - loss: 0.5465 - accuracy: 0.9067\n","Epoch 49/117\n","1/1 [==============================] - 0s 265ms/step - loss: 0.5417 - accuracy: 0.9067\n","Epoch 50/117\n","1/1 [==============================] - 0s 378ms/step - loss: 0.5390 - accuracy: 0.9067\n","Epoch 51/117\n","1/1 [==============================] - 0s 291ms/step - loss: 0.5372 - accuracy: 0.9067\n","Epoch 52/117\n","1/1 [==============================] - 0s 306ms/step - loss: 0.5361 - accuracy: 0.9087\n","Epoch 53/117\n","1/1 [==============================] - 0s 305ms/step - loss: 0.5355 - accuracy: 0.9107\n","Epoch 54/117\n","1/1 [==============================] - 0s 316ms/step - loss: 0.5345 - accuracy: 0.9147\n","Epoch 55/117\n","1/1 [==============================] - 0s 369ms/step - loss: 0.5317 - accuracy: 0.9187\n","Epoch 56/117\n","1/1 [==============================] - 0s 330ms/step - loss: 0.5286 - accuracy: 0.9187\n","Epoch 57/117\n","1/1 [==============================] - 0s 340ms/step - loss: 0.5261 - accuracy: 0.9187\n","Epoch 58/117\n","1/1 [==============================] - 0s 276ms/step - loss: 0.5241 - accuracy: 0.9167\n","Epoch 59/117\n","1/1 [==============================] - 0s 304ms/step - loss: 0.5222 - accuracy: 0.9167\n","Epoch 60/117\n","1/1 [==============================] - 0s 234ms/step - loss: 0.5204 - accuracy: 0.9147\n","Epoch 61/117\n","1/1 [==============================] - 0s 290ms/step - loss: 0.5187 - accuracy: 0.9147\n","Epoch 62/117\n","1/1 [==============================] - 1s 590ms/step - loss: 0.5172 - accuracy: 0.9127\n","Epoch 63/117\n","1/1 [==============================] - 0s 275ms/step - loss: 0.5157 - accuracy: 0.9147\n","Epoch 64/117\n","1/1 [==============================] - 0s 309ms/step - loss: 0.5138 - accuracy: 0.9147\n","Epoch 65/117\n","1/1 [==============================] - 0s 437ms/step - loss: 0.5115 - accuracy: 0.9167\n","Epoch 66/117\n","1/1 [==============================] - 0s 394ms/step - loss: 0.5089 - accuracy: 0.9167\n","Epoch 67/117\n","1/1 [==============================] - 0s 296ms/step - loss: 0.5065 - accuracy: 0.9167\n","Epoch 68/117\n","1/1 [==============================] - 0s 436ms/step - loss: 0.5041 - accuracy: 0.9167\n","Epoch 69/117\n","1/1 [==============================] - 0s 396ms/step - loss: 0.5015 - accuracy: 0.9167\n","Epoch 70/117\n","1/1 [==============================] - 0s 421ms/step - loss: 0.4987 - accuracy: 0.9187\n","Epoch 71/117\n","1/1 [==============================] - 0s 314ms/step - loss: 0.4960 - accuracy: 0.9246\n","Epoch 72/117\n","1/1 [==============================] - 1s 598ms/step - loss: 0.4932 - accuracy: 0.9246\n","Epoch 73/117\n","1/1 [==============================] - 0s 376ms/step - loss: 0.4902 - accuracy: 0.9246\n","Epoch 74/117\n","1/1 [==============================] - 0s 249ms/step - loss: 0.4868 - accuracy: 0.9246\n","Epoch 75/117\n","1/1 [==============================] - 0s 380ms/step - loss: 0.4839 - accuracy: 0.9266\n","Epoch 76/117\n","1/1 [==============================] - 0s 398ms/step - loss: 0.4812 - accuracy: 0.9266\n","Epoch 77/117\n","1/1 [==============================] - 0s 376ms/step - loss: 0.4780 - accuracy: 0.9266\n","Epoch 78/117\n","1/1 [==============================] - 0s 397ms/step - loss: nan - accuracy: 0.9286\n","Epoch 79/117\n","1/1 [==============================] - 0s 305ms/step - loss: nan - accuracy: 0.9067\n","Epoch 80/117\n","1/1 [==============================] - 0s 228ms/step - loss: nan - accuracy: 0.9067\n","Epoch 81/117\n","1/1 [==============================] - 0s 191ms/step - loss: nan - accuracy: 0.9067\n","Epoch 82/117\n","1/1 [==============================] - 0s 251ms/step - loss: nan - accuracy: 0.9067\n","Epoch 83/117\n","1/1 [==============================] - 0s 191ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/117\n","1/1 [==============================] - 0s 238ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/117\n","1/1 [==============================] - 0s 380ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/117\n","1/1 [==============================] - 0s 309ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/117\n","1/1 [==============================] - 0s 199ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/117\n","1/1 [==============================] - 0s 341ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/117\n","1/1 [==============================] - 0s 244ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/117\n","1/1 [==============================] - 0s 213ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/117\n","1/1 [==============================] - 0s 323ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/117\n","1/1 [==============================] - 0s 232ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/117\n","1/1 [==============================] - 0s 200ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/117\n","1/1 [==============================] - 0s 304ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/117\n","1/1 [==============================] - 0s 252ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/117\n","1/1 [==============================] - 0s 248ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/117\n","1/1 [==============================] - 0s 189ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/117\n","1/1 [==============================] - 0s 219ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/117\n","1/1 [==============================] - 0s 215ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/117\n","1/1 [==============================] - 0s 271ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/117\n","1/1 [==============================] - 0s 483ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/117\n","1/1 [==============================] - 1s 625ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/117\n","1/1 [==============================] - 0s 307ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/117\n","1/1 [==============================] - 0s 393ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/117\n","1/1 [==============================] - 0s 405ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/117\n","1/1 [==============================] - 0s 363ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/117\n","1/1 [==============================] - 0s 488ms/step - loss: nan - accuracy: 0.9067\n","Epoch 108/117\n","1/1 [==============================] - 1s 628ms/step - loss: nan - accuracy: 0.9067\n","Epoch 109/117\n","1/1 [==============================] - 1s 749ms/step - loss: nan - accuracy: 0.9067\n","Epoch 110/117\n","1/1 [==============================] - 0s 422ms/step - loss: nan - accuracy: 0.9067\n","Epoch 111/117\n","1/1 [==============================] - 0s 433ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/117\n","1/1 [==============================] - 0s 363ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/117\n","1/1 [==============================] - 0s 206ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/117\n","1/1 [==============================] - 0s 256ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/117\n","1/1 [==============================] - 0s 286ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/117\n","1/1 [==============================] - 0s 452ms/step - loss: nan - accuracy: 0.9067\n","Epoch 117/117\n","1/1 [==============================] - 0s 431ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b45638c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:11:24,481]\u001b[0m Trial 47 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 200, 'num_epochs': 117}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1/1 [==============================] - 6s 6s/step - loss: 10.8636 - accuracy: 0.0179\n","Epoch 2/5\n","1/1 [==============================] - 0s 116ms/step - loss: 1.4402 - accuracy: 0.9067\n","Epoch 3/5\n","1/1 [==============================] - 0s 161ms/step - loss: 1.2941 - accuracy: 0.9067\n","Epoch 4/5\n","1/1 [==============================] - 0s 191ms/step - loss: 1.1611 - accuracy: 0.9067\n","Epoch 5/5\n","1/1 [==============================] - 0s 163ms/step - loss: 0.9952 - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b2782cf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 2s 2s/step - loss: 1.3289 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:11:32,984]\u001b[0m Trial 48 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 90, 'num_epochs': 5}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/125\n","1/1 [==============================] - 5s 5s/step - loss: 6.9950 - accuracy: 0.0020\n","Epoch 2/125\n","1/1 [==============================] - 0s 170ms/step - loss: 1.6899 - accuracy: 0.9067\n","Epoch 3/125\n","1/1 [==============================] - 0s 196ms/step - loss: 1.3682 - accuracy: 0.9067\n","Epoch 4/125\n","1/1 [==============================] - 0s 156ms/step - loss: 1.1693 - accuracy: 0.9067\n","Epoch 5/125\n","1/1 [==============================] - 0s 166ms/step - loss: 0.9950 - accuracy: 0.9067\n","Epoch 6/125\n","1/1 [==============================] - 0s 137ms/step - loss: 0.8828 - accuracy: 0.9067\n","Epoch 7/125\n","1/1 [==============================] - 0s 234ms/step - loss: 0.8217 - accuracy: 0.9067\n","Epoch 8/125\n","1/1 [==============================] - 0s 190ms/step - loss: 0.7866 - accuracy: 0.9067\n","Epoch 9/125\n","1/1 [==============================] - 0s 186ms/step - loss: 0.7530 - accuracy: 0.9067\n","Epoch 10/125\n","1/1 [==============================] - 0s 189ms/step - loss: 0.7197 - accuracy: 0.9067\n","Epoch 11/125\n","1/1 [==============================] - 0s 227ms/step - loss: 0.6924 - accuracy: 0.9067\n","Epoch 12/125\n","1/1 [==============================] - 0s 179ms/step - loss: 0.6500 - accuracy: 0.9067\n","Epoch 13/125\n","1/1 [==============================] - 0s 161ms/step - loss: 0.6039 - accuracy: 0.9067\n","Epoch 14/125\n","1/1 [==============================] - 0s 127ms/step - loss: 0.5933 - accuracy: 0.9067\n","Epoch 15/125\n","1/1 [==============================] - 0s 162ms/step - loss: 0.5837 - accuracy: 0.9067\n","Epoch 16/125\n","1/1 [==============================] - 0s 156ms/step - loss: 0.5800 - accuracy: 0.9067\n","Epoch 17/125\n","1/1 [==============================] - 0s 131ms/step - loss: 0.5976 - accuracy: 0.9067\n","Epoch 18/125\n","1/1 [==============================] - 0s 151ms/step - loss: 0.5961 - accuracy: 0.9067\n","Epoch 19/125\n","1/1 [==============================] - 0s 147ms/step - loss: 0.5951 - accuracy: 0.9067\n","Epoch 20/125\n","1/1 [==============================] - 0s 140ms/step - loss: 0.5807 - accuracy: 0.9067\n","Epoch 21/125\n","1/1 [==============================] - 0s 144ms/step - loss: 0.5968 - accuracy: 0.9067\n","Epoch 22/125\n","1/1 [==============================] - 0s 156ms/step - loss: 0.6549 - accuracy: 0.9067\n","Epoch 23/125\n","1/1 [==============================] - 0s 177ms/step - loss: 0.7061 - accuracy: 0.9067\n","Epoch 24/125\n","1/1 [==============================] - 0s 171ms/step - loss: 0.7460 - accuracy: 0.9067\n","Epoch 25/125\n","1/1 [==============================] - 0s 204ms/step - loss: 0.7750 - accuracy: 0.9067\n","Epoch 26/125\n","1/1 [==============================] - 0s 166ms/step - loss: 0.7728 - accuracy: 0.9067\n","Epoch 27/125\n","1/1 [==============================] - 0s 192ms/step - loss: 0.7841 - accuracy: 0.9067\n","Epoch 28/125\n","1/1 [==============================] - 0s 160ms/step - loss: 0.7919 - accuracy: 0.9067\n","Epoch 29/125\n","1/1 [==============================] - 0s 172ms/step - loss: 0.7965 - accuracy: 0.9067\n","Epoch 30/125\n","1/1 [==============================] - 0s 157ms/step - loss: 0.7987 - accuracy: 0.9067\n","Epoch 31/125\n","1/1 [==============================] - 0s 178ms/step - loss: 0.7992 - accuracy: 0.9067\n","Epoch 32/125\n","1/1 [==============================] - 0s 162ms/step - loss: 0.7984 - accuracy: 0.9067\n","Epoch 33/125\n","1/1 [==============================] - 0s 159ms/step - loss: 0.7961 - accuracy: 0.9067\n","Epoch 34/125\n","1/1 [==============================] - 0s 213ms/step - loss: 0.7927 - accuracy: 0.9067\n","Epoch 35/125\n","1/1 [==============================] - 0s 145ms/step - loss: 0.7877 - accuracy: 0.9067\n","Epoch 36/125\n","1/1 [==============================] - 0s 136ms/step - loss: 0.7813 - accuracy: 0.9067\n","Epoch 37/125\n","1/1 [==============================] - 0s 159ms/step - loss: 0.7736 - accuracy: 0.9067\n","Epoch 38/125\n","1/1 [==============================] - 0s 160ms/step - loss: 0.7642 - accuracy: 0.9067\n","Epoch 39/125\n","1/1 [==============================] - 0s 152ms/step - loss: 0.7520 - accuracy: 0.9067\n","Epoch 40/125\n","1/1 [==============================] - 0s 130ms/step - loss: 0.7382 - accuracy: 0.9067\n","Epoch 41/125\n","1/1 [==============================] - 0s 147ms/step - loss: 0.7240 - accuracy: 0.9067\n","Epoch 42/125\n","1/1 [==============================] - 0s 145ms/step - loss: 0.7097 - accuracy: 0.9067\n","Epoch 43/125\n","1/1 [==============================] - 0s 121ms/step - loss: 0.6951 - accuracy: 0.9067\n","Epoch 44/125\n","1/1 [==============================] - 0s 162ms/step - loss: 0.6804 - accuracy: 0.9067\n","Epoch 45/125\n","1/1 [==============================] - 0s 131ms/step - loss: 0.6653 - accuracy: 0.9067\n","Epoch 46/125\n","1/1 [==============================] - 0s 142ms/step - loss: 0.6500 - accuracy: 0.9067\n","Epoch 47/125\n","1/1 [==============================] - 0s 142ms/step - loss: 0.6345 - accuracy: 0.9067\n","Epoch 48/125\n","1/1 [==============================] - 0s 136ms/step - loss: 0.6189 - accuracy: 0.9067\n","Epoch 49/125\n","1/1 [==============================] - 0s 158ms/step - loss: 0.6034 - accuracy: 0.9067\n","Epoch 50/125\n","1/1 [==============================] - 0s 133ms/step - loss: 0.5882 - accuracy: 0.9067\n","Epoch 51/125\n","1/1 [==============================] - 0s 176ms/step - loss: 0.5736 - accuracy: 0.9067\n","Epoch 52/125\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5601 - accuracy: 0.9067\n","Epoch 53/125\n","1/1 [==============================] - 0s 135ms/step - loss: 0.5480 - accuracy: 0.9067\n","Epoch 54/125\n","1/1 [==============================] - 0s 154ms/step - loss: 0.5376 - accuracy: 0.9067\n","Epoch 55/125\n","1/1 [==============================] - 0s 146ms/step - loss: 0.5286 - accuracy: 0.9067\n","Epoch 56/125\n","1/1 [==============================] - 0s 152ms/step - loss: 0.5214 - accuracy: 0.9067\n","Epoch 57/125\n","1/1 [==============================] - 0s 229ms/step - loss: 0.5161 - accuracy: 0.9067\n","Epoch 58/125\n","1/1 [==============================] - 0s 199ms/step - loss: 0.5127 - accuracy: 0.9067\n","Epoch 59/125\n","1/1 [==============================] - 0s 143ms/step - loss: 0.5114 - accuracy: 0.9067\n","Epoch 60/125\n","1/1 [==============================] - 0s 163ms/step - loss: 0.5111 - accuracy: 0.9067\n","Epoch 61/125\n","1/1 [==============================] - 0s 134ms/step - loss: 0.5121 - accuracy: 0.9067\n","Epoch 62/125\n","1/1 [==============================] - 0s 148ms/step - loss: 0.5151 - accuracy: 0.9067\n","Epoch 63/125\n","1/1 [==============================] - 0s 153ms/step - loss: 0.5362 - accuracy: 0.9067\n","Epoch 64/125\n","1/1 [==============================] - 0s 148ms/step - loss: 0.5363 - accuracy: 0.9067\n","Epoch 65/125\n","1/1 [==============================] - 0s 135ms/step - loss: 0.5359 - accuracy: 0.9067\n","Epoch 66/125\n","1/1 [==============================] - 0s 266ms/step - loss: 0.5347 - accuracy: 0.9067\n","Epoch 67/125\n","1/1 [==============================] - 0s 165ms/step - loss: 0.5330 - accuracy: 0.9067\n","Epoch 68/125\n","1/1 [==============================] - 0s 175ms/step - loss: 0.5110 - accuracy: 0.9067\n","Epoch 69/125\n","1/1 [==============================] - 0s 141ms/step - loss: 0.5063 - accuracy: 0.9067\n","Epoch 70/125\n","1/1 [==============================] - 0s 143ms/step - loss: 0.5039 - accuracy: 0.9067\n","Epoch 71/125\n","1/1 [==============================] - 0s 132ms/step - loss: 0.5025 - accuracy: 0.9067\n","Epoch 72/125\n","1/1 [==============================] - 0s 140ms/step - loss: 0.5014 - accuracy: 0.9067\n","Epoch 73/125\n","1/1 [==============================] - 0s 122ms/step - loss: 0.5006 - accuracy: 0.9067\n","Epoch 74/125\n","1/1 [==============================] - 0s 143ms/step - loss: 0.4999 - accuracy: 0.9067\n","Epoch 75/125\n","1/1 [==============================] - 0s 125ms/step - loss: 0.4991 - accuracy: 0.9067\n","Epoch 76/125\n","1/1 [==============================] - 0s 126ms/step - loss: 0.4981 - accuracy: 0.9067\n","Epoch 77/125\n","1/1 [==============================] - 0s 152ms/step - loss: 0.4971 - accuracy: 0.9067\n","Epoch 78/125\n","1/1 [==============================] - 0s 156ms/step - loss: 0.4961 - accuracy: 0.9067\n","Epoch 79/125\n","1/1 [==============================] - 0s 138ms/step - loss: 0.4950 - accuracy: 0.9067\n","Epoch 80/125\n","1/1 [==============================] - 0s 177ms/step - loss: 0.4940 - accuracy: 0.9067\n","Epoch 81/125\n","1/1 [==============================] - 0s 159ms/step - loss: 0.4931 - accuracy: 0.9067\n","Epoch 82/125\n","1/1 [==============================] - 0s 166ms/step - loss: 0.4923 - accuracy: 0.9067\n","Epoch 83/125\n","1/1 [==============================] - 0s 167ms/step - loss: 0.4914 - accuracy: 0.9067\n","Epoch 84/125\n","1/1 [==============================] - 0s 131ms/step - loss: 0.4906 - accuracy: 0.9067\n","Epoch 85/125\n","1/1 [==============================] - 0s 141ms/step - loss: 0.4898 - accuracy: 0.9067\n","Epoch 86/125\n","1/1 [==============================] - 0s 179ms/step - loss: 0.4890 - accuracy: 0.9067\n","Epoch 87/125\n","1/1 [==============================] - 0s 168ms/step - loss: 0.4882 - accuracy: 0.9067\n","Epoch 88/125\n","1/1 [==============================] - 0s 161ms/step - loss: 0.4874 - accuracy: 0.9067\n","Epoch 89/125\n","1/1 [==============================] - 0s 210ms/step - loss: 0.4866 - accuracy: 0.9067\n","Epoch 90/125\n","1/1 [==============================] - 0s 181ms/step - loss: 0.4858 - accuracy: 0.9067\n","Epoch 91/125\n","1/1 [==============================] - 0s 151ms/step - loss: 0.4850 - accuracy: 0.9067\n","Epoch 92/125\n","1/1 [==============================] - 0s 135ms/step - loss: 0.4841 - accuracy: 0.9067\n","Epoch 93/125\n","1/1 [==============================] - 0s 148ms/step - loss: 0.4833 - accuracy: 0.9067\n","Epoch 94/125\n","1/1 [==============================] - 0s 172ms/step - loss: 0.4824 - accuracy: 0.9067\n","Epoch 95/125\n","1/1 [==============================] - 0s 194ms/step - loss: 0.4816 - accuracy: 0.9067\n","Epoch 96/125\n","1/1 [==============================] - 0s 237ms/step - loss: 0.4808 - accuracy: 0.9067\n","Epoch 97/125\n","1/1 [==============================] - 0s 242ms/step - loss: 0.4800 - accuracy: 0.9067\n","Epoch 98/125\n","1/1 [==============================] - 0s 219ms/step - loss: 0.4792 - accuracy: 0.9067\n","Epoch 99/125\n","1/1 [==============================] - 0s 291ms/step - loss: 0.4784 - accuracy: 0.9067\n","Epoch 100/125\n","1/1 [==============================] - 0s 249ms/step - loss: 0.4776 - accuracy: 0.9067\n","Epoch 101/125\n","1/1 [==============================] - 0s 237ms/step - loss: 0.4768 - accuracy: 0.9067\n","Epoch 102/125\n","1/1 [==============================] - 0s 162ms/step - loss: 0.4760 - accuracy: 0.9067\n","Epoch 103/125\n","1/1 [==============================] - 0s 141ms/step - loss: 0.4752 - accuracy: 0.9067\n","Epoch 104/125\n","1/1 [==============================] - 0s 151ms/step - loss: 0.4744 - accuracy: 0.9067\n","Epoch 105/125\n","1/1 [==============================] - 0s 158ms/step - loss: 0.4737 - accuracy: 0.9067\n","Epoch 106/125\n","1/1 [==============================] - 0s 160ms/step - loss: 0.4729 - accuracy: 0.9067\n","Epoch 107/125\n","1/1 [==============================] - 0s 206ms/step - loss: 0.4721 - accuracy: 0.9087\n","Epoch 108/125\n","1/1 [==============================] - 0s 177ms/step - loss: 0.4714 - accuracy: 0.9087\n","Epoch 109/125\n","1/1 [==============================] - 0s 191ms/step - loss: 0.4707 - accuracy: 0.9087\n","Epoch 110/125\n","1/1 [==============================] - 0s 232ms/step - loss: 0.4700 - accuracy: 0.9087\n","Epoch 111/125\n","1/1 [==============================] - 0s 236ms/step - loss: 0.4692 - accuracy: 0.9087\n","Epoch 112/125\n","1/1 [==============================] - 0s 197ms/step - loss: 0.4685 - accuracy: 0.9087\n","Epoch 113/125\n","1/1 [==============================] - 0s 179ms/step - loss: 0.4678 - accuracy: 0.9087\n","Epoch 114/125\n","1/1 [==============================] - 0s 183ms/step - loss: 0.4670 - accuracy: 0.9087\n","Epoch 115/125\n","1/1 [==============================] - 0s 156ms/step - loss: 0.4663 - accuracy: 0.9107\n","Epoch 116/125\n","1/1 [==============================] - 0s 146ms/step - loss: 0.4655 - accuracy: 0.9107\n","Epoch 117/125\n","1/1 [==============================] - 0s 160ms/step - loss: 0.4648 - accuracy: 0.9127\n","Epoch 118/125\n","1/1 [==============================] - 0s 154ms/step - loss: 0.4640 - accuracy: 0.9147\n","Epoch 119/125\n","1/1 [==============================] - 0s 200ms/step - loss: 0.4632 - accuracy: 0.9147\n","Epoch 120/125\n","1/1 [==============================] - 0s 163ms/step - loss: 0.4624 - accuracy: 0.9147\n","Epoch 121/125\n","1/1 [==============================] - 0s 169ms/step - loss: 0.4616 - accuracy: 0.9167\n","Epoch 122/125\n","1/1 [==============================] - 0s 208ms/step - loss: 0.4608 - accuracy: 0.9167\n","Epoch 123/125\n","1/1 [==============================] - 0s 158ms/step - loss: 0.4600 - accuracy: 0.9167\n","Epoch 124/125\n","1/1 [==============================] - 0s 187ms/step - loss: 0.4591 - accuracy: 0.9167\n","Epoch 125/125\n","1/1 [==============================] - 0s 171ms/step - loss: 0.4583 - accuracy: 0.9187\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b4c197280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: 1.1005 - accuracy: 0.9235\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-21 14:12:01,573]\u001b[0m Trial 49 finished with value: 0.9234693646430969 and parameters: {'embedding_output_dim': 77, 'num_epochs': 125}. Best is trial 0 with value: 0.9234693646430969.\u001b[0m\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xEfU2HlUhxMy","executionInfo":{"status":"ok","timestamp":1629040958014,"user_tz":180,"elapsed":89931,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}},"outputId":"6d47cf4f-e5da-4848-8299-876e2d691a95"}},{"cell_type":"code","execution_count":44,"source":["input_dim = len(vocab)\n","input_length = max_len\n","\n","best_trial_params = study.best_trial.params\n","\n","embedding_output_dim = best_trial_params['embedding_output_dim']\n","num_epochs = best_trial_params['num_epochs']\n","\n","model = Sequential([\n","    Embedding(input_dim=input_dim, output_dim=embedding_output_dim, input_length=input_length),\n","    Bidirectional(LSTM(units=embedding_output_dim, return_sequences=True, dropout=0.01), merge_mode='concat'),\n","    LSTM(units=embedding_output_dim, return_sequences=True, dropout=0.01),\n","    TimeDistributed(Dense(len(tags), activation='relu'))\n","])\n","\n","optimizer = optimizers.Adam(clipvalue=0.5)\n","\n","model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","\n","model.fit(train_tokens, np.array(train_tags), epochs=num_epochs)\n","\n","model.evaluate(test_tokens, np.array(test_tags))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/121\n","1/1 [==============================] - 6s 6s/step - loss: 5.8709 - accuracy: 0.4127\n","Epoch 2/121\n","1/1 [==============================] - 0s 146ms/step - loss: 1.5083 - accuracy: 0.9067\n","Epoch 3/121\n","1/1 [==============================] - 0s 154ms/step - loss: 1.1873 - accuracy: 0.9067\n","Epoch 4/121\n","1/1 [==============================] - 0s 159ms/step - loss: 1.1211 - accuracy: 0.9067\n","Epoch 5/121\n","1/1 [==============================] - 0s 155ms/step - loss: 1.1016 - accuracy: 0.9067\n","Epoch 6/121\n","1/1 [==============================] - 0s 141ms/step - loss: 1.0723 - accuracy: 0.9067\n","Epoch 7/121\n","1/1 [==============================] - 0s 125ms/step - loss: 1.1142 - accuracy: 0.9067\n","Epoch 8/121\n","1/1 [==============================] - 0s 125ms/step - loss: 1.1959 - accuracy: 0.9067\n","Epoch 9/121\n","1/1 [==============================] - 0s 126ms/step - loss: 1.2290 - accuracy: 0.9067\n","Epoch 10/121\n","1/1 [==============================] - 0s 116ms/step - loss: 1.2577 - accuracy: 0.9067\n","Epoch 11/121\n","1/1 [==============================] - 0s 115ms/step - loss: 1.2738 - accuracy: 0.8968\n","Epoch 12/121\n","1/1 [==============================] - 0s 125ms/step - loss: 1.2781 - accuracy: 0.8869\n","Epoch 13/121\n","1/1 [==============================] - 0s 120ms/step - loss: 1.2738 - accuracy: 0.8770\n","Epoch 14/121\n","1/1 [==============================] - 0s 136ms/step - loss: 1.2624 - accuracy: 0.8810\n","Epoch 15/121\n","1/1 [==============================] - 0s 128ms/step - loss: 1.2463 - accuracy: 0.8909\n","Epoch 16/121\n","1/1 [==============================] - 0s 133ms/step - loss: 1.2265 - accuracy: 0.8948\n","Epoch 17/121\n","1/1 [==============================] - 0s 125ms/step - loss: 1.2044 - accuracy: 0.9048\n","Epoch 18/121\n","1/1 [==============================] - 0s 121ms/step - loss: 1.1799 - accuracy: 0.9067\n","Epoch 19/121\n","1/1 [==============================] - 0s 114ms/step - loss: 1.1538 - accuracy: 0.9067\n","Epoch 20/121\n","1/1 [==============================] - 0s 116ms/step - loss: 1.1287 - accuracy: 0.9067\n","Epoch 21/121\n","1/1 [==============================] - 0s 121ms/step - loss: 1.1050 - accuracy: 0.9067\n","Epoch 22/121\n","1/1 [==============================] - 0s 118ms/step - loss: 1.0854 - accuracy: 0.9067\n","Epoch 23/121\n","1/1 [==============================] - 0s 120ms/step - loss: 1.0613 - accuracy: 0.9067\n","Epoch 24/121\n","1/1 [==============================] - 0s 125ms/step - loss: 1.0417 - accuracy: 0.9067\n","Epoch 25/121\n","1/1 [==============================] - 0s 170ms/step - loss: 1.0249 - accuracy: 0.9067\n","Epoch 26/121\n","1/1 [==============================] - 0s 161ms/step - loss: 1.0097 - accuracy: 0.9067\n","Epoch 27/121\n","1/1 [==============================] - 0s 163ms/step - loss: 1.0208 - accuracy: 0.9067\n","Epoch 28/121\n","1/1 [==============================] - 0s 155ms/step - loss: 0.9656 - accuracy: 0.9067\n","Epoch 29/121\n","1/1 [==============================] - 0s 168ms/step - loss: 0.9012 - accuracy: 0.9067\n","Epoch 30/121\n","1/1 [==============================] - 0s 157ms/step - loss: 0.8945 - accuracy: 0.9067\n","Epoch 31/121\n","1/1 [==============================] - 0s 157ms/step - loss: 0.9147 - accuracy: 0.9067\n","Epoch 32/121\n","1/1 [==============================] - 0s 150ms/step - loss: 0.9082 - accuracy: 0.9067\n","Epoch 33/121\n","1/1 [==============================] - 0s 163ms/step - loss: 0.9019 - accuracy: 0.9067\n","Epoch 34/121\n","1/1 [==============================] - 0s 141ms/step - loss: 0.9166 - accuracy: 0.9067\n","Epoch 35/121\n","1/1 [==============================] - 0s 154ms/step - loss: 0.9083 - accuracy: 0.9067\n","Epoch 36/121\n","1/1 [==============================] - 0s 162ms/step - loss: 0.9023 - accuracy: 0.9067\n","Epoch 37/121\n","1/1 [==============================] - 0s 128ms/step - loss: 0.8995 - accuracy: 0.9067\n","Epoch 38/121\n","1/1 [==============================] - 0s 155ms/step - loss: 0.8980 - accuracy: 0.9067\n","Epoch 39/121\n","1/1 [==============================] - 0s 130ms/step - loss: 0.8957 - accuracy: 0.9067\n","Epoch 40/121\n","1/1 [==============================] - 0s 123ms/step - loss: 0.8935 - accuracy: 0.9067\n","Epoch 41/121\n","1/1 [==============================] - 0s 124ms/step - loss: 0.8916 - accuracy: 0.9067\n","Epoch 42/121\n","1/1 [==============================] - 0s 153ms/step - loss: 0.8908 - accuracy: 0.9067\n","Epoch 43/121\n","1/1 [==============================] - 0s 155ms/step - loss: 0.8900 - accuracy: 0.9067\n","Epoch 44/121\n","1/1 [==============================] - 0s 159ms/step - loss: 0.8887 - accuracy: 0.9067\n","Epoch 45/121\n","1/1 [==============================] - 0s 168ms/step - loss: 0.8871 - accuracy: 0.9067\n","Epoch 46/121\n","1/1 [==============================] - 0s 154ms/step - loss: 0.8856 - accuracy: 0.9067\n","Epoch 47/121\n","1/1 [==============================] - 0s 131ms/step - loss: 0.8842 - accuracy: 0.9067\n","Epoch 48/121\n","1/1 [==============================] - 0s 120ms/step - loss: 0.8835 - accuracy: 0.9067\n","Epoch 49/121\n","1/1 [==============================] - 0s 133ms/step - loss: 0.8830 - accuracy: 0.9067\n","Epoch 50/121\n","1/1 [==============================] - 0s 161ms/step - loss: 0.8828 - accuracy: 0.9107\n","Epoch 51/121\n","1/1 [==============================] - 0s 164ms/step - loss: 0.8805 - accuracy: 0.9067\n","Epoch 52/121\n","1/1 [==============================] - 0s 191ms/step - loss: 0.8788 - accuracy: 0.9067\n","Epoch 53/121\n","1/1 [==============================] - 0s 162ms/step - loss: 0.8771 - accuracy: 0.9067\n","Epoch 54/121\n","1/1 [==============================] - 0s 154ms/step - loss: 0.8758 - accuracy: 0.9067\n","Epoch 55/121\n","1/1 [==============================] - 0s 160ms/step - loss: 0.8750 - accuracy: 0.9067\n","Epoch 56/121\n","1/1 [==============================] - 0s 155ms/step - loss: 0.8736 - accuracy: 0.9067\n","Epoch 57/121\n","1/1 [==============================] - 0s 173ms/step - loss: 0.8725 - accuracy: 0.9067\n","Epoch 58/121\n","1/1 [==============================] - 0s 164ms/step - loss: 0.8701 - accuracy: 0.9107\n","Epoch 59/121\n","1/1 [==============================] - 0s 133ms/step - loss: nan - accuracy: 0.9107\n","Epoch 60/121\n","1/1 [==============================] - 0s 134ms/step - loss: nan - accuracy: 0.9067\n","Epoch 61/121\n","1/1 [==============================] - 0s 148ms/step - loss: nan - accuracy: 0.9067\n","Epoch 62/121\n","1/1 [==============================] - 0s 141ms/step - loss: nan - accuracy: 0.9067\n","Epoch 63/121\n","1/1 [==============================] - 0s 134ms/step - loss: nan - accuracy: 0.9067\n","Epoch 64/121\n","1/1 [==============================] - 0s 149ms/step - loss: nan - accuracy: 0.9067\n","Epoch 65/121\n","1/1 [==============================] - 0s 143ms/step - loss: nan - accuracy: 0.9067\n","Epoch 66/121\n","1/1 [==============================] - 0s 161ms/step - loss: nan - accuracy: 0.9067\n","Epoch 67/121\n","1/1 [==============================] - 0s 145ms/step - loss: nan - accuracy: 0.9067\n","Epoch 68/121\n","1/1 [==============================] - 0s 170ms/step - loss: nan - accuracy: 0.9067\n","Epoch 69/121\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 70/121\n","1/1 [==============================] - 0s 189ms/step - loss: nan - accuracy: 0.9067\n","Epoch 71/121\n","1/1 [==============================] - 0s 177ms/step - loss: nan - accuracy: 0.9067\n","Epoch 72/121\n","1/1 [==============================] - 0s 177ms/step - loss: nan - accuracy: 0.9067\n","Epoch 73/121\n","1/1 [==============================] - 0s 180ms/step - loss: nan - accuracy: 0.9067\n","Epoch 74/121\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 75/121\n","1/1 [==============================] - 0s 181ms/step - loss: nan - accuracy: 0.9067\n","Epoch 76/121\n","1/1 [==============================] - 0s 186ms/step - loss: nan - accuracy: 0.9067\n","Epoch 77/121\n","1/1 [==============================] - 0s 187ms/step - loss: nan - accuracy: 0.9067\n","Epoch 78/121\n","1/1 [==============================] - 0s 130ms/step - loss: nan - accuracy: 0.9067\n","Epoch 79/121\n","1/1 [==============================] - 0s 130ms/step - loss: nan - accuracy: 0.9067\n","Epoch 80/121\n","1/1 [==============================] - 0s 110ms/step - loss: nan - accuracy: 0.9067\n","Epoch 81/121\n","1/1 [==============================] - 0s 105ms/step - loss: nan - accuracy: 0.9067\n","Epoch 82/121\n","1/1 [==============================] - 0s 141ms/step - loss: nan - accuracy: 0.9067\n","Epoch 83/121\n","1/1 [==============================] - 0s 144ms/step - loss: nan - accuracy: 0.9067\n","Epoch 84/121\n","1/1 [==============================] - 0s 164ms/step - loss: nan - accuracy: 0.9067\n","Epoch 85/121\n","1/1 [==============================] - 0s 146ms/step - loss: nan - accuracy: 0.9067\n","Epoch 86/121\n","1/1 [==============================] - 0s 141ms/step - loss: nan - accuracy: 0.9067\n","Epoch 87/121\n","1/1 [==============================] - 0s 104ms/step - loss: nan - accuracy: 0.9067\n","Epoch 88/121\n","1/1 [==============================] - 0s 132ms/step - loss: nan - accuracy: 0.9067\n","Epoch 89/121\n","1/1 [==============================] - 0s 146ms/step - loss: nan - accuracy: 0.9067\n","Epoch 90/121\n","1/1 [==============================] - 0s 143ms/step - loss: nan - accuracy: 0.9067\n","Epoch 91/121\n","1/1 [==============================] - 0s 163ms/step - loss: nan - accuracy: 0.9067\n","Epoch 92/121\n","1/1 [==============================] - 0s 153ms/step - loss: nan - accuracy: 0.9067\n","Epoch 93/121\n","1/1 [==============================] - 0s 148ms/step - loss: nan - accuracy: 0.9067\n","Epoch 94/121\n","1/1 [==============================] - 0s 139ms/step - loss: nan - accuracy: 0.9067\n","Epoch 95/121\n","1/1 [==============================] - 0s 149ms/step - loss: nan - accuracy: 0.9067\n","Epoch 96/121\n","1/1 [==============================] - 0s 144ms/step - loss: nan - accuracy: 0.9067\n","Epoch 97/121\n","1/1 [==============================] - 0s 147ms/step - loss: nan - accuracy: 0.9067\n","Epoch 98/121\n","1/1 [==============================] - 0s 152ms/step - loss: nan - accuracy: 0.9067\n","Epoch 99/121\n","1/1 [==============================] - 0s 119ms/step - loss: nan - accuracy: 0.9067\n","Epoch 100/121\n","1/1 [==============================] - 0s 138ms/step - loss: nan - accuracy: 0.9067\n","Epoch 101/121\n","1/1 [==============================] - 0s 141ms/step - loss: nan - accuracy: 0.9067\n","Epoch 102/121\n","1/1 [==============================] - 0s 151ms/step - loss: nan - accuracy: 0.9067\n","Epoch 103/121\n","1/1 [==============================] - 0s 177ms/step - loss: nan - accuracy: 0.9067\n","Epoch 104/121\n","1/1 [==============================] - 0s 190ms/step - loss: nan - accuracy: 0.9067\n","Epoch 105/121\n","1/1 [==============================] - 0s 178ms/step - loss: nan - accuracy: 0.9067\n","Epoch 106/121\n","1/1 [==============================] - 0s 172ms/step - loss: nan - accuracy: 0.9067\n","Epoch 107/121\n","1/1 [==============================] - 0s 148ms/step - loss: nan - accuracy: 0.9067\n","Epoch 108/121\n","1/1 [==============================] - 0s 174ms/step - loss: nan - accuracy: 0.9067\n","Epoch 109/121\n","1/1 [==============================] - 0s 180ms/step - loss: nan - accuracy: 0.9067\n","Epoch 110/121\n","1/1 [==============================] - 0s 176ms/step - loss: nan - accuracy: 0.9067\n","Epoch 111/121\n","1/1 [==============================] - 0s 245ms/step - loss: nan - accuracy: 0.9067\n","Epoch 112/121\n","1/1 [==============================] - 0s 183ms/step - loss: nan - accuracy: 0.9067\n","Epoch 113/121\n","1/1 [==============================] - 0s 168ms/step - loss: nan - accuracy: 0.9067\n","Epoch 114/121\n","1/1 [==============================] - 0s 151ms/step - loss: nan - accuracy: 0.9067\n","Epoch 115/121\n","1/1 [==============================] - 0s 167ms/step - loss: nan - accuracy: 0.9067\n","Epoch 116/121\n","1/1 [==============================] - 0s 157ms/step - loss: nan - accuracy: 0.9067\n","Epoch 117/121\n","1/1 [==============================] - 0s 148ms/step - loss: nan - accuracy: 0.9067\n","Epoch 118/121\n","1/1 [==============================] - 0s 143ms/step - loss: nan - accuracy: 0.9067\n","Epoch 119/121\n","1/1 [==============================] - 0s 159ms/step - loss: nan - accuracy: 0.9067\n","Epoch 120/121\n","1/1 [==============================] - 0s 165ms/step - loss: nan - accuracy: 0.9067\n","Epoch 121/121\n","1/1 [==============================] - 0s 160ms/step - loss: nan - accuracy: 0.9067\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5b122053a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1/1 [==============================] - 1s 1s/step - loss: nan - accuracy: 0.9235\n"]},{"output_type":"execute_result","data":{"text/plain":["[nan, 0.9234693646430969]"]},"metadata":{},"execution_count":44}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vj6aijDahxM1","executionInfo":{"status":"ok","timestamp":1629041165528,"user_tz":180,"elapsed":13750,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}},"outputId":"66c5a2af-d599-4279-c174-3838342e5201"}},{"cell_type":"code","execution_count":45,"source":["t = 'como usar lucros e reinvestir sem ser esmagado por impostos'.split(' ')\n","t2 = [word2idx[word] for word in t if word in vocab]\n","t3 = model.predict(t2)\n","[idx2tag[np.argmax(cat)] for cat in t3]"],"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Model was constructed with shape (None, 28) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28), dtype=tf.float32, name='embedding_51_input'), name='embedding_51_input', description=\"created by layer 'embedding_51_input'\"), but it was called on an input with incompatible shape (None, 1).\n"]},{"output_type":"execute_result","data":{"text/plain":["['OTHER',\n"," 'OTHER',\n"," 'OTHER',\n"," 'OTHER',\n"," 'OTHER',\n"," 'OTHER',\n"," 'OTHER',\n"," 'OTHER',\n"," 'OTHER',\n"," 'OTHER']"]},"metadata":{},"execution_count":45}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EfyA-t6DhxM2","executionInfo":{"status":"ok","timestamp":1629041171600,"user_tz":180,"elapsed":1085,"user":{"displayName":"Gabriel Barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggw88xGTXMf80J0aqy6vO_pCJQ6zQoz5-NF0-VWKQ=s64","userId":"03667439445750825711"}},"outputId":"a702f38c-a98f-4de6-e701-e30598ba3c7e"}},{"cell_type":"markdown","source":["Não foram obtidos bons resultados dada a pequeneza do _dataset_, mas é possível observar que a rede neural aprendeu a sugerir a tag _OTHER_, que possui número igual ao usado para _padding_."],"metadata":{}}]}